{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading wide table\n",
    "df_wide = pd.read_csv('../microservices/dash_app/wide.csv')\n",
    "df_wide['datetime'] = pd.to_datetime(df_wide['hour'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_gamestop = df_wide.set_index('datetime')\n",
    "\n",
    "train_org_df = df_gamestop[df_gamestop.index.year == 2020]\n",
    "test_org_df = df_gamestop[df_gamestop.index.year == 2021]\n",
    "test_org_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding missing values\n",
    "df_wide.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datatime for visualization\n",
    "train_datetime_list = list(train_org_df.index)\n",
    "test_datetime_list = list(test_org_df.index)\n",
    "gamestop_datetime_list = list(df_gamestop.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a feature set for training and testing \n",
    "feature_set_finn = [\"openprice\", \"highprice\", \"lowprice\", \"volume\", \"closeprice\"]\n",
    "\n",
    "feature_set_wide = [\n",
    "    \"avg_all_post_pos\",\n",
    "    \"avg_all_post_neg\",\n",
    "    \"avg_all_post_neu\",\n",
    "    \"cnt_all_user\",\n",
    "    \"cnt_all_tag\",\n",
    "    \"cnt_all_post\",\n",
    "    \"cnt_all_comments\",\n",
    "    \"avg_gme_post_pos\",\n",
    "    \"avg_gme_post_neg\",\n",
    "    \"avg_gme_post_neu\",\n",
    "    \"cnt_gme_user\",\n",
    "    \"cnt_gme_tag\",\n",
    "    \"cnt_gme_post\",\n",
    "    \"cnt_gme_comments\",\n",
    "    \"volume\",\n",
    "    \"openprice\",\n",
    "    \"highprice\",\n",
    "    \"lowprice\",\n",
    "    \"closeprice\"\n",
    "]\n",
    "feature_set_eng = [\n",
    "    \"avg_all_post_pos\",\n",
    "    \"avg_all_post_neg\",\n",
    "    \"avg_all_post_neu\",\n",
    "    \"cnt_all_user\",\n",
    "    \"cnt_all_tag\",\n",
    "    \"cnt_all_post\",\n",
    "    \"cnt_all_comments\",\n",
    "    \"avg_gme_post_pos\",\n",
    "    \"avg_gme_post_neg\",\n",
    "    \"avg_gme_post_neu\",\n",
    "    \"cnt_gme_user\",\n",
    "    \"cnt_gme_tag\",\n",
    "    \"cnt_gme_post\",\n",
    "    \"cnt_gme_comments\",\n",
    "    \"closeprice\"\n",
    "]\n",
    "\n",
    "feature_label_cols = feature_set_eng\n",
    "feature_cols = feature_label_cols[:-1]\n",
    "label_cols = feature_label_cols[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_org_df[feature_label_cols]\n",
    "test_df = test_org_df[feature_label_cols]\n",
    "\n",
    "concat_df = df_gamestop[feature_label_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(df, feature_set, scaler=None):\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        scaled_arr = scaler.fit_transform(df[feature_set])\n",
    "    else:\n",
    "        scaled_arr = scaler.transform(df[feature_set])\n",
    "    return scaled_arr, scaler\n",
    "\n",
    "def split(train_arr, train_ratio):\n",
    "    # split the data to train, validate\n",
    "    n = len(train_arr)\n",
    "    train_set = train_arr[:int(n*train_ratio)]\n",
    "    val_set = train_arr[int(n*train_ratio):]\n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Windowing for time series forecasting\n",
    "\n",
    "> Refer to [Data Windowing](https://www.tensorflow.org/tutorials/structured_data/time_series#data_windowing) for more details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate time windows for time series forecasting with LSTM network\n",
    "def generate_window(dataset, label, train_window, pred_horizon):\n",
    "    dataset_seq = []\n",
    "    size = len(dataset)\n",
    "    x_arr = []\n",
    "    y_arr = []\n",
    "    for i in range(size - train_window - pred_horizon):\n",
    "        x = dataset[i:(i+train_window), :]\n",
    "        y = label[i+train_window+pred_horizon-1:i+train_window+pred_horizon]\n",
    "        x_arr.append(x)\n",
    "        y_arr.append(y)\n",
    "\n",
    "    x_tensor = torch.tensor(x_arr).float()\n",
    "    y_tensor = torch.tensor(y_arr).float()\n",
    "    num_features = x_tensor.shape[2]\n",
    "    dataset_seq = (x_tensor, y_tensor)\n",
    "    return dataset_seq, num_features\n",
    "\n",
    "def create_batch_set(dataset_seq, batch_size = 100):\n",
    "    x_tensor, y_tensor = dataset_seq\n",
    "    tensor_dataset = TensorDataset(x_tensor,y_tensor)\n",
    "    tensor_dataloader = DataLoader(tensor_dataset, batch_size, False)\n",
    "    return tensor_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create sequential training dataset with various traning windows and prediction horizons\n",
    "# given that the times series data has 1-hour resolution\n",
    "# 24hours * (days)\n",
    "train_window_list = [3, 5, 10, 24, 24*2, 24*5, 24*7]\n",
    "prediction_horizon_list = [1, 2, 3, 5, 24*1, 24*2, 24*3]\n",
    "\n",
    "# choose these set for testing \n",
    "train_window = train_window_list[3]\n",
    "prediction_horizon = prediction_horizon_list[0]\n",
    "print('train window: ', train_window)\n",
    "print('prediction horizon: ', prediction_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = train_df[label_cols]\n",
    "label_test = test_df[label_cols]\n",
    "label_all = concat_df[label_cols]\n",
    "\n",
    "\n",
    "\n",
    "train_set, train_scaler = scale(train_df, feature_cols)\n",
    "target_set, _ = scale(test_df, feature_cols, train_scaler)\n",
    "train_set , val_set = split(train_set, 0.8)\n",
    "label_train , label_val = split(label_train, 0.8)\n",
    "\n",
    "gamestop_set, _ = scale(concat_df, feature_cols)\n",
    "\n",
    "train_seq, num_features = generate_window(train_set, label_train, train_window, prediction_horizon)\n",
    "val_seq, _ = generate_window(val_set, label_val, train_window, prediction_horizon)\n",
    "\n",
    "x_train, _ =train_seq\n",
    "x_val, _ = val_seq\n",
    "train_batch_size = int(len(x_train)*0.5)\n",
    "val_batch_size = int(len(x_val)*1)\n",
    "\n",
    "train_batches = create_batch_set(train_seq, batch_size=200)\n",
    "val_batches = create_batch_set(val_seq, batch_size=2)\n",
    "\n",
    "target_seq, _ = generate_window(target_set, label_test, train_window, prediction_horizon)\n",
    "gamestop_seq, _ = generate_window(gamestop_set, label_all, train_window, prediction_horizon)\n",
    "\n",
    "datetime_target = test_datetime_list[train_window+prediction_horizon:]\n",
    "datetime_gamestop = gamestop_datetime_list[train_window+prediction_horizon:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, seq_length, hidden_size=100, num_layers = 2, output_size=1):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.seq_len = seq_length\n",
    "        \n",
    "        self.hidden_state = None\n",
    "        self.cell_state = None\n",
    "        self.hidden = (self.hidden_state, self.cell_state)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(self.hidden_size * self.seq_len, self.output_size)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        self.hidden_state = torch.zeros(self.num_layers, batch_size ,self.hidden_size).to(device)\n",
    "        self.cell_state = torch.zeros(self.num_layers, batch_size ,self.hidden_size).to(device)\n",
    "        self.hidden = (self.hidden_state, self.cell_state)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "                x,\n",
    "                self.hidden\n",
    "            )\n",
    "        outputs = self.linear(lstm_out.reshape(x.size(0),-1))\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, train_batches, val_batches=None, num_epochs=50):\n",
    "\n",
    "    learning_rate = 1e-3\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_loss = np.Inf\n",
    "    val_loss = None\n",
    "    \n",
    "\n",
    "    criterion = torch.nn.MSELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        batch_losses = []\n",
    "        for batch_ndx, train_batch in enumerate(train_batches):\n",
    "            model.train()\n",
    "            x_train, y_train = train_batch\n",
    "            batch_size = x_train.size(0)\n",
    "            x_train = x_train.to(device)\n",
    "            y_train = y_train.to(device)\n",
    "            model.init_hidden(batch_size, device)\n",
    "            outputs = model(x_train)\n",
    "            loss = criterion(outputs, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_loss = loss.item()\n",
    "            batch_losses.append(batch_loss)\n",
    "            \n",
    "        if val_batches is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_batch_losses = []\n",
    "                for _, val_batch in enumerate(val_batches):\n",
    "                    x_val, y_val = val_batch\n",
    "                    batch_size = x_val.size(0)\n",
    "                    x_val = x_val.to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    model.init_hidden(batch_size, device)\n",
    "                    pred = model(x_val)\n",
    "                    loss = criterion(pred, y_val)\n",
    "                    val_loss = loss.item()\n",
    "                    val_batch_losses.append(val_loss)\n",
    "                    if val_loss < best_loss:\n",
    "                        best_loss = val_loss\n",
    "        \n",
    "        val_losses_mean = np.mean(val_batch_losses)\n",
    "        batch_losses_mean = np.mean(batch_losses)\n",
    "        print(f\"Epoch {epoch}: train loss {batch_losses_mean}, val loss {val_losses_mean}\")\n",
    "        val_losses.append(val_losses_mean)\n",
    "        train_losses.append(batch_losses_mean)\n",
    "        \n",
    "        \n",
    "    return model.eval(), train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM_SEED = 42\n",
    "# np.random.seed(RANDOM_SEED)\n",
    "# torch.manual_seed(RANDOM_SEED)\n",
    "# torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = LSTM(input_size=num_features, seq_length=train_window)\n",
    "model.to(device)\n",
    "\n",
    "model, train_losses, val_losses = train_model(model, device, train_batches, val_batches, num_epochs=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"Training loss\")\n",
    "plt.plot(val_losses, label=\"Test loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "plt.title('Training Loss Distribution', fontsize=16)\n",
    "sns.distplot(train_losses, bins=50, kde=True)\n",
    "plt.xlim([0.0, 5.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, target_seq):\n",
    "    observed, predicted, losses = [], [], []\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    criterion = nn.L1Loss(reduction='sum').to(device)\n",
    "    model.to(device)\n",
    "    print(device)\n",
    "\n",
    "    X_test, y_test = target_seq\n",
    "    set_size = X_test.size(0)\n",
    "\n",
    "    # model.init_hidden(X_test.size(0), device)\n",
    "    # y_pred = model(X_test)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i in range(set_size):\n",
    "            x_i = X_test[i:i+1]\n",
    "            y_i = y_test[i:i+1]\n",
    "            x_i.to(device)\n",
    "            y_i.to(device)\n",
    "            model.init_hidden(x_i.size(0), device)\n",
    "            y_pred = model(x_i)\n",
    "            loss = criterion(y_pred, y_i)\n",
    "            predicted.append(y_pred.item())\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    observed = y_test.cpu().numpy().flatten()\n",
    "    predictions = np.array(predicted).flatten()\n",
    "    losses = p.array(losses).flatten()\n",
    "    return predictions, observed, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions, observed, pred_losses = predict(model, target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(datetime_target,predictions, label=\"Predicted\")\n",
    "plt.plot(datetime_target, observed, label=\"Observed\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(datetime_target ,pred_losses, label=\"Prediction Loss\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "np.mean(f\"Prediction loss mean = {pred_losses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "plt.title('Anomaly Loss Distribution', fontsize=16)\n",
    "sns.distplot(pred_losses, bins=50, kde=True)\n",
    "plt.xlim([0.0, 5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_org_df[label_cols].plot()\n",
    "test_org_df['prediction'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a prediciton column to the original dataframe\n",
    "predictions, observed, losses = predict(model, gamestop_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gamestop['prediction'] = np.nan\n",
    "X_test, y_test = gamestop_seq\n",
    "set_size = X_test.size(0)\n",
    "for i in range(set_size)\n",
    "    df_gamestop.at[datetime_gamestop[i], 'prediction'] = predictions(i)\n",
    "\n",
    "df_gamestop.to_csv('../foobar/data/processed/wide.csv')\n",
    "print('The wide table is updated with the prediction results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(datetime_gamestop,predictions, label=\"Predictions\")\n",
    "plt.plot(datetime_gamestop, truth, label=\"Truth\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving...')\n",
    "state = {\n",
    "    'model': model.state_dict(),\n",
    "    'scaler': train_scaler,\n",
    "    'feature_set': feature_set_wide,\n",
    "    'train_loss': train_losses,\n",
    "    'val_loss': val_losses,\n",
    "    'pred_horizon': prediction_horizon,\n",
    "    'train_window': train_window\n",
    "}\n",
    "\n",
    "if not os.path.isdir('checkpoint'):\n",
    "    os.mkdir('checkpoint')\n",
    "torch.save(state, f'./checkpoint/m2.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd0e4b32c4c655fa694298ff80f540860bd845c191a9c38f277b9165c7be10f4e8e",
   "display_name": "Python 3.8.8 64-bit ('cmpt733': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}