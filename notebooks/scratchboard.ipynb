{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0e4b32c4c655fa694298ff80f540860bd845c191a9c38f277b9165c7be10f4e8e",
   "display_name": "Python 3.8.8 64-bit ('cmpt733': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw financial data\n",
    "# hourly resolution\n",
    "gamestop_data = pd.read_csv('../foobar/data/raw/stock_candle_60_2020-03-01_2021-03-01.csv')\n",
    "gamestop_data = gamestop_data.sort_values(by=['timestamp'], axis=0)\n",
    "gamestop_data['datetime'] = pd.to_datetime(gamestop_data['timestamp'], unit='s')\n",
    "gamestop_data = gamestop_data.set_index('datetime')\n",
    "\n",
    "# resmaple the data hourly and pad the gaps with the previous record\n",
    "gamestop_data = gamestop_data.resample('H', label='right').pad()\n",
    "print(gamestop_data.shape)\n",
    "\n",
    "train_data = gamestop_data[gamestop_data.index.year == 2020]\n",
    "short_squeeze_data = gamestop_data[gamestop_data.index.year == 2021]\n",
    "print(train_data.shape)\n",
    "print(short_squeeze_data.shape)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "source": [
    "## Data Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test dataset\n",
    "# sorted based on timestamp \n",
    "\n",
    "# extract timestamps for visualization\n",
    "train_datetime_list = list(train_data.index)\n",
    "target_datetime_list = list(short_squeeze_data.index)\n",
    "\n",
    "# reordering the columns: put the prediction column to the last column\n",
    "cols = ['open_price', 'high-price', 'low-price', 'volume','close_price']\n",
    "df_train = train_data[cols]\n",
    "df_target = short_squeeze_data[cols]\n",
    "\n",
    "# check the df column types to ensure they have correct types\n",
    "print(df_train.dtypes)\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['close_price'].plot()\n",
    "df_target['close_price'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding missing values\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the ditribution of the features in the train dataset\n",
    "df_std = (df_train - df_train.mean()) / df_train.std()\n",
    "df_std = df_std.melt(var_name='Column', value_name='Normalized')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(df_train, df_target, feature_set):\n",
    "    sc = StandardScaler()\n",
    "    train_arr = sc.fit_transform(df_train[feature_set])\n",
    "    target_arr = sc.fit_transform(df_target[feature_set])\n",
    "    return train_arr, target_arr\n",
    "\n",
    "def split(train_arr, train_ratio):\n",
    "    # split the data to train, validate\n",
    "    n = len(train_arr)\n",
    "    train_set = train_arr[:int(n*train_ratio)]\n",
    "    val_set = train_arr[int(n*train_ratio):]\n",
    "    return train_set, val_set"
   ]
  },
  {
   "source": [
    "### Data Windowing for time series forecasting\n",
    "\n",
    "> Refer to [Data Windowing](https://www.tensorflow.org/tutorials/structured_data/time_series#data_windowing) for more details\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate time windows for time series forecasting with LSTM network\n",
    "def generate_window(dataset, train_window, pred_horizon):\n",
    "    dataset_seq = []\n",
    "    size = len(dataset)\n",
    "    x_arr = []\n",
    "    y_arr = []\n",
    "    for i in range(size - train_window - pred_horizon):\n",
    "        x = dataset[i:(i+train_window), :-1]\n",
    "        y = dataset[i+train_window+pred_horizon-1:i+train_window+pred_horizon, -1]\n",
    "        x_arr.append(x)\n",
    "        y_arr.append(y)\n",
    "\n",
    "    x_tensor = torch.tensor(x_arr).float()\n",
    "    y_tensor = torch.tensor(y_arr).float()\n",
    "    num_features = x_tensor.shape[2]\n",
    "    dataset_seq = (x_tensor, y_tensor)\n",
    "    return dataset_seq, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various feature set for model selection\n",
    "feature_set1 = ['open_price', 'high-price', 'low-price', 'volume', 'close_price']\n",
    "feature_set2 = ['open_price', 'high-price', 'low-price', 'close_price']\n",
    "feature_set3 = ['open_price', 'volume', 'close_price']\n",
    "feature_set4 = ['volume', 'close_price']\n",
    "feature_sets = [feature_set1, feature_set1, feature_set3, feature_set4]\n",
    "\n",
    "# let's create sequential training dataset with various traning windows and prediction horizons\n",
    "# given that the times series data has 1-hour resolution\n",
    "# 24hours * (days)\n",
    "train_window_list = 24 * np.array([5, 10, 30])\n",
    "prediction_horizon_list = 24 * np.array([1, 2, 3, 5, 10])\n",
    "\n",
    "\n",
    "train_set, target_set = scale(df_train, df_target, feature_sets[0])\n",
    "train_set , val_set = split(train_set, 0.8)\n",
    "\n",
    "train_window = train_window_list[0]\n",
    "prediction_horizon = prediction_horizon_list[0]\n",
    "\n",
    "train_seq, num_features = generate_window(train_set, train_window, prediction_horizon)\n",
    "val_seq, _ = generate_window(val_set, train_window, prediction_horizon)\n",
    "target_seq, _ = generate_window(target_set, train_window, prediction_horizon)\n",
    "\n",
    "datetime_target = target_datetime_list[train_window+prediction_horizon:]\n"
   ]
  },
  {
   "source": [
    "## Creating LSTM Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, seq_length, hidden_size=100, num_layers = 2, output_size=1):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.seq_len = seq_length\n",
    "        \n",
    "        self.hidden = None\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(self.hidden_size * self.seq_len, self.output_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden_state = torch.zeros(self.num_layers, batch_size ,self.hidden_size)\n",
    "        cell_state = torch.zeros(self.num_layers, batch_size ,self.hidden_size)\n",
    "        self.hidden = (hidden_state, cell_state)\n",
    "\n",
    "    def forward(self, x, forecast_timesteps = 0):\n",
    "        batch_size = x.shape[0]\n",
    "        if self.hidden is None:\n",
    "            self.init_hidden(batch_size)\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "                x,\n",
    "                self.hidden\n",
    "            )\n",
    "        \n",
    "        outputs = self.linear(lstm_out.reshape(batch_size,-1))\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "source": [
    "## Traning LSTM model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, train_seq, val_seq=None, num_epochs=200):\n",
    "    X_train, y_train = train_seq\n",
    "    inputs, labels = X_train.to(device), y_train.to(device)\n",
    "    if val_seq is not None:\n",
    "        X_val, y_val = val_seq\n",
    "        inputs_val, labels_val = X_val.to(device), y_val.to(device)\n",
    "\n",
    "    learning_rate = 1e-3\n",
    "    best_loss = np.Inf\n",
    "    val_loss = None\n",
    "    history = dict(train=[], val=[])\n",
    "\n",
    "    criterion = torch.nn.MSELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        model.init_hidden(inputs.shape[0])\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loss = loss.item()\n",
    "        history['train'].append(train_loss)\n",
    "\n",
    "        if val_seq is not None:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                model.init_hidden(inputs_val.shape[0])\n",
    "                pred = model(inputs_val)\n",
    "                loss = criterion(pred, labels_val)\n",
    "                val_loss = loss.item()\n",
    "                history['val'].append(val_loss)\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
    "\n",
    "        if val_loss < 0.05:\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            break\n",
    "   \n",
    "    return model.eval(), history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LSTM(input_size=num_features, seq_length=train_window)\n",
    "model = model.to(device)\n",
    "\n",
    "model, history = train_model(model, device, train_seq, val_seq)\n",
    "\n",
    "plt.plot(history['train'], label=\"Training loss\")\n",
    "plt.plot(history['val'], label=\"Test loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "source": [
    "## Predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, losses = [], []\n",
    "criterion = nn.L1Loss(reduction='sum').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i in range(X_test.shape[0]):\n",
    "        x_i = X_test[i:i+1]\n",
    "        y_i = y_test[i:i+1]\n",
    "        x_i.to(device)\n",
    "        model.init_hidden(x_i.shape[0])\n",
    "        y_pred = model(x_i)\n",
    "        predictions.append(y_pred.cpu().numpy().flatten())\n",
    "        loss = criterion(y_pred, y_i)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "truth = y_test.cpu().numpy().flatten()\n",
    "predictions = np.array(predictions).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(datetime_target,predictions, label=\"Predictions\")\n",
    "plt.plot(datetime_target, truth, label=\"Truth\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(datetime_target ,losses, label=\"Prediction Loss\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_index = 0\n",
    "for train_window in train_window_list:\n",
    "    for prediction_horizon in prediction_horizon_list:\n",
    "        for feature_set in feature_sets:\n",
    "\n",
    "            train_set, target_set = scale(df_train, df_target, feature_set)\n",
    "            train_set , val_set = split(train_set, 0.8)\n",
    "\n",
    "            train_seq, num_features = generate_window(train_set, train_window, prediction_horizon)\n",
    "            val_seq, _ = generate_window(val_set, train_window, prediction_horizon)\n",
    "            target_seq, _ = generate_window(target_set, train_window, prediction_horizon)\n",
    "\n",
    "            datetime_target = test_datetime_list[train_window+prediction_horizon:]\n",
    "\n",
    "            model = LSTM(input_size=num_features, seq_length=train_window)\n",
    "            model = model.to(device)\n",
    "\n",
    "            model, history = train_model(model, device, train_seq, val_seq)\n",
    "            model_index += 1\n",
    "\n",
    "            print('Saving...')\n",
    "            state = {\n",
    "                'model': model,\n",
    "                'feature_set': feature_set,\n",
    "                'history': history,\n",
    "                'pred_horizon': prediction_horizon,\n",
    "                'train_window': train_window\n",
    "            }\n",
    "\n",
    "            if not os.path.isdir('checkpoint'):\n",
    "                os.mkdir('checkpoint')\n",
    "            torch.save(state, f'./checkpoint/train_{model_index}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}