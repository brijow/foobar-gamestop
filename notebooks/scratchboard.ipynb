{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(7692, 8)\n(6395, 8)\n(1297, 8)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'closeprice', 'highprice', 'lowprice', 'openprice',\n",
       "       'status', 'timestamp', 'volume'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# load raw financial data\n",
    "# hourly resolution\n",
    "gamestop_data = pd.read_csv('../foobar/data/raw/stock_candle_60_2020-03-01_2021-03-01.csv')\n",
    "gamestop_data = gamestop_data.sort_values(by=['timestamp'], axis=0)\n",
    "gamestop_data['hour'] = pd.to_datetime(gamestop_data['timestamp'], unit='s')\n",
    "gamestop_data = gamestop_data.set_index('hour')\n",
    "\n",
    "# resmaple the data hourly and pad the gaps with the previous record\n",
    "gamestop_data = gamestop_data.resample('H', label='right').pad()\n",
    "print(gamestop_data.shape)\n",
    "gamestop_data = gamestop_data.rename(\n",
    "                        columns={\n",
    "                            \"close_price\": \"closeprice\",\n",
    "                            \"open_price\": \"openprice\",\n",
    "                            \"high-price\": \"highprice\",\n",
    "                            \"low-price\": \"lowprice\"\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "train_data = gamestop_data[gamestop_data.index.year == 2020]\n",
    "short_squeeze_data = gamestop_data[gamestop_data.index.year == 2021]\n",
    "print(train_data.shape)\n",
    "print(short_squeeze_data.shape)\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "openprice     float64\nhighprice     float64\nlowprice      float64\nvolume          int64\ncloseprice    float64\ndtype: object\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                     openprice  highprice  lowprice   volume  closeprice  \\\n",
       "hour                                                                       \n",
       "2020-04-09 13:00:00      3.450       3.72     3.450   319130        3.70   \n",
       "2020-04-09 14:00:00      3.700       3.90     3.685   620809        3.86   \n",
       "2020-04-09 15:00:00      3.865       4.08     3.800  1023287        4.06   \n",
       "2020-04-09 16:00:00      4.060       4.19     3.890   602239        4.12   \n",
       "2020-04-09 17:00:00      4.130       4.25     3.870  1006734        4.03   \n",
       "...                        ...        ...       ...      ...         ...   \n",
       "2021-02-23 20:00:00     44.910      45.40    44.000   835553       44.92   \n",
       "2021-02-23 21:00:00     44.970      45.13    42.900   379831       43.21   \n",
       "2021-02-23 22:00:00     43.240      43.55    42.660   125100       43.00   \n",
       "2021-02-23 23:00:00     43.000      44.35    43.000    17172       43.85   \n",
       "2021-02-24 00:00:00     43.870      44.00    43.010    28059       43.72   \n",
       "\n",
       "                     prediction  \n",
       "hour                             \n",
       "2020-04-09 13:00:00         NaN  \n",
       "2020-04-09 14:00:00         NaN  \n",
       "2020-04-09 15:00:00         NaN  \n",
       "2020-04-09 16:00:00         NaN  \n",
       "2020-04-09 17:00:00         NaN  \n",
       "...                         ...  \n",
       "2021-02-23 20:00:00         NaN  \n",
       "2021-02-23 21:00:00         NaN  \n",
       "2021-02-23 22:00:00         NaN  \n",
       "2021-02-23 23:00:00         NaN  \n",
       "2021-02-24 00:00:00         NaN  \n",
       "\n",
       "[7692 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>openprice</th>\n      <th>highprice</th>\n      <th>lowprice</th>\n      <th>volume</th>\n      <th>closeprice</th>\n      <th>prediction</th>\n    </tr>\n    <tr>\n      <th>hour</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-04-09 13:00:00</th>\n      <td>3.450</td>\n      <td>3.72</td>\n      <td>3.450</td>\n      <td>319130</td>\n      <td>3.70</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2020-04-09 14:00:00</th>\n      <td>3.700</td>\n      <td>3.90</td>\n      <td>3.685</td>\n      <td>620809</td>\n      <td>3.86</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2020-04-09 15:00:00</th>\n      <td>3.865</td>\n      <td>4.08</td>\n      <td>3.800</td>\n      <td>1023287</td>\n      <td>4.06</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2020-04-09 16:00:00</th>\n      <td>4.060</td>\n      <td>4.19</td>\n      <td>3.890</td>\n      <td>602239</td>\n      <td>4.12</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2020-04-09 17:00:00</th>\n      <td>4.130</td>\n      <td>4.25</td>\n      <td>3.870</td>\n      <td>1006734</td>\n      <td>4.03</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-02-23 20:00:00</th>\n      <td>44.910</td>\n      <td>45.40</td>\n      <td>44.000</td>\n      <td>835553</td>\n      <td>44.92</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2021-02-23 21:00:00</th>\n      <td>44.970</td>\n      <td>45.13</td>\n      <td>42.900</td>\n      <td>379831</td>\n      <td>43.21</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2021-02-23 22:00:00</th>\n      <td>43.240</td>\n      <td>43.55</td>\n      <td>42.660</td>\n      <td>125100</td>\n      <td>43.00</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2021-02-23 23:00:00</th>\n      <td>43.000</td>\n      <td>44.35</td>\n      <td>43.000</td>\n      <td>17172</td>\n      <td>43.85</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2021-02-24 00:00:00</th>\n      <td>43.870</td>\n      <td>44.00</td>\n      <td>43.010</td>\n      <td>28059</td>\n      <td>43.72</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>7692 rows Ã— 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# create train and test dataset\n",
    "# sorted based on timestamp \n",
    "\n",
    "# extract timestamps for visualization\n",
    "train_datetime_list = list(train_data.index)\n",
    "target_datetime_list = list(short_squeeze_data.index)\n",
    "gamestop_datetime_list = list(gamestop_data.index)\n",
    "\n",
    "# reordering the columns: put the prediction column to the last column\n",
    "cols = ['openprice', 'highprice', 'lowprice', 'volume', 'closeprice']\n",
    "df_train = train_data[cols]\n",
    "df_target = short_squeeze_data[cols]\n",
    "df_gamestop = gamestop_data[cols]\n",
    "\n",
    "out = df_gamestop.copy()\n",
    "out['prediction'] = np.nan\n",
    "out.to_csv('../foobar/data/processed/gme.csv')\n",
    "# check the df column types to ensure they have correct types\n",
    "print(df_train.dtypes)\n",
    "cols\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='hour'>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"273.394062pt\" version=\"1.1\" viewBox=\"0 0 375.2875 273.394062\" width=\"375.2875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-09T22:26:16.008832</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 273.394062 \nL 375.2875 273.394062 \nL 375.2875 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 224.64 \nL 368.0875 224.64 \nL 368.0875 7.2 \nL 33.2875 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m5f47da2de7\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m5f47da2de7\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"55.706171\" xlink:href=\"#m5f47da2de7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- May -->\n      <g transform=\"translate(45.368671 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n        <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n        <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-77\"/>\n       <use x=\"86.279297\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"147.558594\" xlink:href=\"#DejaVuSans-121\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"88.093533\" xlink:href=\"#m5f47da2de7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- Jun -->\n      <g transform=\"translate(80.281033 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 5.078125 \nQ 19.671875 -8.109375 14.671875 -14.0625 \nQ 9.671875 -20.015625 -1.421875 -20.015625 \nL -5.171875 -20.015625 \nL -5.171875 -11.71875 \nL -2.09375 -11.71875 \nQ 4.4375 -11.71875 7.125 -8.046875 \nQ 9.8125 -4.390625 9.8125 5.078125 \nz\n\" id=\"DejaVuSans-74\"/>\n        <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n        <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-74\"/>\n       <use x=\"29.492188\" xlink:href=\"#DejaVuSans-117\"/>\n       <use x=\"92.871094\" xlink:href=\"#DejaVuSans-110\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"119.436141\" xlink:href=\"#m5f47da2de7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- Jul -->\n      <g transform=\"translate(113.403329 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-74\"/>\n       <use x=\"29.492188\" xlink:href=\"#DejaVuSans-117\"/>\n       <use x=\"92.871094\" xlink:href=\"#DejaVuSans-108\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"151.823503\" xlink:href=\"#m5f47da2de7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- Aug -->\n      <g transform=\"translate(142.060222 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n        <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-65\"/>\n       <use x=\"68.408203\" xlink:href=\"#DejaVuSans-117\"/>\n       <use x=\"131.787109\" xlink:href=\"#DejaVuSans-103\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"184.210865\" xlink:href=\"#m5f47da2de7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- Sep -->\n      <g transform=\"translate(174.785865 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 53.515625 70.515625 \nL 53.515625 60.890625 \nQ 47.90625 63.578125 42.921875 64.890625 \nQ 37.9375 66.21875 33.296875 66.21875 \nQ 25.25 66.21875 20.875 63.09375 \nQ 16.5 59.96875 16.5 54.203125 \nQ 16.5 49.359375 19.40625 46.890625 \nQ 22.3125 44.4375 30.421875 42.921875 \nL 36.375 41.703125 \nQ 47.40625 39.59375 52.65625 34.296875 \nQ 57.90625 29 57.90625 20.125 \nQ 57.90625 9.515625 50.796875 4.046875 \nQ 43.703125 -1.421875 29.984375 -1.421875 \nQ 24.8125 -1.421875 18.96875 -0.25 \nQ 13.140625 0.921875 6.890625 3.21875 \nL 6.890625 13.375 \nQ 12.890625 10.015625 18.65625 8.296875 \nQ 24.421875 6.59375 29.984375 6.59375 \nQ 38.421875 6.59375 43.015625 9.90625 \nQ 47.609375 13.234375 47.609375 19.390625 \nQ 47.609375 24.75 44.3125 27.78125 \nQ 41.015625 30.8125 33.5 32.328125 \nL 27.484375 33.5 \nQ 16.453125 35.6875 11.515625 40.375 \nQ 6.59375 45.0625 6.59375 53.421875 \nQ 6.59375 63.09375 13.40625 68.65625 \nQ 20.21875 74.21875 32.171875 74.21875 \nQ 37.3125 74.21875 42.625 73.28125 \nQ 47.953125 72.359375 53.515625 70.515625 \nz\n\" id=\"DejaVuSans-83\"/>\n        <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n        <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-83\"/>\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"125\" xlink:href=\"#DejaVuSans-112\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"215.553473\" xlink:href=\"#m5f47da2de7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- Oct -->\n      <g transform=\"translate(206.908161 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 39.40625 66.21875 \nQ 28.65625 66.21875 22.328125 58.203125 \nQ 16.015625 50.203125 16.015625 36.375 \nQ 16.015625 22.609375 22.328125 14.59375 \nQ 28.65625 6.59375 39.40625 6.59375 \nQ 50.140625 6.59375 56.421875 14.59375 \nQ 62.703125 22.609375 62.703125 36.375 \nQ 62.703125 50.203125 56.421875 58.203125 \nQ 50.140625 66.21875 39.40625 66.21875 \nz\nM 39.40625 74.21875 \nQ 54.734375 74.21875 63.90625 63.9375 \nQ 73.09375 53.65625 73.09375 36.375 \nQ 73.09375 19.140625 63.90625 8.859375 \nQ 54.734375 -1.421875 39.40625 -1.421875 \nQ 24.03125 -1.421875 14.8125 8.828125 \nQ 5.609375 19.09375 5.609375 36.375 \nQ 5.609375 53.65625 14.8125 63.9375 \nQ 24.03125 74.21875 39.40625 74.21875 \nz\n\" id=\"DejaVuSans-79\"/>\n        <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n        <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-79\"/>\n       <use x=\"78.710938\" xlink:href=\"#DejaVuSans-99\"/>\n       <use x=\"133.691406\" xlink:href=\"#DejaVuSans-116\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"247.940835\" xlink:href=\"#m5f47da2de7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- Nov -->\n      <g transform=\"translate(238.18146 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 9.8125 72.90625 \nL 23.09375 72.90625 \nL 55.421875 11.921875 \nL 55.421875 72.90625 \nL 64.984375 72.90625 \nL 64.984375 0 \nL 51.703125 0 \nL 19.390625 60.984375 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-78\"/>\n        <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n        <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-78\"/>\n       <use x=\"74.804688\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"135.986328\" xlink:href=\"#DejaVuSans-118\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"279.283443\" xlink:href=\"#m5f47da2de7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- Dec -->\n      <g transform=\"translate(269.607662 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.671875 64.796875 \nL 19.671875 8.109375 \nL 31.59375 8.109375 \nQ 46.6875 8.109375 53.6875 14.9375 \nQ 60.6875 21.78125 60.6875 36.53125 \nQ 60.6875 51.171875 53.6875 57.984375 \nQ 46.6875 64.796875 31.59375 64.796875 \nz\nM 9.8125 72.90625 \nL 30.078125 72.90625 \nQ 51.265625 72.90625 61.171875 64.09375 \nQ 71.09375 55.28125 71.09375 36.53125 \nQ 71.09375 17.671875 61.125 8.828125 \nQ 51.171875 0 30.078125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-68\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-68\"/>\n       <use x=\"77.001953\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"138.525391\" xlink:href=\"#DejaVuSans-99\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"311.670805\" xlink:href=\"#m5f47da2de7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- Jan -->\n      <g transform=\"translate(303.962993 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-74\"/>\n       <use x=\"29.492188\" xlink:href=\"#DejaVuSans-97\"/>\n       <use x=\"90.771484\" xlink:href=\"#DejaVuSans-110\"/>\n      </g>\n      <!-- 2021 -->\n      <g transform=\"translate(298.945805 250.43625)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"344.058167\" xlink:href=\"#m5f47da2de7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- Feb -->\n      <g transform=\"translate(335.206605 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 9.8125 72.90625 \nL 51.703125 72.90625 \nL 51.703125 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.109375 \nL 48.578125 43.109375 \nL 48.578125 34.8125 \nL 19.671875 34.8125 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-70\"/>\n        <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-70\"/>\n       <use x=\"52.019531\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"113.542969\" xlink:href=\"#DejaVuSans-98\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"368.0875\" xlink:href=\"#m5f47da2de7\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_13\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 2 \n\" id=\"m74cb4d812d\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"36.900606\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"44.213881\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.527157\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"58.840432\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_17\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"66.153707\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"73.466983\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_19\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"80.780258\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_20\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"95.406808\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_21\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"102.720084\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_22\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"110.033359\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_23\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"117.346634\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_24\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"124.659909\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_25\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"131.973185\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_26\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"139.28646\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_27\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"146.599735\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_28\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"153.91301\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_29\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"161.226286\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_30\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"168.539561\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_31\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"175.852836\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_32\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"183.166111\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_33\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"190.479387\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_34\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"197.792662\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_35\">\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"205.105937\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_36\">\n     <g id=\"line2d_36\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"212.419212\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_37\">\n     <g id=\"line2d_37\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"219.732488\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_38\">\n     <g id=\"line2d_38\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"227.045763\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_39\">\n     <g id=\"line2d_39\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"234.359038\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_40\">\n     <g id=\"line2d_40\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"241.672313\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_41\">\n     <g id=\"line2d_41\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"248.985589\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_42\">\n     <g id=\"line2d_42\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"256.298864\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_43\">\n     <g id=\"line2d_43\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"263.612139\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_44\">\n     <g id=\"line2d_44\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"270.925414\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_45\">\n     <g id=\"line2d_45\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"278.23869\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_46\">\n     <g id=\"line2d_46\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"285.551965\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_47\">\n     <g id=\"line2d_47\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"292.86524\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_48\">\n     <g id=\"line2d_48\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"300.178515\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_49\">\n     <g id=\"line2d_49\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"307.491791\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_50\">\n     <g id=\"line2d_50\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"314.805066\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_51\">\n     <g id=\"line2d_51\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"322.118341\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_52\">\n     <g id=\"line2d_52\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"329.431616\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_53\">\n     <g id=\"line2d_53\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"336.744892\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_54\">\n     <g id=\"line2d_54\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"351.371442\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_55\">\n     <g id=\"line2d_55\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"358.684718\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_56\">\n     <g id=\"line2d_56\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"365.997993\" xlink:href=\"#m74cb4d812d\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_11\">\n     <!-- hour -->\n     <g transform=\"translate(189.235156 264.114375)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"63.378906\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"124.560547\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"187.939453\" xlink:href=\"#DejaVuSans-114\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_57\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m860af05879\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m860af05879\" y=\"216.32992\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 220.129139)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_58\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m860af05879\" y=\"173.801364\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 177.600583)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_59\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m860af05879\" y=\"131.272809\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 200 -->\n      <g transform=\"translate(7.2 135.072027)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_60\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m860af05879\" y=\"88.744253\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 300 -->\n      <g transform=\"translate(7.2 92.543472)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_61\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m860af05879\" y=\"46.215697\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 400 -->\n      <g transform=\"translate(7.2 50.014916)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_62\">\n    <path clip-path=\"url(#p1201b57bef)\" d=\"M 33.2875 214.756364 \nL 33.418094 214.577744 \nL 33.461626 214.616019 \nL 33.59222 214.705329 \nL 33.722814 214.650042 \nL 37.422983 214.560732 \nL 37.510046 214.428894 \nL 37.553577 214.452284 \nL 38.25008 214.28855 \nL 38.467737 214.241768 \nL 38.511268 213.795218 \nL 38.598331 213.897287 \nL 38.685394 213.918551 \nL 38.815988 213.608093 \nL 38.946582 213.69315 \nL 39.51249 213.778207 \nL 39.556022 214.180102 \nL 39.599553 214.029125 \nL 39.643084 213.950448 \nL 39.686616 213.995102 \nL 40.034867 214.143952 \nL 41.68906 214.235389 \nL 41.863186 214.407629 \nL 41.99378 214.25878 \nL 44.77979 214.284297 \nL 44.910384 214.003608 \nL 45.084509 213.96108 \nL 45.824543 213.910045 \nL 45.955137 214.224757 \nL 46.0422 214.231136 \nL 46.216326 214.294929 \nL 52.136596 214.207745 \nL 52.484848 213.863264 \nL 53.137819 213.863264 \nL 53.18135 214.173722 \nL 53.268413 214.018493 \nL 53.48607 213.939815 \nL 54.182572 213.939815 \nL 54.313166 213.778207 \nL 54.574355 213.739931 \nL 55.227326 213.739931 \nL 55.35792 213.867517 \nL 56.315611 213.978091 \nL 56.402674 213.948321 \nL 56.576799 213.756943 \nL 59.40634 213.756943 \nL 59.580466 214.003608 \nL 59.885186 213.978091 \nL 61.495847 214.037631 \nL 61.626442 214.201366 \nL 61.757036 214.22901 \nL 62.105287 214.246021 \nL 67.764369 214.28855 \nL 67.8079 214.386365 \nL 67.894963 214.309814 \nL 70.332722 214.556479 \nL 74.032891 214.535215 \nL 74.163485 214.435273 \nL 74.468205 214.407629 \nL 78.255437 214.518204 \nL 78.386031 214.547974 \nL 78.734282 214.501192 \nL 82.477982 214.407629 \nL 82.652108 214.450158 \nL 83.65333 214.416135 \nL 83.827456 214.322572 \nL 84.698084 214.426767 \nL 84.828678 214.458664 \nL 85.742837 214.535215 \nL 85.873432 214.569238 \nL 90.79248 214.488434 \nL 90.966605 214.437399 \nL 92.838456 214.507572 \nL 92.96905 214.509698 \nL 93.360832 214.539468 \nL 95.972716 214.458664 \nL 96.233905 214.207745 \nL 96.973939 214.190734 \nL 97.104533 214.331078 \nL 97.278658 214.22263 \nL 97.496315 214.305561 \nL 98.018692 214.305561 \nL 98.062224 214.02062 \nL 98.149287 214.075907 \nL 98.323412 214.177975 \nL 98.497538 214.118435 \nL 98.976383 214.118435 \nL 99.19404 214.513951 \nL 99.324634 214.486307 \nL 99.977605 214.377859 \nL 106.63791 214.292802 \nL 106.812035 214.246021 \nL 107.073224 214.224757 \nL 112.688774 214.275791 \nL 112.775837 214.450158 \nL 112.819368 214.416135 \nL 113.080557 214.458664 \nL 126.488228 214.567112 \nL 126.749417 214.586249 \nL 132.582624 214.488434 \nL 132.713218 214.428894 \nL 133.017938 214.513951 \nL 136.848701 214.620272 \nL 137.10989 214.628778 \nL 155.523672 214.539468 \nL 155.654266 214.356595 \nL 156.263706 214.394871 \nL 158.657933 214.445905 \nL 158.832059 214.560732 \nL 161.835725 214.475675 \nL 162.009851 214.465043 \nL 162.271039 214.488434 \nL 164.969986 214.386365 \nL 165.144112 214.360848 \nL 166.798305 214.309814 \nL 173.328015 214.305561 \nL 173.415078 214.126941 \nL 173.458609 214.19286 \nL 173.893923 214.190734 \nL 176.418744 214.148205 \nL 176.505807 214.369354 \nL 176.549339 214.297055 \nL 177.768218 214.220504 \nL 178.116469 214.237515 \nL 178.682377 214.158837 \nL 178.856503 214.156711 \nL 180.728353 214.065275 \nL 180.902479 213.973838 \nL 181.468387 213.914298 \nL 183.601425 213.842 \nL 183.644957 213.786713 \nL 183.688488 213.948321 \nL 183.73202 213.506024 \nL 183.819082 213.659127 \nL 183.949677 213.363553 \nL 183.993208 213.48476 \nL 184.080271 213.437978 \nL 184.254396 213.352921 \nL 184.776773 213.429473 \nL 184.907367 213.142405 \nL 185.125024 213.089244 \nL 185.255619 213.076486 \nL 185.865058 213.099876 \nL 185.90859 213.189186 \nL 185.952121 213.06798 \nL 186.169778 213.055221 \nL 186.300372 212.778786 \nL 186.735686 212.753269 \nL 186.909812 213.080739 \nL 187.040406 213.165796 \nL 187.083937 213.159416 \nL 187.258063 212.953153 \nL 187.649846 212.953153 \nL 187.78044 213.093497 \nL 188.738131 213.076486 \nL 192.046517 213.09775 \nL 192.220643 212.872349 \nL 192.351237 213.059474 \nL 192.394768 212.995681 \nL 193.134802 213.059474 \nL 193.308928 213.172175 \nL 193.39599 213.204071 \nL 193.439522 213.569817 \nL 193.526585 213.540047 \nL 193.918367 213.540047 \nL 194.048961 213.69315 \nL 194.223087 213.446484 \nL 194.527807 213.688897 \nL 198.227976 213.727173 \nL 198.271507 213.420967 \nL 198.35857 213.527288 \nL 199.229198 213.382691 \nL 200.448077 213.331657 \nL 200.578671 212.88936 \nL 200.622203 212.523614 \nL 200.709266 212.638442 \nL 200.970454 212.53212 \nL 201.362237 212.638442 \nL 201.405768 212.714993 \nL 201.4493 212.672464 \nL 201.579894 212.400282 \nL 201.841082 212.438557 \nL 202.668179 212.362006 \nL 202.71171 212.206777 \nL 202.755242 212.272696 \nL 202.972899 212.344995 \nL 205.671845 212.379017 \nL 205.715377 212.734131 \nL 205.80244 212.629936 \nL 205.933034 212.608672 \nL 206.107159 212.26419 \nL 206.455411 212.26419 \nL 206.673068 211.894192 \nL 206.716599 211.99626 \nL 206.76013 211.93672 \nL 206.890725 211.775112 \nL 207.108382 211.770859 \nL 207.717821 211.685802 \nL 207.848415 211.992008 \nL 207.935478 211.881433 \nL 208.022541 212.0558 \nL 208.066072 212.030283 \nL 208.806106 212.030283 \nL 208.9367 212.430052 \nL 208.980232 212.349247 \nL 209.023763 212.391776 \nL 209.197889 212.459822 \nL 209.85086 212.468327 \nL 210.068517 212.066432 \nL 211.679179 212.034536 \nL 212.854526 212.034536 \nL 212.898058 211.868675 \nL 212.941589 211.992008 \nL 213.072183 212.225915 \nL 213.420435 211.992008 \nL 214.029874 211.992008 \nL 214.073406 211.87718 \nL 214.160468 211.919709 \nL 214.639314 211.90695 \nL 215.074628 211.949479 \nL 215.118159 211.787445 \nL 215.161691 211.822319 \nL 215.379348 212.030283 \nL 216.293507 212.132352 \nL 216.467633 212.170627 \nL 216.641758 212.140857 \nL 217.033541 212.140857 \nL 217.164135 212.315225 \nL 217.294729 212.32373 \nL 217.904169 212.332236 \nL 221.386681 212.27057 \nL 221.473744 212.20465 \nL 221.517275 212.268443 \nL 221.734932 212.391776 \nL 221.865526 212.289707 \nL 223.650314 212.387523 \nL 223.737376 210.048453 \nL 223.780908 210.201555 \nL 223.824439 210.210061 \nL 223.955033 210.337647 \nL 224.346816 210.426957 \nL 224.47741 210.975575 \nL 224.564473 210.210061 \nL 224.608004 210.567301 \nL 224.869193 211.396608 \nL 225.043318 211.332815 \nL 227.350483 211.332815 \nL 227.437546 211.175459 \nL 227.524608 211.417872 \nL 227.56814 211.349826 \nL 227.655203 211.098908 \nL 227.698734 211.243505 \nL 227.829328 211.400861 \nL 227.87286 211.315803 \nL 228.046985 211.260516 \nL 228.917613 211.281781 \nL 228.961145 210.907529 \nL 229.048207 211.043621 \nL 229.222333 210.979828 \nL 229.43999 210.979828 \nL 229.570584 210.852242 \nL 229.74471 211.183965 \nL 229.831773 211.043621 \nL 229.875304 211.069138 \nL 230.005898 211.128678 \nL 230.702401 211.205229 \nL 230.876526 210.367417 \nL 230.920058 210.331267 \nL 230.963589 210.120751 \nL 231.00712 210.452474 \nL 231.181246 210.520519 \nL 231.660091 210.456727 \nL 231.747154 210.40144 \nL 231.790686 210.46098 \nL 232.008343 210.716151 \nL 232.269531 210.614082 \nL 234.924946 210.558795 \nL 235.142603 210.333394 \nL 235.186135 210.414198 \nL 235.229666 210.320635 \nL 235.9697 210.358911 \nL 236.100294 210.448221 \nL 236.666202 210.397187 \nL 237.362705 210.286612 \nL 237.580362 210.235578 \nL 238.059207 210.235578 \nL 238.102739 209.86558 \nL 238.189801 210.056958 \nL 238.320396 209.988913 \nL 238.538053 209.959143 \nL 242.238222 209.946384 \nL 242.368816 210.645979 \nL 242.412347 210.569427 \nL 242.455879 210.51414 \nL 242.49941 210.592818 \nL 242.717067 210.567301 \nL 243.239444 210.503508 \nL 243.282975 210.847989 \nL 243.370038 210.826725 \nL 243.631226 210.979828 \nL 244.327729 211.02023 \nL 244.458323 211.145689 \nL 244.545386 211.149942 \nL 244.67598 211.303045 \nL 246.156048 211.349826 \nL 246.286642 211.439136 \nL 246.417236 211.439136 \nL 246.591362 211.860169 \nL 246.721956 211.889939 \nL 246.765487 211.7581 \nL 246.809019 211.851663 \nL 247.070207 211.87718 \nL 249.464434 211.87718 \nL 249.595028 211.647526 \nL 249.769154 211.855916 \nL 250.639782 211.753848 \nL 250.726845 211.409366 \nL 250.770376 211.439136 \nL 250.813907 211.498676 \nL 250.857439 211.388102 \nL 251.118627 211.434883 \nL 251.597473 211.434883 \nL 251.684535 211.256263 \nL 251.81513 211.664538 \nL 252.206912 211.673043 \nL 252.729289 211.673043 \nL 252.772821 211.383849 \nL 252.859883 211.447642 \nL 253.251666 211.443389 \nL 253.861106 211.381723 \nL 253.9917 211.255838 \nL 254.035231 211.281781 \nL 254.383482 211.264769 \nL 257.038898 211.366838 \nL 257.213023 211.417872 \nL 257.953057 211.460401 \nL 257.996589 211.634768 \nL 258.04012 211.515688 \nL 258.083651 211.430631 \nL 258.127183 211.464653 \nL 258.214246 211.604998 \nL 258.257777 211.400861 \nL 258.34484 211.439136 \nL 258.910748 211.447642 \nL 260.042564 211.311551 \nL 260.390816 211.566722 \nL 264.221579 211.651779 \nL 264.352173 211.060632 \nL 264.743956 211.354079 \nL 265.048675 211.354079 \nL 265.17927 211.162701 \nL 265.353395 211.515688 \nL 265.483989 211.277528 \nL 265.658115 211.422125 \nL 266.311086 211.311551 \nL 266.44168 211.320056 \nL 266.7464 211.400861 \nL 267.399371 211.324309 \nL 267.573497 210.984081 \nL 267.791154 211.077644 \nL 268.400593 210.992586 \nL 268.444125 210.703392 \nL 268.531187 210.801208 \nL 268.661782 210.924541 \nL 268.792376 210.886265 \nL 269.010033 210.920288 \nL 271.534854 210.839484 \nL 271.796042 210.418451 \nL 272.057231 210.397187 \nL 272.710202 210.382302 \nL 272.753733 210.541784 \nL 272.840796 210.512014 \nL 273.014922 210.469485 \nL 273.624361 210.469485 \nL 273.842018 210.120751 \nL 273.929081 209.980407 \nL 274.016144 210.056958 \nL 275.583274 210.056958 \nL 275.7574 209.385432 \nL 275.800931 209.635926 \nL 275.844463 209.504087 \nL 275.931525 209.410524 \nL 275.975057 209.440294 \nL 278.586941 209.440294 \nL 278.630472 208.398345 \nL 278.674004 208.462137 \nL 278.717535 209.100066 \nL 278.804598 208.929951 \nL 278.848129 208.275012 \nL 278.891661 208.581217 \nL 279.109318 209.287191 \nL 279.152849 209.21064 \nL 279.283443 209.125583 \nL 279.631695 209.125583 \nL 279.762289 208.895929 \nL 279.936414 209.376501 \nL 279.979946 209.355237 \nL 280.023477 209.363743 \nL 280.154071 209.606156 \nL 280.197603 209.53811 \nL 280.41526 209.504087 \nL 280.807042 209.504087 \nL 280.894105 209.65719 \nL 280.937637 209.406271 \nL 281.024699 209.474317 \nL 281.068231 209.559374 \nL 281.111762 209.521098 \nL 281.285888 209.248916 \nL 281.634139 209.304203 \nL 281.98239 209.236157 \nL 282.112984 209.555121 \nL 282.156516 209.527478 \nL 282.330641 209.4488 \nL 282.504767 209.402018 \nL 282.983612 209.414777 \nL 283.114207 209.027767 \nL 283.201269 209.095813 \nL 283.244801 209.189376 \nL 283.331864 209.12133 \nL 283.593052 209.108571 \nL 286.117873 209.100066 \nL 286.204936 209.397766 \nL 286.248467 209.244663 \nL 286.857907 209.257421 \nL 287.24969 209.363743 \nL 287.336752 209.423283 \nL 287.467347 209.134089 \nL 287.597941 210.269601 \nL 287.728535 210.367417 \nL 287.989723 210.367417 \nL 288.033255 210.25259 \nL 288.120318 210.316382 \nL 288.163849 210.227072 \nL 288.20738 210.329141 \nL 288.381506 210.303624 \nL 288.5121 210.524772 \nL 288.642694 210.482244 \nL 288.81682 210.524772 \nL 289.165071 210.524772 \nL 289.252134 210.754427 \nL 289.339197 210.435462 \nL 289.382728 210.499255 \nL 289.600385 210.31213 \nL 289.643917 210.363164 \nL 289.687448 210.324888 \nL 289.905105 210.290908 \nL 290.340419 210.290908 \nL 290.471013 210.707645 \nL 290.558076 210.580059 \nL 290.601607 210.669369 \nL 290.906327 210.694887 \nL 293.431148 210.665117 \nL 293.518211 211.098908 \nL 293.561743 210.899024 \nL 293.605274 210.911782 \nL 293.648805 210.792702 \nL 293.692337 210.826725 \nL 293.822931 210.886265 \nL 294.562965 210.830978 \nL 294.780622 210.43121 \nL 294.954747 210.448221 \nL 295.694781 210.3419 \nL 295.738313 210.31213 \nL 295.781844 210.384428 \nL 295.95597 210.473738 \nL 296.478346 210.473738 \nL 296.608941 210.299371 \nL 296.652472 210.346152 \nL 296.826598 210.061211 \nL 297.000723 210.035694 \nL 297.610163 209.997418 \nL 297.82782 209.495581 \nL 297.914883 209.682707 \nL 297.958414 209.631673 \nL 298.13254 209.62742 \nL 300.526767 209.62742 \nL 300.613829 210.022935 \nL 300.657361 209.827304 \nL 300.700892 209.844315 \nL 300.744424 209.312708 \nL 300.787955 209.572133 \nL 301.005612 209.791155 \nL 301.223269 209.610408 \nL 301.789177 209.499834 \nL 301.963303 208.100645 \nL 302.093897 208.066622 \nL 302.224491 208.172943 \nL 302.659805 208.160185 \nL 302.703337 208.160185 \nL 302.746868 207.420188 \nL 302.790399 207.896508 \nL 302.833931 207.892255 \nL 302.964525 207.126741 \nL 303.008056 207.130994 \nL 303.051588 207.105476 \nL 303.225713 207.581796 \nL 303.791622 207.581796 \nL 303.922216 207.343636 \nL 304.096342 207.785933 \nL 307.840042 207.713635 \nL 307.927105 206.42502 \nL 307.970636 206.803524 \nL 308.014168 206.846052 \nL 308.144762 207.394671 \nL 308.188293 207.347889 \nL 308.231825 207.458463 \nL 308.275356 207.634957 \nL 308.318887 207.552026 \nL 308.493013 207.356395 \nL 309.145984 207.305361 \nL 309.189515 208.177196 \nL 309.276578 208.092139 \nL 309.32011 207.913519 \nL 309.363641 207.964553 \nL 309.494235 208.100645 \nL 310.2778 208.13892 \nL 310.408395 208.240989 \nL 310.538989 208.160185 \nL 310.58252 208.185702 \nL 310.626052 208.104898 \nL 311.235491 208.121909 \nL 311.279023 208.22823 \nL 311.322554 208.092139 \nL 311.409617 207.943289 \nL 311.540211 208.334552 \nL 311.627274 208.334552 \nL 311.627274 208.334552 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_63\">\n    <path clip-path=\"url(#p1201b57bef)\" d=\"M 311.670805 208.334552 \nL 315.153317 208.334552 \nL 315.283911 208.07938 \nL 315.414506 208.91294 \nL 315.588631 208.857653 \nL 315.675694 208.997997 \nL 315.762757 208.929951 \nL 315.936882 208.94271 \nL 316.415728 208.94271 \nL 316.459259 208.785354 \nL 316.546322 208.861906 \nL 316.589853 208.827883 \nL 316.633385 208.895929 \nL 316.676916 208.985239 \nL 316.720448 208.946963 \nL 316.894573 208.900182 \nL 317.504013 208.836389 \nL 317.678138 208.326046 \nL 317.72167 208.581217 \nL 317.808733 208.547194 \nL 318.505235 208.547194 \nL 318.548766 208.070875 \nL 318.592298 208.313287 \nL 318.809955 208.623746 \nL 318.98408 208.564206 \nL 319.59352 208.564206 \nL 319.680583 208.866159 \nL 319.724114 208.730067 \nL 319.767646 208.844894 \nL 319.854708 208.806619 \nL 319.985303 208.687539 \nL 320.115897 208.802366 \nL 322.597187 208.802366 \nL 322.640718 207.182028 \nL 322.727781 207.607313 \nL 322.771312 208.104898 \nL 322.858375 207.994323 \nL 322.901906 207.666853 \nL 322.988969 207.853979 \nL 323.076032 207.879496 \nL 323.206626 207.798692 \nL 323.729003 207.836968 \nL 323.772534 207.973059 \nL 323.816066 207.741278 \nL 323.903129 207.777428 \nL 323.990191 207.909266 \nL 324.033723 207.849726 \nL 324.25138 207.815703 \nL 324.686694 207.815703 \nL 324.773757 207.675359 \nL 324.947882 201.738373 \nL 325.034945 202.291244 \nL 325.122008 203.039747 \nL 325.165539 202.712277 \nL 325.339665 202.380554 \nL 325.600853 202.380554 \nL 325.644385 200.377459 \nL 325.731447 201.01964 \nL 325.774979 200.72194 \nL 325.81851 200.755963 \nL 325.862042 201.364121 \nL 325.905573 201.34711 \nL 326.12323 199.339762 \nL 326.166761 199.475854 \nL 326.210293 198.32333 \nL 326.297356 198.467927 \nL 326.73267 198.552984 \nL 326.993858 201.712856 \nL 327.080921 201.036652 \nL 327.124452 200.21585 \nL 327.167984 201.236536 \nL 327.255046 201.296076 \nL 327.385641 201.01964 \nL 330.824621 201.01964 \nL 330.955216 198.786891 \nL 330.998747 200.00746 \nL 331.042278 198.944247 \nL 331.08581 198.999534 \nL 331.129341 200.271138 \nL 331.172873 199.480106 \nL 331.216404 198.463674 \nL 331.259935 198.918729 \nL 331.521124 199.913898 \nL 331.869375 199.913898 \nL 331.999969 200.241368 \nL 332.043501 199.990449 \nL 332.087032 200.224356 \nL 332.130563 200.169069 \nL 332.174095 200.653895 \nL 332.217626 199.594934 \nL 332.304689 199.620451 \nL 332.34822 199.492865 \nL 332.391752 199.692749 \nL 332.522346 199.828841 \nL 333.131786 199.930909 \nL 333.175317 199.956426 \nL 333.26238 198.3531 \nL 333.349443 198.361605 \nL 333.392974 199.080338 \nL 333.436505 198.034136 \nL 333.480037 198.051147 \nL 333.523568 197.829998 \nL 333.5671 197.026209 \nL 333.654162 197.04322 \nL 334.045945 197.149542 \nL 334.133008 197.085749 \nL 334.176539 197.511034 \nL 334.307133 194.130014 \nL 334.350665 186.294128 \nL 334.394196 192.939215 \nL 334.52479 187.584869 \nL 334.568322 189.196702 \nL 334.611853 189.23923 \nL 334.742447 190.387501 \nL 337.093143 190.387501 \nL 337.136674 171.509075 \nL 337.223737 176.170205 \nL 337.267269 177.352499 \nL 337.354331 178.198817 \nL 337.441394 166.54174 \nL 337.484926 186.806597 \nL 337.571988 183.519139 \nL 337.61552 183.672242 \nL 337.746114 181.056736 \nL 337.876708 178.534793 \nL 338.137897 178.534793 \nL 338.224959 176.353078 \nL 338.268491 178.692148 \nL 338.355554 178.458241 \nL 338.399085 179.24502 \nL 338.486148 179.038756 \nL 338.660273 154.663514 \nL 338.703805 120.874577 \nL 338.790868 120.96814 \nL 338.921462 127.228343 \nL 339.18265 127.228343 \nL 339.269713 67.054689 \nL 339.313244 132.170161 \nL 339.400307 118.088956 \nL 339.48737 57.698407 \nL 339.530901 83.381402 \nL 339.574433 70.669617 \nL 339.617964 77.43591 \nL 339.661496 81.089113 \nL 339.705027 69.606403 \nL 339.748558 77.150969 \nL 339.879153 79.813256 \nL 340.227404 79.813256 \nL 340.270935 73.859258 \nL 340.401529 21.123849 \nL 340.488592 17.083636 \nL 340.575655 107.869344 \nL 340.749781 132.36154 \nL 340.880375 68.755832 \nL 340.923906 83.645079 \nL 341.272157 83.645079 \nL 341.315689 35.18379 \nL 341.402752 48.337872 \nL 341.533346 81.357043 \nL 341.620409 70.971569 \nL 341.66394 76.827752 \nL 341.707471 98.527947 \nL 341.751003 95.974107 \nL 341.794534 76.836257 \nL 341.881597 86.188287 \nL 341.925128 82.058764 \nL 342.012191 83.215541 \nL 344.406418 83.215541 \nL 344.44995 68.330546 \nL 344.537012 73.008687 \nL 344.667607 101.50282 \nL 344.711138 115.439427 \nL 344.798201 110.663471 \nL 344.841732 110.008531 \nL 344.972326 128.061903 \nL 345.015858 140.629091 \nL 345.102921 136.482557 \nL 345.451172 136.482557 \nL 345.755892 177.416292 \nL 345.842954 167.485874 \nL 345.886486 172.002407 \nL 346.147674 180.095591 \nL 346.495925 180.095591 \nL 346.539457 179.24502 \nL 346.582988 175.502507 \nL 346.62652 177.441809 \nL 346.670051 178.11376 \nL 346.800645 174.128834 \nL 346.844177 176.544456 \nL 346.887708 173.814123 \nL 346.931239 170.781837 \nL 346.974771 174.677453 \nL 347.148896 178.224334 \nL 347.279491 178.50077 \nL 347.540679 178.50077 \nL 347.671273 175.872505 \nL 347.714805 176.140435 \nL 347.758336 175.957562 \nL 347.975993 190.783017 \nL 348.324244 195.418629 \nL 348.585433 195.418629 \nL 348.628964 190.442788 \nL 348.716027 191.344394 \nL 348.759558 191.199797 \nL 348.80309 192.041862 \nL 348.846621 181.129035 \nL 348.890152 186.147404 \nL 349.020747 190.447041 \nL 349.064278 190.421524 \nL 349.194872 186.768321 \nL 349.238404 187.750731 \nL 349.368998 188.065442 \nL 351.719693 188.065442 \nL 351.850288 185.943267 \nL 352.198539 191.293359 \nL 352.24207 190.829798 \nL 352.329133 190.982901 \nL 352.372664 190.902097 \nL 352.416196 191.02543 \nL 352.764447 191.02543 \nL 352.938573 192.726572 \nL 352.982104 192.216229 \nL 353.112698 195.703571 \nL 353.199761 195.558974 \nL 353.330355 194.215071 \nL 353.591544 195.065642 \nL 353.809201 195.065642 \nL 353.983326 194.576564 \nL 354.026858 194.64461 \nL 354.11392 195.163458 \nL 354.244515 192.373585 \nL 354.375109 195.299549 \nL 354.41864 194.980585 \nL 354.592766 195.108171 \nL 354.984548 195.108171 \nL 355.071611 194.942309 \nL 355.115143 194.151278 \nL 355.158674 195.014608 \nL 355.289268 195.652536 \nL 355.3328 195.627019 \nL 355.376331 194.682885 \nL 355.463394 194.963574 \nL 355.898708 194.938057 \nL 356.029302 194.640357 \nL 356.072834 194.52553 \nL 356.116365 194.682885 \nL 356.159896 194.721161 \nL 356.203428 193.500591 \nL 356.290491 194.159784 \nL 356.334022 194.487254 \nL 356.377553 194.474495 \nL 356.421085 194.074727 \nL 356.464616 194.2576 \nL 356.508148 194.31714 \nL 356.638742 193.832314 \nL 360.121254 193.832314 \nL 360.251848 193.687717 \nL 360.556568 195.384606 \nL 360.600099 195.278285 \nL 360.643631 195.376101 \nL 361.296602 195.401618 \nL 361.340133 195.252768 \nL 361.514259 196.435062 \nL 361.601321 197.060232 \nL 361.644853 196.77529 \nL 361.731916 196.817819 \nL 361.86251 196.724256 \nL 362.16723 196.724256 \nL 362.297824 196.469085 \nL 362.341355 196.766785 \nL 362.384887 195.797133 \nL 362.428418 196.303223 \nL 362.515481 197.655631 \nL 362.559012 196.724256 \nL 362.602544 197.37069 \nL 362.820201 199.105855 \nL 362.950795 199.233441 \nL 363.211983 199.233441 \nL 363.342577 198.604018 \nL 363.386109 198.910224 \nL 363.42964 198.859189 \nL 363.516703 198.157468 \nL 363.560234 198.510455 \nL 363.690829 199.726772 \nL 363.864954 198.106434 \nL 364.300268 198.093675 \nL 366.346244 198.093675 \nL 366.433307 196.128856 \nL 366.476838 197.40046 \nL 366.607432 196.681727 \nL 366.650964 198.212755 \nL 366.738027 197.591839 \nL 366.912152 196.192649 \nL 366.955684 196.532877 \nL 367.086278 196.74552 \nL 367.434529 196.702992 \nL 367.521592 197.702413 \nL 367.565123 197.685401 \nL 367.608655 197.46 \nL 367.652186 197.744941 \nL 367.78278 198.038388 \nL 367.869843 197.213334 \nL 367.913374 197.226093 \nL 368.000437 198.042641 \nL 368.043969 197.681148 \nL 368.0875 197.736436 \nL 368.0875 197.736436 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 224.64 \nL 33.2875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 368.0875 224.64 \nL 368.0875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 224.64 \nL 368.0875 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 7.2 \nL 368.0875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p1201b57bef\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAESCAYAAAAG+ZUXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAipklEQVR4nO3deZxcZZ3v8c+3qrfsoclCQiBhCVuQNQIKKoIKKgqOMoKouNxhnGFUHGcBda6iF2VmxG2QcdBBGWeuEK86IOoghoRh3DBhNRBICAECMfvWSbrTVfW7f5xT3ZWmu9NLVVdX5ft+vfp1tuec86vTVb966jnnPEcRgZmZ1ZdMtQMwM7Pyc3I3M6tDTu5mZnXIyd3MrA45uZuZ1SEndzOzOtRQ7QAApkyZEnPmzKl2GGZmNWXp0qUbI2Jqb8tGRXKfM2cOS5YsqXYYZmY1RdKzfS1zs4yZWR1ycjczq0NO7mZmdcjJ3cysDjm5m5nVISd3M7M65ORuZlYuhUK1I+ji5G5mVg4vLIXPHgCrFlc7EsDJ3cysPFbdlwyfvre6caSc3M3MyqGQS4bZpurGkXJyNzMrh/yeZOjkbmZWR/KdyTCTrW4cKSd3M7OyiGSg0ZFWR0cUZma1LqLaEezFyd3MrCyKyV1VjaLIyd3MrA45uZuZ1SEndzOzcpKbZczM6odPqJqZ1TPX3M3MrEKc3M3M6pCTu5lZORTb3H1C1cysHjm5m5lZhTi5m5mV088/Cbu3VjsKJ3czs/Iouc5986rqhZFycjczK4fSm5hGwQ1NTu5mZmXn5G5mVn+iUO0InNzNzMqjtFnGyd3MrP64zd3MrB7VUHKXlJX0kKS70ulWSfdIWpEODygpe42klZKelHReJQI3Mxu1aqxZ5qPAEyXTVwMLI2IusDCdRtJxwCXAPOB84CZJ2fKEa2Y2SkUNtrlLmgW8GfhWyewLgVvT8VuBi0rm3xYRHRHxDLASOK0s0ZqZjVq1eZ37V4C/AUq/jqZHxFqAdDgtnX8w8HxJuTXpPDOz/UQNJHdJFwDrI2LpALfZW5doL3mlkq6QtETSkg0bNgxw02ZmNaBGmmXOBN4qaTVwG3COpH8H1kmaAZAO16fl1wCHlKw/C3ix50Yj4uaImB8R86dOnTqMl2BmNgrUWpt7RFwTEbMiYg7JidJ7I+LdwJ3A5Wmxy4E70vE7gUskNUs6DJgLPFD2yM3MRqvqt8rQMIx1rwcWSPog8BxwMUBELJO0AHgcyAFXRkR+2JGamdWM6mf3QSX3iFgMLE7HNwHn9lHuOuC6YcZmZlabauhqGTMz61f0MV4dTu5mZuUwCmrrpZzczczKbRQkeid3M7Oyc3I3M6sTtdn9gJmZ9aVQgEdur3YUe3FyNzMbrkf+L3TuLJnhmruZWe3btWnvaTfLmJnVIyd3M7P645q7mZlVgpO7mVnZueZuZlZ/3CxjZlYPej6AzsndzKwOVD+Z9+TkbmY2XIu+sPe0m2XMzOpAbnePGQFbn4cbT4PtL3mE9IhwcjczK7cIWPpt2PgkPPQfVQnByd3MrNwiIJM+xbSQq0oITu5mZuW26Lru5B75qoTg5G5mVm5bnoFMNhl3zd3MrI4Ua+75zursvip7NTOrez1vbBpZTu5mZnXIyd3MrA45uZuZVYLSZpkq3a3q5G5mVoec3M3MKsInVM3M6pibZczM6o/b3M3M6ojcLGNmVn/u/kQyVHXSrJO7mVklzPujZDjtmKrs3sndzKwSXveZZDhaa+6SWiQ9IOkRScskXZvOb5V0j6QV6fCAknWukbRS0pOSzqvkCzAzG5WKSX0Un1DtAM6JiBOBk4DzJZ0BXA0sjIi5wMJ0GknHAZcA84DzgZskZSsQu5nZ6NV1h2qhKrvfZ3KPRFs62Zj+BXAhcGs6/1bgonT8QuC2iOiIiGeAlcBp5QzazGz0K14tM3pr7kjKSnoYWA/cExG/BaZHxFqAdDgtLX4w8HzJ6mvSeT23eYWkJZKWbNiwYRgvwcxsFKqFvmUiIh8RJwGzgNMkHd9P8d4u7nzJq4uImyNifkTMnzp16oCCNTOrHTVQcy+KiK3AYpK29HWSZgCkw/VpsTXAISWrzQJeHG6gZmY1ZbTX3CVNlTQ5HR8DvA5YDtwJXJ4Wuxy4Ix2/E7hEUrOkw4C5wANljtvMbHTrulqmOidUGwZQZgZwa3rFSwZYEBF3Sfo1sEDSB4HngIsBImKZpAXA40AOuDKiSo//NjOrmup2P7DP5B4RjwIn9zJ/E3BuH+tcB1w37OjMzGrVaG+WMTOzoaihE6pmZjZArrmbmdUhueZuZlaHRnn3A2ZmNkjv+r6bZczM6krjWDjqDfiEqplZPXLN3cysHrnmbmZWR9Kk7pq7mVk9cs3dzKz+uOZuZlZHum5eqm7HYU7uZmYV5Zq7mVntKzbDuFnGzKwepck9v6cqe3dyNzMrqx419/v+vipROLmbmZVTV0dhPqFqZlY/qtTG3pOTu5lZORVr7nLN3cysjhRr7k7uZmb1o+elkFXi5G5mVlY9kvukQ6oSRUNV9mpmVq9KH6s37ThoPbwqYbjmbmZWKcr4DlUzs7oj4b5lzMzqjTJ7N9OMICd3M7PhKPSTvJ3czcxqVT/NLk7uZmZ1yMndzKxG9Xc1jJO7mVkdcnI3M6tH8nXuZma1aV/NMqM0uUs6RNIiSU9IWibpo+n8Vkn3SFqRDg8oWecaSSslPSnpvEq+ADOzUUuCyFdl1wOpueeAj0fEscAZwJWSjgOuBhZGxFxgYTpNuuwSYB5wPnCTpGwlgjczG9U0iptlImJtRDyYju8AngAOBi4Ebk2L3QpclI5fCNwWER0R8QywEjitzHGbmY0O/SbvGul+QNIc4GTgt8D0iFgLyRcAMC0tdjDwfMlqa9J5Zmb7lyr26T7g5C5pPPAD4KqI2N5f0V7mveSrS9IVkpZIWrJhw4aBhmFmVluq1CwzoP7cJTWSJPb/iIgfprPXSZoREWslzQDWp/PXAKW9088CXuy5zYi4GbgZYP78+aPjibJmZoOWpq+ZJ8MJ74TZZ5YsG8XNMpIE/CvwRER8qWTRncDl6fjlwB0l8y+R1CzpMGAu8ED5QjYzG4WOfQuc8Wcw44TueVU8oTqQmvuZwHuAxyQ9nM77BHA9sEDSB4HngIsBImKZpAXA4yRX2lwZUaVrgczMqqp6Nfd9JveI+B/6foz3uX2scx1w3TDiMjOrDf3VzFctSrofaN8GLZNGLiZ8h6qZWZn0Ugcu9ivzh9+PbCg4uZuZjYCRb5pxcjczG5YBJO4q9Azp5G5mVg793bDk5G5mVkemH58MndzNzGpMf1fLXPDltIyTu5lZjeqlWUZpiq3CjUxO7mZmlVJsh3fN3cysjnTV3J3czcxqzL76c8fJ3cysZvV2KaTb3M3M6pCbZczMalR/tXIndzOzWtdfs4yTu5lZ/XByNzOrVQNplvEJVTOz2tTr1TK+FNLMrP64WcbMrEb5ahkzszpUTNzKvnSZk7uZWY3qSu69pFO3uZuZ1ahis0x/3Q/4GapmZrWmmNx7q7m7WcbMrDZ1Ncv4DlUzs/rRb5u7b2IyM6tNA0rurrmbmdUWXy1jZlaHXHM3M6tDXYnbJ1TNzOrDnp3wi2uTcdfczczqxP1fgmU/TMZ7S+7F2vzTi0YspCIndzOzocq1d4/3V3Nf5eRuZlY7Sq9f77f7gZHn5G5mNlSlben91dwBNq+qfDwl9pncJd0iab2k35fMa5V0j6QV6fCAkmXXSFop6UlJ51UqcDOzqlvzQPf4vmruXzu58vGUGEjN/TvA+T3mXQ0sjIi5wMJ0GknHAZcA89J1bpJ66+TYzKwOvLC0e7y/m5iqYJ/JPSL+G9jcY/aFwK3p+K3ARSXzb4uIjoh4BlgJnFaeUM3MRrFaS+59mB4RawHS4bR0/sHA8yXl1qTzXkLSFZKWSFqyYcOGIYZhZjZKVPHkaW/KHU1vX1O9docWETdHxPyImD916tQyh2FmNsIGktzbt1U+jtRQk/s6STMA0uH6dP4a4JCScrOAF4cenplZrRhAE8z1h1Y+jNRQk/udwOXp+OXAHSXzL5HULOkwYC7wQC/rm5nVl8hXO4K9NOyrgKTvAWcDUyStAT4NXA8skPRB4DngYoCIWCZpAfA4kAOujBhlr9jMrBJK71btS8vkiodRtM/kHhGX9rHo3D7KXwdcN5ygzMxqwsRZsH1NMr57a+9lZp3WfT38zJNGIirAd6iamQ1d09ju8ZPf03uZySWnISfPrmw8JZzczcyGa8aJ0NDU+7LzPg9vuxnGTYMHb4XbLhuRkJzczcyGqpBLhk0T+i4z4SA48Z3QOCaZXn5X5ePCyd3MbOiK1613DOD69cw+T3GWlZO7mdlQ7dqUDHN79l02M7LdbDm5m5kN15wz912mZ81950boaKtMPDi5m5kNzvMPwA3H7t2VQLaPk6mletbc//EI+MZZ5Y2tdHcV27KZWT1a+FnY8SKsug+mzUvmHTWAR1eU1ty3rE6Hz5Q9vK7dVWzLZmb1aPX9yfDez8HMk2H8QXDEOfterzS5f/XEysRWuruK78HMrB61pf0lDvQqmNLnFl14UzIcP728MZVwcjczG4r2rfDkTwb+QI7SLoFPvgyOuwja1kGhMt1vObmbmQ1KSTI/5HQ4/U8HttrO9XtPP/6fyXBRZbricnI3MxuU9PlDr/0UvOt2eOWHB7ba+IP2nv7z3yTD+2+A778fCoXyhcgAeoU0M7MSM06EtY/AtGMHt96Yycnw3E8nw9L1l/0Q5r8fFn0eTn1/Mi/XDg3NcOIlQwrTyd3MbDBOeS/85OMw6+WDW69lUjIc29o9b9xU2Jk+Q/rWtyTD536993qHn530TzNIbpYxMxuMXEcybGge3HrFpL5nZ/e8YmIv9d474cMPwgVfTqZvOBqe+PGgw3RyNzMbjG3pwzkax/ZfrqdTLodj3wJHv7F73ht6nEy96vdw+GvgwCPgxHd1z7/93fDkz5Lx1b+Ez0xK/vqhiBhcgBUwf/78WLJkSbXDMDPr3451cMNRyfintw78Msj+RPS9nY0r4cZTk/GWSXDlA/Ddt8H6xwHQtduXRsT83lZ1zd3MrNSKX8A3z0lOmhYt/Q585wJYckv3vHIk9n1tZ8qR8KYvJuPt25ImmjSx74uTu5lZqQXvgReWwiO3d8/78UeTbgfuuz6ZfvcPRy6e0/5k7+kLvgzHvhXe/KV+V3NyNzMrKuShc1cy/puvw7rHk7+ejjx3ZOO69LZkeN4XYP4H4J3fhZd/sN9VfCmkmVnRqkV7T//zK15a5mMDaxYpq6PfCJ8ZwNOeSji5m5kV/fvb956++Dt0dTcw5ywYN2WkIxoyJ3czs54+8hC0Hl7tKIbFyd3MrGjcVBg7peYTO/iEqplZt6ZxcNDLqh1FWTi5m1nNiQjyhb1vwPzur1fz/SXPD2/DuT3QMIDnodYAN8uYWU3Zkytw1KeSW/HPOWYaZx05hVyhwOd/uhyAry9ayX9eeSZ/2N7OMQdN7F6xsx0aW5LxXEd33zCd7XBdyRORGlpG4mVUnJO7mZVVLl/gyE/+jNkHjuUnH3kV45sb6MwX2LxzD5n0bsyJYxpoymbQEO7yvOr2h7rG712+nnuX7/0QjNWbdnHGZ+/iZ01Xs21cE5PGNMLmVcnCuefBiru7C7/sj5MufEsdOHfQMY1GTu5mVbJ7T57mhgyZzOAS3Ka2Du5fsZGZk8cASRPFtIktHDZlXCXC3Kft7Z1s3NHBOTfcB8BJh0wG4NlNuzj+03f3syb84i9fzZHTJvS5fGdHjo8veIR1O9o56ZDJfPuXq7uW3ffXZzOuuYGGjGjIZmjIiHXb2/ncXY8zoSHPI8uP4JXTp8CEZuhoS56EtKJHPI8tSP4Ajn87vPpvYNoxgz4Go5E7DjOrkjlX/wSAW943n3OOmd5Vu1385Hr+efHTrN60i7OPnsqMSS088vw2Hl+7nWMOmsDyP+zodXunzj6Af7r05K6kXw4duXxXDXvVhjZ++thajj5oIrl8gZ88tpaHntvKC1t377XO6Ye18ttnNu8179gZE9mTy/O+V87h6Q07+elja1m/o2OvMu8+41Amj2niF0+s49DWsazZspvH127vNa6bLjuFN71sxsBfSASsXAgbn0puCGo9DLY8C5GHbFPyN37awLc3Skjqs+MwJ3ezIfh/S9fwu2c2c/rhrVxwwkz+sK2dzkKB9ds72La7k92dOaaMb+aEgyczoaXhJbXzYtNF0fSJzazb3tFzN7Q0ZpjY0sjmnXsoRHDmkVOYNKaRCS0NvPllM5HgwWe3cMM9T3WtM2NSC+2deT5y7lwOmtjCU+va+PIvupcfPX0C63e009KYpaUxyzMbk/7FjzloAgeMbWL2gWNpzGZ47IVtPPz81n7ja2rIMGvyGF41dwqnzmnlDcdNp6Uxu8/jFxFc/I1fs+TZLf2Wax3XxMdefxR/PH8WzQ1Zfv30Jk6YNYlxzW50ACd324/l8gVWb9rJb1ZtZuqEZubNnEhGIiPRkcvz9IY25s9pZWJL40vWLRSCj3//EX700As0ZsXMyWNoymZYsb5tUDGMb25g8thG8oWgMx9sbOtg5qQWXtzWDsBbTpzJro4cC5ev5/Ap49i1J8+bT5jBB846jIMHWAsvFILfrd7MvcvX8/SGnfziiXX9ln/Tyw5ifHMDu/bkWbR8PTv35DnryCms2bKLXXvydOYLdOaDto4ckJy4zBWC1x07jeMPnkRLQ5bJYxvL8iuhmIMkERH8y3+v4rx5B1WtmamW9JfcK/b1J+l84KtAFvhWRFw/0HUjgmt//DhjmrL82dlH7PXB27a7k2UvbmPejElMGrv3B7JQCO5fuZFVG9oQMLapAQRt7ckbtLEhwwkHT2LKhOYBf2hGQkcuz472HK1jmwbd/ro/KhSCADrzBTISTQ0ZcvkCOzvybN61pytB/el3lw54m1MnNHPZ6YeSlXh6QxvL/7CDzTv3dDUddOaDxmyGI6aOZ8X6NlrHNXHtW+dxyy+f4en1bbz1pJm8fE4ry17cTkNGzDlwHNt2d7KjI8eaLUlHVA2Z5Evlwee2cPRBEzk5gqvOncvc6X23OQ9UJiNOP/xATj/8QACe3tBGW3uOhqzIZsTU8c0cOH6QTw4aIaUnVSXxodccUcVo6kdFau6SssBTwOuBNcDvgEsjotced2YcOS8+9OUFdOYLtHcW+NXKjV21mlKt45rYvHNP1/SB45rIFZLrXfOFYHdnfsAxtjRmaMxm0locZCQ27dxDc0OG0w5rpXVcEwIKkVx6taGtg6xEc2OGpmyGbEZ05gvsyRfozAX5CDpyeYSYObmFcU0NXbHlCgX25Ap05ArpT+EMnflgU1sHDz63teS4JT+Zc4WgUEi2mU/HO3IF2jpyZNMEoTTmbbs7ObR1LLMPHLvXa8lkSsZLypcuB9L4gly+QK4Q7MkVGNuUpbkhQ1P615DJEJEk1AgIgkIk4xBEQCEiXZaW6SqfDAsBz2/eRSGSJNmUzdA6rqnr2OQDOjrztHfmaevI0ZErUCi89L3Znivs9R4oyijZR28uPe0QpoxvZkxTlgPGJv/XINnfQ89v5ddPb3pJ+y8ktdXpE5v5xJuOZUJJBaNQCH8J26gw4s0ykl4BfCYizkunrwGIiC/0Vr55xtyYe8WNNDVkaMyK9s4C82cfQDYj5k4fz8Yde8gVgrFNWYJAiFwhyGagIZMk6GINJStxSOsYXj6nld2deXZ25Dlq+niEWLlhBxvb9rB87Q627e4kShJTIYINOzrIF4KNbXvYsitJIBkJAQeMa6Ixq64kXUyCjdnkS6KYPCX4w7Z22nN5GjLJl0ByNl80N2RpTxNYU0OWpoYMM9P20SXPbuHo6RNoHddEQzZJyA2ZJAlnlVwNMLGlIY01jbkQrNq4s+uncyFNqsmy7oRbfH3d40lzBdB1lUE2veIgIrp+kievNU+uEF2vTXS/Tih+aYDoXq5i2XR+Rsm8bEZMm9BMNiO27NrDzo7kZF1TesVIS0OGlsYs45sbaG5Ijl1P+UJw31MbuOyM2UQET6zdwawDxjCxpYHJY5sY15xl9oHjGN/cwI72HKfMnkxzw8DagCPo+kJtzPa+f7PRpBrNMgcDpbeKrQFO76vw0dMn8Oi151UolG6nzk4eUHvevME/SdzqW/ELKYMYwPlAs1GvUt0P9Fbl2esngqQrJC2RtGTblk0VCsPMbP9UqeS+BjikZHoW8GJpgYi4OSLmR8T8qVOnVigMM7P9U6WS+++AuZIOk9QEXALcWaF9mZlZDxVpc4+InKS/AO4muRTylohYVol9mZnZS1XsOveI+Cnw00pt38zM+ub+3M3M6pCTu5lZHXJyNzOrQ6Oi4zBJO4AnS2ZNArb1Uby/Zb0tnwJsrMB2B7qsdP8jtc/isr72Pdzt9qW35cUYqnHsJwGNDO34l2tZz//BSB774v47K7DdgSwb6WPf2/JyfP6Hc4wq/fk/OiJ675woue26un/Akh7TN/dTts9lvS0v3XY5tzuIZUuqsM+b+9v3cLc7mOXFGKp07G8e6vEv17JyvbeHeoyAJfvLsd/X8R/pY9/b/7/cx76/7Y/WZpkfD3HZcNat1HaHus3h7NPbrd526+m1eLvD3+dw1htWzhktzTJLoo/Ob0bztkf7/qv92kdDDN6/33/1fPz72/5oqbnfXKPbHu37r/Zrh+rH4P3vn/suqnYMld5/n9sfFTV3MzMrr9FSczczszJycjczq0M1m9wlhaTvlkw3SNog6a4qxTO4pyZXIQZJiyWV9eSOpLel/4tjyrndQcbwSUnLJD0q6WFJfT4YpkL7nyXpDkkrJD0t6atpb6h9lb9K0tgy7Dck3VAy/VeSPjPc7Q5i//n0eC+T9Iikv5RUlZxSrc9fyTEo/s3pp2zZP3/9qdnkDuwEjpdUfNL164EXqhjP/upS4H9IunUecekjHS8ATomIE4DXsfdTwCq9fwE/BP4zIuYCRwHjgev6We0qYNjJHegA/kjSlDJsayh2R8RJETGP5PP3JuDTVYqlWorHoPi3utoBFdVycgf4GfDmdPxS4HvFBZJOk/QrSQ+lw6PT+fdLOqmk3C8lnVCOYCSdXfrLQdKNkt6Xjq+WdK2kByU9Vqmabn8xVGBf44EzgQ+SJvd9HIM3SVou6X8kfa1Mv7JmABsjogMgIjZGxIuSTpV0n6Slku6WNCONYbGkr6Tvid9LOm2Y+z8HaI+Ib6f7zwMfAz4gaZykL6b/70clfVjSR4CZwCJJi4a57xzJ1RIf67lA0mxJC9P9LpR0qKRJ6fswk5YZK+l5SY091x+siFgPXAH8hRJZSf8o6XdpDH9aEtvfpMfkEUnXD3ffJdsdn77W4mfswnT+HElPSPpm+ivj5yWVwrLr672XencZ33v9qvXkfhtwiaQW4ATgtyXLlgOvjoiTgf8NfD6d/y3gfQCSjgKaI+LREYp3Y0ScAvwz8FcjtM9Kugj4r4h4Ctgs6ZS+Cqb/o38B3hgRZwHlevzWz4FDJD0l6SZJr0mT1T8B74iIU4Fb2LsmPS4iXgn8ebpsOOYBS0tnRMR24DngfwGHASenvyr+IyK+RvJUstdGxGuHuW+ArwOXSZrUY/6NwL8V9wt8LSK2AY8Ar0nLvAW4OyI6yxAHEbGKJKdMI/nC3xYRLwdeDvyJkof3vJHkfXN6RJwI/EM59p1qB96WfsZeC9yQ/rICmAt8Pf2VsRV4e5n2OaakSeZHI/ze61fF+nMfCRHxaNrGdSkv7Tt+EnCrpLkkz28t1k6+D/ydpL8GPgB8Z2SiBZKf75Akgz8awf1WyqXAV9Lx29Lpn/RR9hhgVUQ8k05/j6SmNywR0SbpVOBVJB/o24H/AxwP3JN+trPA2pLVvpeu+9+SJkqaHBFbhxiC6PF84JL5rwa+ERG5dH+bh7iPPkXEdkn/BnwE2F2y6BV0v8e+S3cSvR14J7CI5NfWTWUOqZhM3wCcIOkd6fQkkgT7OuDbEbErjb+cx0TA5yW9GigABwPT02XPRMTD6fhSYE6Z9rk7Ik7qCkA6npF77/WrppN76k7gi8DZwIEl8z8HLIqIt6VfAIsBImKXpHuAC4E/Bsp5giPH3r+GWnos70iHeSp37PcVQ1lIOpCkSeJ4SUHyJg6S/0dv++/toellkTaFLAYWS3oMuBJYFhGv6GuVfUwPxjJ61AIlTSR5hvCqYW57oL4CPAh8u58yxTjuBL4gqRU4Fbi3XEFIOpzkvb2e5P/94Yi4u0eZ86ncMbmM5BfhqRHRKWk13e+/jpJyeaBSzTJi5N57/ar1ZhlIftp8NiIe6zF/Et0nWN/XY9m3gK8BvytzzeFZ4DhJzenP5HPLuO3RFsM7SH72z46IORFxCFCslfe2/+XA4eq+muCd5QhC0tHpr7Oik4AngKlKTrYiqVHSvJIy70znn0XSdNBfz3z7shAYK+m96TazwA0kvwh/DnxIUkO6rDVdZwfQe09+Q5C+hxeQNIUU/Yruk9yXkZz0JiLagAeArwJ3pV+MwyZpKvAN4MZI7oy8G/izYnu+pKMkjSM5Jh9QerVQyTEph0nA+jSxvxaYXcZtD9STjNx7r181X3OPiDUkb9Se/oGkWeYv6VE7iYilkrbTf01nwNIPb0dEPC9pAfAosAJ4qBzbH6UxXAr0PBn2A+BdJIlmr/1HxG5Jfw78l6SNJAmmHMYD/yRpMsmvlpUkzT03A19Lv2AaSGq3xef4bpH0K2AiSdPckEVESHobcJOkvyOpMP0U+ARJDfEo4FFJncA3SdrCbwZ+JmltmdrdIflC+YuS6Y8At6TNjxuA95csu52kefLsYe5zjKSHSZo8cyTNP19Kl32LpOnjwbTdewNwUUT8l5ILGpZI2kP3sRqy4nuf5NzCjyUtAR4mqVCMqIjYkzZFVfy9ty/7ZfcDkmaS/Iw/JiIKZdjeicA3I6KiZ79Hewz7Iml82kYukhOBKyLiyyMcw2LgryJiyUju1yqnFt771VAPzTKDkv58/i3wyTIl9g+RnCT51HC3VcsxDNCfpDW9ZSQ/of+luuFYrauh9/6I2y9r7mZm9W6/q7mbWe2SdIikRelNScskfTSd3yrpHiVdQNwj6YB0/uuV3Ez0WDo8p2Rb1ym5iavqXYdUgmvuZlYzlNztOSMiHpQ0geSa9YtIrojbHBHXS7oaOCAi/lbSycC69K7l40lu2jo43dYZJFeXrYiI8dV4PZXk5G5mNUvSHSRXIN0InB0Ra9MvgMURcXSPsiJ5WPXMYncV6fy2ekzubpYxs5qU3jNxMskFEtMjYi1AOpzWyypvBx4qTez1rOavczez/Y+STut+AFyVdsGwr/LzgL8n6RZhv+Cau5nVlPSu1x+QdMRW7K9pnbp7/pxB0gVCsfws4EfAeyPi6ZGOt1qc3M2sZqTt5v8KPBERXypZdCdweTp+OXBHWn4ySWd210TEL0cw1KrzCVUzqxlpnyz3A4+R9PwISfcFvyXp9uJQku6WL46IzZI+BVxD0hVG0RsiYr2kfyDpLmMmSTfM34qIz4zICxkBTu5mZnXIzTJmZnXIyd3MrA45uZuZ1SEndzOzOuTkbmZWh5zcbb8kaY6k31c7DrNKcXI3K5Pis1LNRgMnd9ufZSV9M+0X/OeSxkg6SdJvJD0q6Ucl/YIvljQ/HZ8iaXU6/j5J35f0Y5KHP5uNCk7utj+bC3w9IuYBW0l6Dfw34G8j4gSSuyA/PYDtvAK4PCLO2WdJsxHi5G77s2ci4uF0fClwBDA5Iu5L590KvHoA27knIjZXID6zIXNyt/1Zab/eeWByP2VzdH9eWnos21nGmMzKwsndrNs2YIukV6XT7wGKtfjVwKnp+DtGOC6zQfPZfbO9XQ58Q9JYYBXw/nT+F4EFkt4D3Fut4MwGyr1CmpnVITfLmJnVISd3M7M65ORuZlaHnNzNzOqQk7uZWR1ycjczq0NO7mZmdcjJ3cysDv1/hJRpqB7gJoEAAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "df_train['closeprice'].plot()\n",
    "df_target['closeprice'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "openprice     0\n",
       "highprice     0\n",
       "lowprice      0\n",
       "volume        0\n",
       "closeprice    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# finding missing values\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "openprice     0\n",
       "highprice     0\n",
       "lowprice      0\n",
       "volume        0\n",
       "closeprice    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df_target.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 864x432 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"370.91625pt\" version=\"1.1\" viewBox=\"0 0 717.403125 370.91625\" width=\"717.403125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-04-09T22:26:34.266356</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 370.91625 \nL 717.403125 370.91625 \nL 717.403125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 333.36 \nL 710.203125 333.36 \nL 710.203125 7.2 \nL 40.603125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PolyCollection_1\">\n    <defs>\n     <path d=\"M 107.894887 -52.381705 \nL 107.231363 -52.381705 \nL 106.84482 -52.757618 \nL 106.117364 -53.133531 \nL 104.856419 -53.509444 \nL 102.846449 -53.885357 \nL 99.9068 -54.261271 \nL 95.975805 -54.637184 \nL 91.196124 -55.013097 \nL 85.961431 -55.38901 \nL 80.886836 -55.764923 \nL 76.691047 -56.140837 \nL 74.018366 -56.51675 \nL 73.262476 -56.892663 \nL 74.460299 -57.268576 \nL 77.296808 -57.644489 \nL 81.214193 -58.020403 \nL 85.576492 -58.396316 \nL 89.824914 -58.772229 \nL 93.574336 -59.148142 \nL 96.635475 -59.524055 \nL 98.97993 -59.899969 \nL 100.681025 -60.275882 \nL 101.859193 -60.651795 \nL 102.644825 -61.027708 \nL 103.156656 -61.403621 \nL 103.488144 -61.779534 \nL 103.697769 -62.155448 \nL 103.805563 -62.531361 \nL 103.800542 -62.907274 \nL 103.659699 -63.283187 \nL 103.371908 -63.6591 \nL 102.955511 -64.035014 \nL 102.460487 -64.410927 \nL 101.954152 -64.78684 \nL 101.498184 -65.162753 \nL 101.128789 -65.538666 \nL 100.848932 -65.91458 \nL 100.634443 -66.290493 \nL 100.449363 -66.666406 \nL 100.263319 -67.042319 \nL 100.064778 -67.418232 \nL 99.866352 -67.794146 \nL 99.700529 -68.170059 \nL 99.606676 -68.545972 \nL 99.613566 -68.921885 \nL 99.724772 -69.297798 \nL 99.914527 -69.673712 \nL 100.137299 -70.049625 \nL 100.347212 -70.425538 \nL 100.517774 -70.801451 \nL 100.651941 -71.177364 \nL 100.777753 -71.553278 \nL 100.932381 -71.929191 \nL 101.143122 -72.305104 \nL 101.414605 -72.681017 \nL 101.727471 -73.05693 \nL 102.047539 -73.432844 \nL 102.339646 -73.808757 \nL 102.57941 -74.18467 \nL 102.758947 -74.560583 \nL 102.886718 -74.936496 \nL 102.984156 -75.31241 \nL 103.081419 -75.688323 \nL 103.212673 -76.064236 \nL 103.410029 -76.440149 \nL 103.695965 -76.816062 \nL 104.075959 -77.191976 \nL 104.534381 -77.567889 \nL 105.036048 -77.943802 \nL 105.533518 -78.319715 \nL 105.977682 -78.695628 \nL 106.328029 -79.071542 \nL 106.55959 -79.447455 \nL 106.665342 -79.823368 \nL 106.654627 -80.199281 \nL 106.549069 -80.575194 \nL 106.377621 -80.951108 \nL 106.172068 -81.327021 \nL 105.963754 -81.702934 \nL 105.781644 -82.078847 \nL 105.651026 -82.45476 \nL 105.591835 -82.830674 \nL 105.61606 -83.206587 \nL 105.724923 -83.5825 \nL 105.907461 -83.958413 \nL 106.142025 -84.334326 \nL 106.400865 -84.71024 \nL 106.656332 -85.086153 \nL 106.886461 -85.462066 \nL 107.078127 -85.837979 \nL 107.227257 -86.213892 \nL 107.336782 -86.589806 \nL 107.413597 -86.965719 \nL 107.465743 -87.341632 \nL 107.500488 -87.717545 \nL 107.523459 -88.093458 \nL 107.538579 -88.469372 \nL 107.548429 -88.845285 \nL 107.554699 -89.221198 \nL 107.558539 -89.597111 \nL 107.567711 -89.597111 \nL 107.567711 -89.597111 \nL 107.571551 -89.221198 \nL 107.577821 -88.845285 \nL 107.587671 -88.469372 \nL 107.602791 -88.093458 \nL 107.625762 -87.717545 \nL 107.660507 -87.341632 \nL 107.712653 -86.965719 \nL 107.789468 -86.589806 \nL 107.898993 -86.213892 \nL 108.048123 -85.837979 \nL 108.239789 -85.462066 \nL 108.469918 -85.086153 \nL 108.725385 -84.71024 \nL 108.984225 -84.334326 \nL 109.218789 -83.958413 \nL 109.401327 -83.5825 \nL 109.51019 -83.206587 \nL 109.534415 -82.830674 \nL 109.475224 -82.45476 \nL 109.344606 -82.078847 \nL 109.162496 -81.702934 \nL 108.954182 -81.327021 \nL 108.748629 -80.951108 \nL 108.577181 -80.575194 \nL 108.471623 -80.199281 \nL 108.460908 -79.823368 \nL 108.56666 -79.447455 \nL 108.798221 -79.071542 \nL 109.148568 -78.695628 \nL 109.592732 -78.319715 \nL 110.090202 -77.943802 \nL 110.591869 -77.567889 \nL 111.050291 -77.191976 \nL 111.430285 -76.816062 \nL 111.716221 -76.440149 \nL 111.913577 -76.064236 \nL 112.044831 -75.688323 \nL 112.142094 -75.31241 \nL 112.239532 -74.936496 \nL 112.367303 -74.560583 \nL 112.54684 -74.18467 \nL 112.786604 -73.808757 \nL 113.078711 -73.432844 \nL 113.398779 -73.05693 \nL 113.711645 -72.681017 \nL 113.983128 -72.305104 \nL 114.193869 -71.929191 \nL 114.348497 -71.553278 \nL 114.474309 -71.177364 \nL 114.608476 -70.801451 \nL 114.779038 -70.425538 \nL 114.988951 -70.049625 \nL 115.211723 -69.673712 \nL 115.401478 -69.297798 \nL 115.512684 -68.921885 \nL 115.519574 -68.545972 \nL 115.425721 -68.170059 \nL 115.259898 -67.794146 \nL 115.061472 -67.418232 \nL 114.862931 -67.042319 \nL 114.676887 -66.666406 \nL 114.491807 -66.290493 \nL 114.277318 -65.91458 \nL 113.997461 -65.538666 \nL 113.628066 -65.162753 \nL 113.172098 -64.78684 \nL 112.665763 -64.410927 \nL 112.170739 -64.035014 \nL 111.754342 -63.6591 \nL 111.466551 -63.283187 \nL 111.325708 -62.907274 \nL 111.320687 -62.531361 \nL 111.428481 -62.155448 \nL 111.638106 -61.779534 \nL 111.969594 -61.403621 \nL 112.481425 -61.027708 \nL 113.267057 -60.651795 \nL 114.445225 -60.275882 \nL 116.14632 -59.899969 \nL 118.490775 -59.524055 \nL 121.551914 -59.148142 \nL 125.301336 -58.772229 \nL 129.549758 -58.396316 \nL 133.912057 -58.020403 \nL 137.829442 -57.644489 \nL 140.665951 -57.268576 \nL 141.863774 -56.892663 \nL 141.107884 -56.51675 \nL 138.435203 -56.140837 \nL 134.239414 -55.764923 \nL 129.164819 -55.38901 \nL 123.930126 -55.013097 \nL 119.150445 -54.637184 \nL 115.21945 -54.261271 \nL 112.279801 -53.885357 \nL 110.269831 -53.509444 \nL 109.008886 -53.133531 \nL 108.28143 -52.757618 \nL 107.894887 -52.381705 \nz\n\" id=\"md5df7e4c62\" style=\"stroke:#3d3d3d;stroke-width:1.5;\"/>\n    </defs>\n    <g clip-path=\"url(#p8ef8af6472)\">\n     <use style=\"fill:#3274a1;stroke:#3d3d3d;stroke-width:1.5;\" x=\"0\" xlink:href=\"#md5df7e4c62\" y=\"370.91625\"/>\n    </g>\n   </g>\n   <g id=\"PolyCollection_2\">\n    <defs>\n     <path d=\"M 242.271496 -52.782067 \nL 240.694754 -52.782067 \nL 239.887623 -53.166767 \nL 238.489968 -53.551467 \nL 236.273965 -53.936167 \nL 233.065331 -54.320867 \nL 228.839835 -54.705567 \nL 223.812491 -55.090267 \nL 218.471556 -55.474967 \nL 213.518667 -55.859667 \nL 209.712871 -56.244368 \nL 207.664871 -56.629068 \nL 207.660295 -57.013768 \nL 209.584879 -57.398468 \nL 212.980124 -57.783168 \nL 217.199467 -58.167868 \nL 221.594827 -58.552568 \nL 225.661559 -58.937268 \nL 229.102209 -59.321968 \nL 231.812993 -59.706669 \nL 233.826646 -60.091369 \nL 235.2493 -60.476069 \nL 236.21333 -60.860769 \nL 236.848595 -61.245469 \nL 237.264194 -61.630169 \nL 237.53474 -62.014869 \nL 237.69259 -62.399569 \nL 237.731727 -62.784269 \nL 237.62591 -63.16897 \nL 237.35563 -63.55367 \nL 236.931734 -63.93837 \nL 236.403743 -64.32307 \nL 235.848406 -64.70777 \nL 235.344482 -65.09247 \nL 234.946532 -65.47717 \nL 234.669876 -65.86187 \nL 234.491995 -66.24657 \nL 234.367579 -66.631271 \nL 234.249588 -67.015971 \nL 234.10823 -67.400671 \nL 233.941856 -67.785371 \nL 233.776553 -68.170071 \nL 233.654454 -68.554771 \nL 233.61484 -68.939471 \nL 233.675902 -69.324171 \nL 233.825882 -69.708871 \nL 234.028406 -70.093572 \nL 234.23953 -70.478272 \nL 234.427675 -70.862972 \nL 234.586191 -71.247672 \nL 234.732721 -71.632372 \nL 234.896862 -72.017072 \nL 235.1035 -72.401772 \nL 235.360332 -72.786472 \nL 235.654672 -73.171172 \nL 235.959225 -73.555873 \nL 236.242712 -73.940573 \nL 236.480577 -74.325273 \nL 236.66279 -74.709973 \nL 236.797615 -75.094673 \nL 236.910944 -75.479373 \nL 237.040879 -75.864073 \nL 237.228003 -76.248773 \nL 237.503483 -76.633473 \nL 237.878698 -77.018174 \nL 238.340168 -77.402874 \nL 238.851705 -77.787574 \nL 239.362964 -78.172274 \nL 239.821221 -78.556974 \nL 240.182472 -78.941674 \nL 240.418876 -79.326374 \nL 240.521547 -79.711074 \nL 240.499484 -80.095774 \nL 240.376229 -80.480475 \nL 240.185554 -80.865175 \nL 239.966765 -81.249875 \nL 239.759778 -81.634575 \nL 239.600095 -82.019275 \nL 239.51414 -82.403975 \nL 239.515764 -82.788675 \nL 239.604816 -83.173375 \nL 239.768347 -83.558075 \nL 239.984286 -83.942776 \nL 240.226588 -84.327476 \nL 240.470406 -84.712176 \nL 240.695981 -85.096876 \nL 240.89063 -85.481576 \nL 241.048828 -85.866276 \nL 241.170925 -86.250976 \nL 241.261158 -86.635676 \nL 241.32563 -87.020376 \nL 241.370704 -87.405077 \nL 241.402012 -87.789777 \nL 241.424017 -88.174477 \nL 241.439934 -88.559177 \nL 241.451869 -88.943877 \nL 241.461053 -89.328577 \nL 241.468132 -89.713277 \nL 241.473449 -90.097977 \nL 241.477247 -90.482677 \nL 241.479788 -90.867378 \nL 241.486462 -90.867378 \nL 241.486462 -90.867378 \nL 241.489003 -90.482677 \nL 241.492801 -90.097977 \nL 241.498118 -89.713277 \nL 241.505197 -89.328577 \nL 241.514381 -88.943877 \nL 241.526316 -88.559177 \nL 241.542233 -88.174477 \nL 241.564238 -87.789777 \nL 241.595546 -87.405077 \nL 241.64062 -87.020376 \nL 241.705092 -86.635676 \nL 241.795325 -86.250976 \nL 241.917422 -85.866276 \nL 242.07562 -85.481576 \nL 242.270269 -85.096876 \nL 242.495844 -84.712176 \nL 242.739662 -84.327476 \nL 242.981964 -83.942776 \nL 243.197903 -83.558075 \nL 243.361434 -83.173375 \nL 243.450486 -82.788675 \nL 243.45211 -82.403975 \nL 243.366155 -82.019275 \nL 243.206472 -81.634575 \nL 242.999485 -81.249875 \nL 242.780696 -80.865175 \nL 242.590021 -80.480475 \nL 242.466766 -80.095774 \nL 242.444703 -79.711074 \nL 242.547374 -79.326374 \nL 242.783778 -78.941674 \nL 243.145029 -78.556974 \nL 243.603286 -78.172274 \nL 244.114545 -77.787574 \nL 244.626082 -77.402874 \nL 245.087552 -77.018174 \nL 245.462767 -76.633473 \nL 245.738247 -76.248773 \nL 245.925371 -75.864073 \nL 246.055306 -75.479373 \nL 246.168635 -75.094673 \nL 246.30346 -74.709973 \nL 246.485673 -74.325273 \nL 246.723538 -73.940573 \nL 247.007025 -73.555873 \nL 247.311578 -73.171172 \nL 247.605918 -72.786472 \nL 247.86275 -72.401772 \nL 248.069388 -72.017072 \nL 248.233529 -71.632372 \nL 248.380059 -71.247672 \nL 248.538575 -70.862972 \nL 248.72672 -70.478272 \nL 248.937844 -70.093572 \nL 249.140368 -69.708871 \nL 249.290348 -69.324171 \nL 249.35141 -68.939471 \nL 249.311796 -68.554771 \nL 249.189697 -68.170071 \nL 249.024394 -67.785371 \nL 248.85802 -67.400671 \nL 248.716662 -67.015971 \nL 248.598671 -66.631271 \nL 248.474255 -66.24657 \nL 248.296374 -65.86187 \nL 248.019718 -65.47717 \nL 247.621768 -65.09247 \nL 247.117844 -64.70777 \nL 246.562507 -64.32307 \nL 246.034516 -63.93837 \nL 245.61062 -63.55367 \nL 245.34034 -63.16897 \nL 245.234523 -62.784269 \nL 245.27366 -62.399569 \nL 245.43151 -62.014869 \nL 245.702056 -61.630169 \nL 246.117655 -61.245469 \nL 246.75292 -60.860769 \nL 247.71695 -60.476069 \nL 249.139604 -60.091369 \nL 251.153257 -59.706669 \nL 253.864041 -59.321968 \nL 257.304691 -58.937268 \nL 261.371423 -58.552568 \nL 265.766783 -58.167868 \nL 269.986126 -57.783168 \nL 273.381371 -57.398468 \nL 275.305955 -57.013768 \nL 275.301379 -56.629068 \nL 273.253379 -56.244368 \nL 269.447583 -55.859667 \nL 264.494694 -55.474967 \nL 259.153759 -55.090267 \nL 254.126415 -54.705567 \nL 249.900919 -54.320867 \nL 246.692285 -53.936167 \nL 244.476282 -53.551467 \nL 243.078627 -53.166767 \nL 242.271496 -52.782067 \nz\n\" id=\"m919d79c2de\" style=\"stroke:#3d3d3d;stroke-width:1.5;\"/>\n    </defs>\n    <g clip-path=\"url(#p8ef8af6472)\">\n     <use style=\"fill:#e1812c;stroke:#3d3d3d;stroke-width:1.5;\" x=\"0\" xlink:href=\"#m919d79c2de\" y=\"370.91625\"/>\n    </g>\n   </g>\n   <g id=\"PolyCollection_3\">\n    <defs>\n     <path d=\"M 375.743996 -52.419115 \nL 375.062254 -52.419115 \nL 374.668108 -52.793401 \nL 373.928825 -53.167687 \nL 372.650848 -53.541973 \nL 370.618018 -53.916259 \nL 367.649575 -54.290544 \nL 363.684379 -54.66483 \nL 358.866479 -55.039116 \nL 353.592195 -55.413402 \nL 348.480918 -55.787688 \nL 344.257151 -56.161974 \nL 341.571486 -56.53626 \nL 340.822551 -56.910545 \nL 342.048958 -57.284831 \nL 344.933108 -57.659117 \nL 348.910761 -58.033403 \nL 353.337385 -58.407689 \nL 357.645787 -58.781975 \nL 361.444702 -59.156261 \nL 364.542115 -59.530547 \nL 366.910079 -59.904832 \nL 368.624107 -60.279118 \nL 369.806693 -60.653404 \nL 370.589069 -61.02769 \nL 371.090007 -61.401976 \nL 371.40376 -61.776262 \nL 371.591748 -62.150548 \nL 371.678965 -62.524833 \nL 371.659552 -62.899119 \nL 371.513255 -63.273405 \nL 371.227881 -63.647691 \nL 370.817645 -64.021977 \nL 370.327975 -64.396263 \nL 369.824066 -64.770549 \nL 369.369026 -65.144834 \nL 369.002544 -65.51912 \nL 368.729946 -65.893406 \nL 368.52562 -66.267692 \nL 368.348182 -66.641978 \nL 368.160572 -67.016264 \nL 367.947409 -67.39055 \nL 367.723306 -67.764835 \nL 367.528644 -68.139121 \nL 367.413483 -68.513407 \nL 367.415346 -68.887693 \nL 367.54049 -69.261979 \nL 367.758078 -69.636265 \nL 368.011181 -70.010551 \nL 368.240153 -70.384837 \nL 368.407381 -70.759122 \nL 368.511687 -71.133408 \nL 368.586077 -71.507694 \nL 368.68074 -71.88198 \nL 368.839923 -72.256266 \nL 369.083258 -72.630552 \nL 369.398994 -73.004838 \nL 369.750432 -73.379123 \nL 370.091041 -73.753409 \nL 370.380972 -74.127695 \nL 370.59884 -74.501981 \nL 370.746101 -74.876267 \nL 370.844747 -75.250553 \nL 370.930594 -75.624839 \nL 371.044422 -75.999124 \nL 371.222665 -76.37341 \nL 371.489125 -76.747696 \nL 371.849275 -77.121982 \nL 372.288469 -77.496268 \nL 372.774603 -77.870554 \nL 373.264626 -78.24484 \nL 373.713247 -78.619126 \nL 374.081628 -78.993411 \nL 374.344012 -79.367697 \nL 374.49093 -79.741983 \nL 374.52866 -80.116269 \nL 374.475522 -80.490555 \nL 374.356341 -80.864841 \nL 374.196835 -81.239127 \nL 374.019811 -81.613412 \nL 373.844372 -81.987698 \nL 373.687825 -82.361984 \nL 373.568265 -82.73627 \nL 373.505007 -83.110556 \nL 373.515327 -83.484842 \nL 373.608515 -83.859128 \nL 373.780599 -84.233413 \nL 374.01314 -84.607699 \nL 374.277353 -84.981985 \nL 374.541816 -85.356271 \nL 374.7803 -85.730557 \nL 374.976684 -86.104843 \nL 375.125805 -86.479129 \nL 375.231084 -86.853414 \nL 375.300741 -87.2277 \nL 375.344302 -87.601986 \nL 375.370294 -87.976272 \nL 375.38524 -88.350558 \nL 375.393598 -88.724844 \nL 375.398174 -89.09913 \nL 375.400629 -89.473416 \nL 375.405621 -89.473416 \nL 375.405621 -89.473416 \nL 375.408076 -89.09913 \nL 375.412652 -88.724844 \nL 375.42101 -88.350558 \nL 375.435956 -87.976272 \nL 375.461948 -87.601986 \nL 375.505509 -87.2277 \nL 375.575166 -86.853414 \nL 375.680445 -86.479129 \nL 375.829566 -86.104843 \nL 376.02595 -85.730557 \nL 376.264434 -85.356271 \nL 376.528897 -84.981985 \nL 376.79311 -84.607699 \nL 377.025651 -84.233413 \nL 377.197735 -83.859128 \nL 377.290923 -83.484842 \nL 377.301243 -83.110556 \nL 377.237985 -82.73627 \nL 377.118425 -82.361984 \nL 376.961878 -81.987698 \nL 376.786439 -81.613412 \nL 376.609415 -81.239127 \nL 376.449909 -80.864841 \nL 376.330728 -80.490555 \nL 376.27759 -80.116269 \nL 376.31532 -79.741983 \nL 376.462238 -79.367697 \nL 376.724622 -78.993411 \nL 377.093003 -78.619126 \nL 377.541624 -78.24484 \nL 378.031647 -77.870554 \nL 378.517781 -77.496268 \nL 378.956975 -77.121982 \nL 379.317125 -76.747696 \nL 379.583585 -76.37341 \nL 379.761828 -75.999124 \nL 379.875656 -75.624839 \nL 379.961503 -75.250553 \nL 380.060149 -74.876267 \nL 380.20741 -74.501981 \nL 380.425278 -74.127695 \nL 380.715209 -73.753409 \nL 381.055818 -73.379123 \nL 381.407256 -73.004838 \nL 381.722992 -72.630552 \nL 381.966327 -72.256266 \nL 382.12551 -71.88198 \nL 382.220173 -71.507694 \nL 382.294563 -71.133408 \nL 382.398869 -70.759122 \nL 382.566097 -70.384837 \nL 382.795069 -70.010551 \nL 383.048172 -69.636265 \nL 383.26576 -69.261979 \nL 383.390904 -68.887693 \nL 383.392767 -68.513407 \nL 383.277606 -68.139121 \nL 383.082944 -67.764835 \nL 382.858841 -67.39055 \nL 382.645678 -67.016264 \nL 382.458068 -66.641978 \nL 382.28063 -66.267692 \nL 382.076304 -65.893406 \nL 381.803706 -65.51912 \nL 381.437224 -65.144834 \nL 380.982184 -64.770549 \nL 380.478275 -64.396263 \nL 379.988605 -64.021977 \nL 379.578369 -63.647691 \nL 379.292995 -63.273405 \nL 379.146698 -62.899119 \nL 379.127285 -62.524833 \nL 379.214502 -62.150548 \nL 379.40249 -61.776262 \nL 379.716243 -61.401976 \nL 380.217181 -61.02769 \nL 380.999557 -60.653404 \nL 382.182143 -60.279118 \nL 383.896171 -59.904832 \nL 386.264135 -59.530547 \nL 389.361548 -59.156261 \nL 393.160463 -58.781975 \nL 397.468865 -58.407689 \nL 401.895489 -58.033403 \nL 405.873142 -57.659117 \nL 408.757292 -57.284831 \nL 409.983699 -56.910545 \nL 409.234764 -56.53626 \nL 406.549099 -56.161974 \nL 402.325332 -55.787688 \nL 397.214055 -55.413402 \nL 391.939771 -55.039116 \nL 387.121871 -54.66483 \nL 383.156675 -54.290544 \nL 380.188232 -53.916259 \nL 378.155402 -53.541973 \nL 376.877425 -53.167687 \nL 376.138142 -52.793401 \nL 375.743996 -52.419115 \nz\n\" id=\"m72d5e70c56\" style=\"stroke:#3d3d3d;stroke-width:1.5;\"/>\n    </defs>\n    <g clip-path=\"url(#p8ef8af6472)\">\n     <use style=\"fill:#3a923a;stroke:#3d3d3d;stroke-width:1.5;\" x=\"0\" xlink:href=\"#m72d5e70c56\" y=\"370.91625\"/>\n    </g>\n   </g>\n   <g id=\"PolyCollection_4\">\n    <defs>\n     <path d=\"M 514.243842 -57.676881 \nL 504.402408 -57.676881 \nL 455.755125 -60.618436 \nL 494.452347 -63.559991 \nL 503.853999 -66.501545 \nL 507.38781 -69.4431 \nL 508.501113 -72.384655 \nL 508.726719 -75.326209 \nL 508.943244 -78.267764 \nL 509.131819 -81.209319 \nL 509.127567 -84.150873 \nL 508.42342 -87.092428 \nL 509.133586 -90.033983 \nL 509.141438 -92.975537 \nL 508.575131 -95.917092 \nL 509.217223 -98.858647 \nL 509.264742 -101.800202 \nL 509.305095 -104.741756 \nL 509.279616 -107.683311 \nL 509.306627 -110.624866 \nL 509.292784 -113.56642 \nL 509.297843 -116.507975 \nL 509.30494 -119.44953 \nL 509.309772 -122.391084 \nL 509.318646 -125.332639 \nL 509.307704 -128.274194 \nL 509.313394 -131.215748 \nL 509.322493 -134.157303 \nL 509.312197 -137.098858 \nL 509.321101 -140.040413 \nL 509.317752 -142.981967 \nL 509.315882 -145.923522 \nL 509.321873 -148.865077 \nL 509.311622 -151.806631 \nL 509.322234 -154.748186 \nL 509.323124 -157.689741 \nL 509.323125 -160.631295 \nL 509.323125 -163.57285 \nL 509.323125 -166.514405 \nL 509.323125 -169.455959 \nL 509.323125 -172.397514 \nL 509.323125 -175.339069 \nL 509.323125 -178.280623 \nL 509.323125 -181.222178 \nL 509.323125 -184.163733 \nL 509.323123 -187.105288 \nL 509.321639 -190.046842 \nL 509.311775 -192.988397 \nL 509.321739 -195.929952 \nL 509.311758 -198.871506 \nL 509.321666 -201.813061 \nL 509.323124 -204.754616 \nL 509.323125 -207.69617 \nL 509.323125 -210.637725 \nL 509.323125 -213.57928 \nL 509.323125 -216.520834 \nL 509.323125 -219.462389 \nL 509.323125 -222.403944 \nL 509.323125 -225.345498 \nL 509.323125 -228.287053 \nL 509.323125 -231.228608 \nL 509.323125 -234.170163 \nL 509.323125 -237.111717 \nL 509.323125 -240.053272 \nL 509.323125 -242.994827 \nL 509.323125 -245.936381 \nL 509.323125 -248.877936 \nL 509.323125 -251.819491 \nL 509.323125 -254.761045 \nL 509.323125 -257.7026 \nL 509.323125 -260.644155 \nL 509.323125 -263.585709 \nL 509.323125 -266.527264 \nL 509.323125 -269.468819 \nL 509.323125 -272.410374 \nL 509.323125 -275.351928 \nL 509.323125 -278.293483 \nL 509.323125 -281.235038 \nL 509.323125 -284.176592 \nL 509.323125 -287.118147 \nL 509.323125 -290.059702 \nL 509.323125 -293.001256 \nL 509.323125 -295.942811 \nL 509.323125 -298.884366 \nL 509.323125 -301.82592 \nL 509.323125 -304.767475 \nL 509.323125 -307.70903 \nL 509.323125 -310.650584 \nL 509.323125 -313.592139 \nL 509.323125 -316.533694 \nL 509.323125 -319.475249 \nL 509.323125 -322.416803 \nL 509.323125 -325.358358 \nL 509.323125 -328.299913 \nL 509.323125 -331.241467 \nL 509.323125 -334.183022 \nL 509.323125 -337.124577 \nL 509.323125 -340.066131 \nL 509.322478 -343.007686 \nL 509.311827 -345.949241 \nL 509.321565 -348.890795 \nL 509.324685 -348.890795 \nL 509.324685 -348.890795 \nL 509.334423 -345.949241 \nL 509.323772 -343.007686 \nL 509.323125 -340.066131 \nL 509.323125 -337.124577 \nL 509.323125 -334.183022 \nL 509.323125 -331.241467 \nL 509.323125 -328.299913 \nL 509.323125 -325.358358 \nL 509.323125 -322.416803 \nL 509.323125 -319.475249 \nL 509.323125 -316.533694 \nL 509.323125 -313.592139 \nL 509.323125 -310.650584 \nL 509.323125 -307.70903 \nL 509.323125 -304.767475 \nL 509.323125 -301.82592 \nL 509.323125 -298.884366 \nL 509.323125 -295.942811 \nL 509.323125 -293.001256 \nL 509.323125 -290.059702 \nL 509.323125 -287.118147 \nL 509.323125 -284.176592 \nL 509.323125 -281.235038 \nL 509.323125 -278.293483 \nL 509.323125 -275.351928 \nL 509.323125 -272.410374 \nL 509.323125 -269.468819 \nL 509.323125 -266.527264 \nL 509.323125 -263.585709 \nL 509.323125 -260.644155 \nL 509.323125 -257.7026 \nL 509.323125 -254.761045 \nL 509.323125 -251.819491 \nL 509.323125 -248.877936 \nL 509.323125 -245.936381 \nL 509.323125 -242.994827 \nL 509.323125 -240.053272 \nL 509.323125 -237.111717 \nL 509.323125 -234.170163 \nL 509.323125 -231.228608 \nL 509.323125 -228.287053 \nL 509.323125 -225.345498 \nL 509.323125 -222.403944 \nL 509.323125 -219.462389 \nL 509.323125 -216.520834 \nL 509.323125 -213.57928 \nL 509.323125 -210.637725 \nL 509.323125 -207.69617 \nL 509.323126 -204.754616 \nL 509.324584 -201.813061 \nL 509.334492 -198.871506 \nL 509.324511 -195.929952 \nL 509.334475 -192.988397 \nL 509.324611 -190.046842 \nL 509.323127 -187.105288 \nL 509.323125 -184.163733 \nL 509.323125 -181.222178 \nL 509.323125 -178.280623 \nL 509.323125 -175.339069 \nL 509.323125 -172.397514 \nL 509.323125 -169.455959 \nL 509.323125 -166.514405 \nL 509.323125 -163.57285 \nL 509.323125 -160.631295 \nL 509.323126 -157.689741 \nL 509.324016 -154.748186 \nL 509.334628 -151.806631 \nL 509.324377 -148.865077 \nL 509.330368 -145.923522 \nL 509.328498 -142.981967 \nL 509.325149 -140.040413 \nL 509.334053 -137.098858 \nL 509.323757 -134.157303 \nL 509.332856 -131.215748 \nL 509.338546 -128.274194 \nL 509.327604 -125.332639 \nL 509.336478 -122.391084 \nL 509.34131 -119.44953 \nL 509.348407 -116.507975 \nL 509.353466 -113.56642 \nL 509.339623 -110.624866 \nL 509.366634 -107.683311 \nL 509.341155 -104.741756 \nL 509.381508 -101.800202 \nL 509.429027 -98.858647 \nL 510.071119 -95.917092 \nL 509.504812 -92.975537 \nL 509.512664 -90.033983 \nL 510.22283 -87.092428 \nL 509.518683 -84.150873 \nL 509.514431 -81.209319 \nL 509.703006 -78.267764 \nL 509.919531 -75.326209 \nL 510.145137 -72.384655 \nL 511.25844 -69.4431 \nL 514.792251 -66.501545 \nL 524.193903 -63.559991 \nL 562.891125 -60.618436 \nL 514.243842 -57.676881 \nz\n\" id=\"m441ffab1ec\" style=\"stroke:#3d3d3d;stroke-width:1.5;\"/>\n    </defs>\n    <g clip-path=\"url(#p8ef8af6472)\">\n     <use style=\"fill:#c03d3e;stroke:#3d3d3d;stroke-width:1.5;\" x=\"0\" xlink:href=\"#m441ffab1ec\" y=\"370.91625\"/>\n    </g>\n   </g>\n   <g id=\"PolyCollection_5\">\n    <defs>\n     <path d=\"M 644.005196 -52.789623 \nL 642.481054 -52.789623 \nL 641.691091 -53.174718 \nL 640.31419 -53.559812 \nL 638.117609 -53.944907 \nL 634.918866 -54.330001 \nL 630.684424 -54.715096 \nL 625.623129 -55.10019 \nL 620.224805 -55.485285 \nL 615.202431 -55.87038 \nL 611.333444 -56.255474 \nL 609.246927 -56.640569 \nL 609.238908 -57.025663 \nL 611.193141 -57.410758 \nL 614.638415 -57.795852 \nL 618.911242 -58.180947 \nL 623.349915 -58.566041 \nL 627.444022 -58.951136 \nL 630.898097 -59.33623 \nL 633.614452 -59.721325 \nL 635.631449 -60.106419 \nL 637.057382 -60.491514 \nL 638.023073 -60.876608 \nL 638.655071 -61.261703 \nL 639.060084 -61.646797 \nL 639.313062 -62.031892 \nL 639.449636 -62.416987 \nL 639.468862 -62.802081 \nL 639.349676 -63.187176 \nL 639.076264 -63.57227 \nL 638.660523 -63.957365 \nL 638.149782 -64.342459 \nL 637.615511 -64.727554 \nL 637.129182 -65.112648 \nL 636.737893 -65.497743 \nL 636.45135 -65.882837 \nL 636.244941 -66.267932 \nL 636.075898 -66.653026 \nL 635.904918 -67.038121 \nL 635.714815 -67.423215 \nL 635.519337 -67.80831 \nL 635.358366 -68.193404 \nL 635.280464 -68.578499 \nL 635.319535 -68.963594 \nL 635.476426 -69.348688 \nL 635.715199 -69.733783 \nL 635.97685 -70.118877 \nL 636.204128 -70.503972 \nL 636.365315 -70.889066 \nL 636.465718 -71.274161 \nL 636.542295 -71.659255 \nL 636.645112 -72.04435 \nL 636.814946 -72.429444 \nL 637.066948 -72.814539 \nL 637.386325 -73.199633 \nL 637.735949 -73.584728 \nL 638.07102 -73.969822 \nL 638.354374 -74.354917 \nL 638.567586 -74.740011 \nL 638.715669 -75.125106 \nL 638.825224 -75.510201 \nL 638.936959 -75.895295 \nL 639.094466 -76.28039 \nL 639.332078 -76.665484 \nL 639.665208 -77.050579 \nL 640.085828 -77.435673 \nL 640.564093 -77.820768 \nL 641.055169 -78.205862 \nL 641.509087 -78.590957 \nL 641.881031 -78.976051 \nL 642.139685 -79.361146 \nL 642.271997 -79.74624 \nL 642.283693 -80.131335 \nL 642.195943 -80.516429 \nL 642.039554 -80.901524 \nL 641.84868 -81.286618 \nL 641.655934 -81.671713 \nL 641.489828 -82.056808 \nL 641.373962 -82.441902 \nL 641.326346 -82.826997 \nL 641.357626 -83.212091 \nL 641.468544 -83.597186 \nL 641.648433 -83.98228 \nL 641.876553 -84.367375 \nL 642.126531 -84.752469 \nL 642.372312 -85.137564 \nL 642.593325 -85.522658 \nL 642.777217 -85.907753 \nL 642.919924 -86.292847 \nL 643.023864 -86.677942 \nL 643.095391 -87.063036 \nL 643.142391 -87.448131 \nL 643.172476 -87.833225 \nL 643.191903 -88.21832 \nL 643.205139 -88.603415 \nL 643.214929 -88.988509 \nL 643.222669 -89.373604 \nL 643.228903 -89.758698 \nL 643.233781 -90.143793 \nL 643.237378 -90.528887 \nL 643.239835 -90.913982 \nL 643.246415 -90.913982 \nL 643.246415 -90.913982 \nL 643.248872 -90.528887 \nL 643.252469 -90.143793 \nL 643.257347 -89.758698 \nL 643.263581 -89.373604 \nL 643.271321 -88.988509 \nL 643.281111 -88.603415 \nL 643.294347 -88.21832 \nL 643.313774 -87.833225 \nL 643.343859 -87.448131 \nL 643.390859 -87.063036 \nL 643.462386 -86.677942 \nL 643.566326 -86.292847 \nL 643.709033 -85.907753 \nL 643.892925 -85.522658 \nL 644.113938 -85.137564 \nL 644.359719 -84.752469 \nL 644.609697 -84.367375 \nL 644.837817 -83.98228 \nL 645.017706 -83.597186 \nL 645.128624 -83.212091 \nL 645.159904 -82.826997 \nL 645.112288 -82.441902 \nL 644.996422 -82.056808 \nL 644.830316 -81.671713 \nL 644.63757 -81.286618 \nL 644.446696 -80.901524 \nL 644.290307 -80.516429 \nL 644.202557 -80.131335 \nL 644.214253 -79.74624 \nL 644.346565 -79.361146 \nL 644.605219 -78.976051 \nL 644.977163 -78.590957 \nL 645.431081 -78.205862 \nL 645.922157 -77.820768 \nL 646.400422 -77.435673 \nL 646.821042 -77.050579 \nL 647.154172 -76.665484 \nL 647.391784 -76.28039 \nL 647.549291 -75.895295 \nL 647.661026 -75.510201 \nL 647.770581 -75.125106 \nL 647.918664 -74.740011 \nL 648.131876 -74.354917 \nL 648.41523 -73.969822 \nL 648.750301 -73.584728 \nL 649.099925 -73.199633 \nL 649.419302 -72.814539 \nL 649.671304 -72.429444 \nL 649.841138 -72.04435 \nL 649.943955 -71.659255 \nL 650.020532 -71.274161 \nL 650.120935 -70.889066 \nL 650.282122 -70.503972 \nL 650.5094 -70.118877 \nL 650.771051 -69.733783 \nL 651.009824 -69.348688 \nL 651.166715 -68.963594 \nL 651.205786 -68.578499 \nL 651.127884 -68.193404 \nL 650.966913 -67.80831 \nL 650.771435 -67.423215 \nL 650.581332 -67.038121 \nL 650.410352 -66.653026 \nL 650.241309 -66.267932 \nL 650.0349 -65.882837 \nL 649.748357 -65.497743 \nL 649.357068 -65.112648 \nL 648.870739 -64.727554 \nL 648.336468 -64.342459 \nL 647.825727 -63.957365 \nL 647.409986 -63.57227 \nL 647.136574 -63.187176 \nL 647.017388 -62.802081 \nL 647.036614 -62.416987 \nL 647.173188 -62.031892 \nL 647.426166 -61.646797 \nL 647.831179 -61.261703 \nL 648.463177 -60.876608 \nL 649.428868 -60.491514 \nL 650.854801 -60.106419 \nL 652.871798 -59.721325 \nL 655.588153 -59.33623 \nL 659.042228 -58.951136 \nL 663.136335 -58.566041 \nL 667.575008 -58.180947 \nL 671.847835 -57.795852 \nL 675.293109 -57.410758 \nL 677.247342 -57.025663 \nL 677.239323 -56.640569 \nL 675.152806 -56.255474 \nL 671.283819 -55.87038 \nL 666.261445 -55.485285 \nL 660.863121 -55.10019 \nL 655.801826 -54.715096 \nL 651.567384 -54.330001 \nL 648.368641 -53.944907 \nL 646.17206 -53.559812 \nL 644.795159 -53.174718 \nL 644.005196 -52.789623 \nz\n\" id=\"m362fa7c75f\" style=\"stroke:#3d3d3d;stroke-width:1.5;\"/>\n    </defs>\n    <g clip-path=\"url(#p8ef8af6472)\">\n     <use style=\"fill:#9372b2;stroke:#3d3d3d;stroke-width:1.5;\" x=\"0\" xlink:href=\"#m362fa7c75f\" y=\"370.91625\"/>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"md85a09a99d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"107.563125\" xlink:href=\"#md85a09a99d\" y=\"333.36\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- openprice -->\n      <g transform=\"translate(82.639688 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n        <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n        <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n        <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n        <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n        <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n        <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"61.181641\" xlink:href=\"#DejaVuSans-112\"/>\n       <use x=\"124.658203\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"186.181641\" xlink:href=\"#DejaVuSans-110\"/>\n       <use x=\"249.560547\" xlink:href=\"#DejaVuSans-112\"/>\n       <use x=\"313.037109\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"354.150391\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"381.933594\" xlink:href=\"#DejaVuSans-99\"/>\n       <use x=\"436.914062\" xlink:href=\"#DejaVuSans-101\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"241.483125\" xlink:href=\"#md85a09a99d\" y=\"333.36\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- highprice -->\n      <g transform=\"translate(218.137813 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n        <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-104\"/>\n       <use x=\"63.378906\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"91.162109\" xlink:href=\"#DejaVuSans-103\"/>\n       <use x=\"154.638672\" xlink:href=\"#DejaVuSans-104\"/>\n       <use x=\"218.017578\" xlink:href=\"#DejaVuSans-112\"/>\n       <use x=\"281.494141\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"322.607422\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"350.390625\" xlink:href=\"#DejaVuSans-99\"/>\n       <use x=\"405.371094\" xlink:href=\"#DejaVuSans-101\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"375.403125\" xlink:href=\"#md85a09a99d\" y=\"333.36\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- lowprice -->\n      <g transform=\"translate(354.421094 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n        <path d=\"M 4.203125 54.6875 \nL 13.1875 54.6875 \nL 24.421875 12.015625 \nL 35.59375 54.6875 \nL 46.1875 54.6875 \nL 57.421875 12.015625 \nL 68.609375 54.6875 \nL 77.59375 54.6875 \nL 63.28125 0 \nL 52.6875 0 \nL 40.921875 44.828125 \nL 29.109375 0 \nL 18.5 0 \nz\n\" id=\"DejaVuSans-119\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"88.964844\" xlink:href=\"#DejaVuSans-119\"/>\n       <use x=\"170.751953\" xlink:href=\"#DejaVuSans-112\"/>\n       <use x=\"234.228516\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"275.341797\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"303.125\" xlink:href=\"#DejaVuSans-99\"/>\n       <use x=\"358.105469\" xlink:href=\"#DejaVuSans-101\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"509.323125\" xlink:href=\"#md85a09a99d\" y=\"333.36\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- volume -->\n      <g transform=\"translate(490.799688 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n        <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n        <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-118\"/>\n       <use x=\"59.179688\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"120.361328\" xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"148.144531\" xlink:href=\"#DejaVuSans-117\"/>\n       <use x=\"211.523438\" xlink:href=\"#DejaVuSans-109\"/>\n       <use x=\"308.935547\" xlink:href=\"#DejaVuSans-101\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"643.243125\" xlink:href=\"#md85a09a99d\" y=\"333.36\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- closeprice -->\n      <g transform=\"translate(617.919688 347.958438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-99\"/>\n       <use x=\"54.980469\" xlink:href=\"#DejaVuSans-108\"/>\n       <use x=\"82.763672\" xlink:href=\"#DejaVuSans-111\"/>\n       <use x=\"143.945312\" xlink:href=\"#DejaVuSans-115\"/>\n       <use x=\"196.044922\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"257.568359\" xlink:href=\"#DejaVuSans-112\"/>\n       <use x=\"321.044922\" xlink:href=\"#DejaVuSans-114\"/>\n       <use x=\"362.158203\" xlink:href=\"#DejaVuSans-105\"/>\n       <use x=\"389.941406\" xlink:href=\"#DejaVuSans-99\"/>\n       <use x=\"444.921875\" xlink:href=\"#DejaVuSans-101\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- Column -->\n     <g transform=\"translate(356.255469 361.636562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 64.40625 67.28125 \nL 64.40625 56.890625 \nQ 59.421875 61.53125 53.78125 63.8125 \nQ 48.140625 66.109375 41.796875 66.109375 \nQ 29.296875 66.109375 22.65625 58.46875 \nQ 16.015625 50.828125 16.015625 36.375 \nQ 16.015625 21.96875 22.65625 14.328125 \nQ 29.296875 6.6875 41.796875 6.6875 \nQ 48.140625 6.6875 53.78125 8.984375 \nQ 59.421875 11.28125 64.40625 15.921875 \nL 64.40625 5.609375 \nQ 59.234375 2.09375 53.4375 0.328125 \nQ 47.65625 -1.421875 41.21875 -1.421875 \nQ 24.65625 -1.421875 15.125 8.703125 \nQ 5.609375 18.84375 5.609375 36.375 \nQ 5.609375 53.953125 15.125 64.078125 \nQ 24.65625 74.21875 41.21875 74.21875 \nQ 47.75 74.21875 53.53125 72.484375 \nQ 59.328125 70.75 64.40625 67.28125 \nz\n\" id=\"DejaVuSans-67\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-67\"/>\n      <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"131.005859\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"158.789062\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"222.167969\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"319.580078\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m1a4f04162e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m1a4f04162e\" y=\"307.915273\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(27.240625 311.714492)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m1a4f04162e\" y=\"269.341557\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g transform=\"translate(27.240625 273.140776)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m1a4f04162e\" y=\"230.767842\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 234.56706)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m1a4f04162e\" y=\"192.194126\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(20.878125 195.993345)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m1a4f04162e\" y=\"153.62041\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(20.878125 157.419629)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m1a4f04162e\" y=\"115.046695\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(20.878125 118.845914)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m1a4f04162e\" y=\"76.472979\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 30 -->\n      <g transform=\"translate(20.878125 80.272198)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#m1a4f04162e\" y=\"37.899263\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 35 -->\n      <g transform=\"translate(20.878125 41.698482)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- Normalized -->\n     <g transform=\"translate(14.798438 198.635469)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 23.09375 72.90625 \nL 55.421875 11.921875 \nL 55.421875 72.90625 \nL 64.984375 72.90625 \nL 64.984375 0 \nL 51.703125 0 \nL 19.390625 60.984375 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-78\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 5.515625 54.6875 \nL 48.1875 54.6875 \nL 48.1875 46.484375 \nL 14.40625 7.171875 \nL 48.1875 7.171875 \nL 48.1875 0 \nL 4.296875 0 \nL 4.296875 8.203125 \nL 38.09375 47.515625 \nL 5.515625 47.515625 \nz\n\" id=\"DejaVuSans-122\"/>\n       <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-78\"/>\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"135.986328\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"175.349609\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"272.761719\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"334.041016\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"361.824219\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"389.607422\" xlink:href=\"#DejaVuSans-122\"/>\n      <use x=\"442.097656\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"503.621094\" xlink:href=\"#DejaVuSans-100\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p8ef8af6472)\" d=\"M 107.563125 315.86041 \nL 107.563125 283.993274 \n\" style=\"fill:none;stroke:#3d3d3d;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p8ef8af6472)\" d=\"M 107.563125 314.237779 \nL 107.563125 301.8924 \n\" style=\"fill:none;stroke:#3d3d3d;stroke-linecap:square;stroke-width:4.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p8ef8af6472)\" d=\"M 241.483125 315.460048 \nL 241.483125 284.328414 \n\" style=\"fill:none;stroke:#3d3d3d;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#p8ef8af6472)\" d=\"M 241.483125 314.284957 \nL 241.483125 301.8803 \n\" style=\"fill:none;stroke:#3d3d3d;stroke-linecap:square;stroke-width:4.5;\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p8ef8af6472)\" d=\"M 375.403125 315.823 \nL 375.403125 284.11697 \n\" style=\"fill:none;stroke:#3d3d3d;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#p8ef8af6472)\" d=\"M 375.403125 314.241926 \nL 375.403125 301.956896 \n\" style=\"fill:none;stroke:#3d3d3d;stroke-linecap:square;stroke-width:4.5;\"/>\n   </g>\n   <g id=\"line2d_20\">\n    <path clip-path=\"url(#p8ef8af6472)\" d=\"M 509.323125 310.565234 \nL 509.323125 305.908837 \n\" style=\"fill:none;stroke:#3d3d3d;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_21\">\n    <path clip-path=\"url(#p8ef8af6472)\" d=\"M 509.323125 310.536815 \nL 509.323125 308.681722 \n\" style=\"fill:none;stroke:#3d3d3d;stroke-linecap:square;stroke-width:4.5;\"/>\n   </g>\n   <g id=\"line2d_22\">\n    <path clip-path=\"url(#p8ef8af6472)\" d=\"M 643.243125 315.452492 \nL 643.243125 284.165465 \n\" style=\"fill:none;stroke:#3d3d3d;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_23\">\n    <path clip-path=\"url(#p8ef8af6472)\" d=\"M 643.243125 314.281319 \nL 643.243125 301.984012 \n\" style=\"fill:none;stroke:#3d3d3d;stroke-linecap:square;stroke-width:4.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 333.36 \nL 40.603125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 710.203125 333.36 \nL 710.203125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 333.36 \nL 710.203125 333.36 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 7.2 \nL 710.203125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <defs>\n     <path d=\"M 0 1.5 \nC 0.397805 1.5 0.77937 1.341951 1.06066 1.06066 \nC 1.341951 0.77937 1.5 0.397805 1.5 0 \nC 1.5 -0.397805 1.341951 -0.77937 1.06066 -1.06066 \nC 0.77937 -1.341951 0.397805 -1.5 0 -1.5 \nC -0.397805 -1.5 -0.77937 -1.341951 -1.06066 -1.06066 \nC -1.341951 -0.77937 -1.5 -0.397805 -1.5 0 \nC -1.5 0.397805 -1.341951 0.77937 -1.06066 1.06066 \nC -0.77937 1.341951 -0.397805 1.5 0 1.5 \nz\n\" id=\"m3f8a3c66bc\" style=\"stroke:#3d3d3d;\"/>\n    </defs>\n    <g clip-path=\"url(#p8ef8af6472)\">\n     <use style=\"fill:#ffffff;stroke:#3d3d3d;\" x=\"107.563125\" xlink:href=\"#m3f8a3c66bc\" y=\"312.615148\"/>\n    </g>\n   </g>\n   <g id=\"PathCollection_2\">\n    <g clip-path=\"url(#p8ef8af6472)\">\n     <use style=\"fill:#ffffff;stroke:#3d3d3d;\" x=\"241.483125\" xlink:href=\"#m3f8a3c66bc\" y=\"312.199585\"/>\n    </g>\n   </g>\n   <g id=\"PathCollection_3\">\n    <g clip-path=\"url(#p8ef8af6472)\">\n     <use style=\"fill:#ffffff;stroke:#3d3d3d;\" x=\"375.403125\" xlink:href=\"#m3f8a3c66bc\" y=\"312.542482\"/>\n    </g>\n   </g>\n   <g id=\"PathCollection_4\">\n    <g clip-path=\"url(#p8ef8af6472)\">\n     <use style=\"fill:#ffffff;stroke:#3d3d3d;\" x=\"509.323125\" xlink:href=\"#m3f8a3c66bc\" y=\"310.046298\"/>\n    </g>\n   </g>\n   <g id=\"PathCollection_5\">\n    <g clip-path=\"url(#p8ef8af6472)\">\n     <use style=\"fill:#ffffff;stroke:#3d3d3d;\" x=\"643.243125\" xlink:href=\"#m3f8a3c66bc\" y=\"312.240134\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p8ef8af6472\">\n   <rect height=\"326.16\" width=\"669.6\" x=\"40.603125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# Visualize the ditribution of the features in the train dataset\n",
    "df_std = (df_train - df_train.mean()) / df_train.std()\n",
    "df_std = df_std.melt(var_name='Column', value_name='Normalized')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(df, feature_set, scaler=None):\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        scaled_arr = scaler.fit_transform(df[feature_set])\n",
    "    else:\n",
    "        scaled_arr = scaler.transform(df[feature_set])\n",
    "    return scaled_arr, scaler\n",
    "\n",
    "def split(train_arr, train_ratio):\n",
    "    # split the data to train, validate\n",
    "    n = len(train_arr)\n",
    "    train_set = train_arr[:int(n*train_ratio)]\n",
    "    val_set = train_arr[int(n*train_ratio):]\n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Windowing for time series forecasting\n",
    "\n",
    "> Refer to [Data Windowing](https://www.tensorflow.org/tutorials/structured_data/time_series#data_windowing) for more details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate time windows for time series forecasting with LSTM network\n",
    "def generate_window(dataset, train_window, pred_horizon):\n",
    "    dataset_seq = []\n",
    "    size = len(dataset)\n",
    "    x_arr = []\n",
    "    y_arr = []\n",
    "    for i in range(size - train_window - pred_horizon):\n",
    "        x = dataset[i:(i+train_window), :-1]\n",
    "        y = dataset[i+train_window+pred_horizon-1:i+train_window+pred_horizon, -1]\n",
    "        x_arr.append(x)\n",
    "        y_arr.append(y)\n",
    "\n",
    "    x_tensor = torch.tensor(x_arr).float()\n",
    "    y_tensor = torch.tensor(y_arr).float()\n",
    "    num_features = x_tensor.shape[2]\n",
    "    dataset_seq = (x_tensor, y_tensor)\n",
    "    return dataset_seq, num_features\n",
    "\n",
    "def create_batch_set(dataset_seq):\n",
    "    x_tensor, y_tensor = dataset_seq\n",
    "    tensor_dataset = TensorDataset(x_tensor,y_tensor)\n",
    "    batch_size = 100\n",
    "    tensor_dataloader = DataLoader(tensor_dataset, batch_size, False)\n",
    "    return tensor_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various feature sets for model selection\n",
    "feature_set1 = ['open_price', 'high-price', 'low-price', 'volume', 'close_price']\n",
    "feature_set2 = ['open_price', 'high-price', 'low-price', 'close_price']\n",
    "feature_set3 = ['open_price', 'volume', 'close_price']\n",
    "feature_set4 = ['volume', 'close_price']\n",
    "feature_sets = [feature_set1, feature_set1, feature_set3, feature_set4]\n",
    "\n",
    "# let's create sequential training dataset with various traning windows and prediction horizons\n",
    "# given that the times series data has 1-hour resolution\n",
    "# 24hours * (days)\n",
    "train_window_list = 24 * np.array([5, 10, 30])\n",
    "prediction_horizon_list = 24 * np.array([1, 2, 3, 5, 10])\n",
    "\n",
    "\n",
    "train_set, train_scaler = scale(df_train, feature_sets[0])\n",
    "target_set, _ = scale(df_target, feature_sets[0], train_scaler)\n",
    "train_set , val_set = split(train_set, 0.8)\n",
    "\n",
    "gamestop_set, _ = scale(df_gamestop, feature_sets[0], train_scaler)\n",
    "\n",
    "train_window = train_window_list[0]\n",
    "prediction_horizon = prediction_horizon_list[0]\n",
    "\n",
    "train_seq, num_features = generate_window(train_set, train_window, prediction_horizon)\n",
    "val_seq, _ = generate_window(val_set, train_window, prediction_horizon)\n",
    "\n",
    "train_batches = create_batch_set(train_seq)\n",
    "val_batches = create_batch_set(val_seq)\n",
    "\n",
    "target_seq, _ = generate_window(target_set, train_window, prediction_horizon)\n",
    "gamestop_seq, _ = generate_window(gamestop_set, train_window, prediction_horizon)\n",
    "\n",
    "datetime_target = target_datetime_list[train_window+prediction_horizon:]\n",
    "datetime_gamestop = gamestop_datetime_list[train_window+prediction_horizon:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, seq_length, hidden_size=100, num_layers = 2, output_size=1):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.seq_len = seq_length\n",
    "        \n",
    "        self.hidden_state = None\n",
    "        self.cell_state = None\n",
    "        self.hidden = (self.hidden_state, self.cell_state)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(self.hidden_size * self.seq_len, self.output_size)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        self.hidden_state = torch.zeros(self.num_layers, batch_size ,self.hidden_size).to(device)\n",
    "        self.cell_state = torch.zeros(self.num_layers, batch_size ,self.hidden_size).to(device)\n",
    "        self.hidden = (self.hidden_state, self.cell_state)\n",
    "\n",
    "    def forward(self, x, forecast_timesteps = 0):\n",
    "        batch_size = x.size(0)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "                x,\n",
    "                self.hidden\n",
    "            )\n",
    "        outputs = self.linear(lstm_out.reshape(batch_size,-1))\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, train_batches, val_batches=None, num_epochs=200):\n",
    "\n",
    "    learning_rate = 1e-3\n",
    "    best_loss = np.Inf\n",
    "    val_loss = None\n",
    "    history = dict(train=[], val=[])\n",
    "\n",
    "    criterion = torch.nn.MSELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_losses = []\n",
    "        for batch_ndx, train_batch in enumerate(train_batches):\n",
    "            model.train()\n",
    "            x_train, y_train = train_batch\n",
    "            batch_size = x_train.size(0)\n",
    "            x_train = x_train.to(device)\n",
    "            y_train = y_train.to(device)\n",
    "            model.init_hidden(batch_size, device)\n",
    "            outputs = model(x_train)\n",
    "            loss = criterion(outputs, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss = loss.item()\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            if val_batches is not None:\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    val_losses = []\n",
    "                    for _, val_batch in enumerate(val_batches):\n",
    "                        x_val, y_val = val_batch\n",
    "                        batch_size = x_val.size(0)\n",
    "                        x_val = x_val.to(device)\n",
    "                        y_val = y_val.to(device)\n",
    "                        model.init_hidden(batch_size, device)\n",
    "                        pred = model(x_val)\n",
    "                        loss = criterion(pred, y_val)\n",
    "                        val_loss = loss.item()\n",
    "                        val_losses.append(val_loss)\n",
    "                        if val_loss < best_loss:\n",
    "                            best_loss = val_loss\n",
    "                        if val_loss < 0.05:\n",
    "                            break\n",
    "            print(f'Batch {batch_ndx} Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n",
    "        history['val'].append(np.mean(val_losses)) \n",
    "        history['train'].append(np.mean(train_losses)) \n",
    "            \n",
    "            \n",
    "    print(f\"best validation loss = {best_loss}\")        \n",
    "    return model.eval(), history, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(history):\n",
    "    plt.plot(history['train'], label=\"Training loss\")\n",
    "    plt.plot(history['val'], label=\"Test loss\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 Epoch 0: train loss 0.4875347912311554 val loss 7.767407417297363\n",
      "Batch 1 Epoch 0: train loss 0.07683544605970383 val loss 9.48511028289795\n",
      "Batch 2 Epoch 0: train loss 0.023724498227238655 val loss 10.579541206359863\n",
      "Batch 3 Epoch 0: train loss 0.3328097462654114 val loss 10.543086051940918\n",
      "Batch 4 Epoch 0: train loss 0.17985093593597412 val loss 9.91950511932373\n",
      "Batch 5 Epoch 0: train loss 0.0010585663840174675 val loss 9.367132186889648\n",
      "Batch 6 Epoch 0: train loss 0.054466474801301956 val loss 8.942644119262695\n",
      "Batch 7 Epoch 0: train loss 0.16070938110351562 val loss 8.637377738952637\n",
      "Batch 8 Epoch 0: train loss 0.11010316759347916 val loss 8.408267974853516\n",
      "Batch 9 Epoch 0: train loss 0.08868589252233505 val loss 8.245092391967773\n",
      "Batch 10 Epoch 0: train loss 0.03649991750717163 val loss 8.106904029846191\n",
      "Batch 11 Epoch 0: train loss 0.01050518173724413 val loss 7.969880104064941\n",
      "Batch 12 Epoch 0: train loss 0.001995294587686658 val loss 7.843054294586182\n",
      "Batch 13 Epoch 0: train loss 0.06532739102840424 val loss 7.785548686981201\n",
      "Batch 14 Epoch 0: train loss 0.0558842234313488 val loss 7.742290496826172\n",
      "Batch 15 Epoch 0: train loss 0.06006121635437012 val loss 7.706982612609863\n",
      "Batch 16 Epoch 0: train loss 0.01067342422902584 val loss 7.6617536544799805\n",
      "Batch 17 Epoch 0: train loss 0.0027370506431907415 val loss 7.639814376831055\n",
      "Batch 18 Epoch 0: train loss 0.0012509777443483472 val loss 7.627315044403076\n",
      "Batch 19 Epoch 0: train loss 0.003916653338819742 val loss 7.62121057510376\n",
      "Batch 20 Epoch 0: train loss 0.008460176177322865 val loss 7.616605758666992\n",
      "Batch 21 Epoch 0: train loss 0.01106087863445282 val loss 7.61237907409668\n",
      "Batch 22 Epoch 0: train loss 0.020038386806845665 val loss 7.605669975280762\n",
      "Batch 23 Epoch 0: train loss 0.012815980240702629 val loss 7.5880818367004395\n",
      "Batch 24 Epoch 0: train loss 0.005166627001017332 val loss 7.562431812286377\n",
      "Batch 25 Epoch 0: train loss 0.0006284090923145413 val loss 7.533287525177002\n",
      "Batch 26 Epoch 0: train loss 0.008622279390692711 val loss 7.516507625579834\n",
      "Batch 27 Epoch 0: train loss 0.013397498056292534 val loss 7.507866382598877\n",
      "Batch 28 Epoch 0: train loss 0.022888988256454468 val loss 7.505740642547607\n",
      "Batch 29 Epoch 0: train loss 0.0333036370575428 val loss 7.489504814147949\n",
      "Batch 30 Epoch 0: train loss 0.022853393107652664 val loss 7.4440693855285645\n",
      "Batch 31 Epoch 0: train loss 0.013513808138668537 val loss 7.367743968963623\n",
      "Batch 32 Epoch 0: train loss 0.024404270574450493 val loss 7.237898349761963\n",
      "Batch 33 Epoch 0: train loss 0.20556968450546265 val loss 6.841339111328125\n",
      "Batch 34 Epoch 0: train loss 0.11259181797504425 val loss 6.1316022872924805\n",
      "Batch 35 Epoch 0: train loss 0.027130596339702606 val loss 5.478178024291992\n",
      "Batch 36 Epoch 0: train loss 0.031389784067869186 val loss 5.072599411010742\n",
      "Batch 37 Epoch 0: train loss 0.08310533314943314 val loss 4.441290378570557\n",
      "Batch 38 Epoch 0: train loss 0.014535337686538696 val loss 3.861433267593384\n",
      "Batch 39 Epoch 0: train loss 0.02831920236349106 val loss 3.1881418228149414\n",
      "Batch 40 Epoch 0: train loss 0.00992038194090128 val loss 0.0425579734146595\n",
      "Batch 41 Epoch 0: train loss 0.043370552361011505 val loss 0.027899017557501793\n",
      "Batch 42 Epoch 0: train loss 0.15375080704689026 val loss 0.01099978107959032\n",
      "Batch 43 Epoch 0: train loss 0.02007676102221012 val loss 0.01722688227891922\n",
      "Batch 44 Epoch 0: train loss 0.04980754107236862 val loss 0.02607986330986023\n",
      "Batch 45 Epoch 0: train loss 0.03233036771416664 val loss 0.02420867420732975\n",
      "Batch 46 Epoch 0: train loss 0.18795377016067505 val loss 0.047754090279340744\n",
      "Batch 47 Epoch 0: train loss 0.7464884519577026 val loss 0.022394590079784393\n",
      "Batch 48 Epoch 0: train loss 0.18605773150920868 val loss 0.00936114601790905\n",
      "Batch 49 Epoch 0: train loss 0.004028173629194498 val loss 4.0173540115356445\n",
      "Batch 0 Epoch 1: train loss 1.5646400451660156 val loss 5.20147180557251\n",
      "Batch 1 Epoch 1: train loss 1.1247878074645996 val loss 6.143506050109863\n",
      "Batch 2 Epoch 1: train loss 0.8669967651367188 val loss 6.866549491882324\n",
      "Batch 3 Epoch 1: train loss 0.28749892115592957 val loss 7.414605617523193\n",
      "Batch 4 Epoch 1: train loss 0.15860968828201294 val loss 7.845726013183594\n",
      "Batch 5 Epoch 1: train loss 0.19313141703605652 val loss 8.203850746154785\n",
      "Batch 6 Epoch 1: train loss 0.15794287621974945 val loss 8.497368812561035\n",
      "Batch 7 Epoch 1: train loss 0.1337747871875763 val loss 8.72222900390625\n",
      "Batch 8 Epoch 1: train loss 0.029383542016148567 val loss 8.865375518798828\n",
      "Batch 9 Epoch 1: train loss 0.0020873434841632843 val loss 8.934967994689941\n",
      "Batch 10 Epoch 1: train loss 0.01788020133972168 val loss 8.955471992492676\n",
      "Batch 11 Epoch 1: train loss 0.052261289209127426 val loss 8.967297554016113\n",
      "Batch 12 Epoch 1: train loss 0.08969996869564056 val loss 8.990110397338867\n",
      "Batch 13 Epoch 1: train loss 0.16106730699539185 val loss 9.005350112915039\n",
      "Batch 14 Epoch 1: train loss 0.07013195753097534 val loss 8.984270095825195\n",
      "Batch 15 Epoch 1: train loss 0.016461702063679695 val loss 8.944339752197266\n",
      "Batch 16 Epoch 1: train loss 0.003205566667020321 val loss 8.916210174560547\n",
      "Batch 17 Epoch 1: train loss 0.026460103690624237 val loss 8.901309967041016\n",
      "Batch 18 Epoch 1: train loss 0.09969651699066162 val loss 8.906597137451172\n",
      "Batch 19 Epoch 1: train loss 0.13691295683383942 val loss 8.965415954589844\n",
      "Batch 20 Epoch 1: train loss 0.13819003105163574 val loss 9.100480079650879\n",
      "Batch 21 Epoch 1: train loss 0.07710824906826019 val loss 9.300909996032715\n",
      "Batch 22 Epoch 1: train loss 0.03890448436141014 val loss 9.55459213256836\n",
      "Batch 23 Epoch 1: train loss 0.0042830645106732845 val loss 9.828109741210938\n",
      "Batch 24 Epoch 1: train loss 0.0058464668691158295 val loss 10.084925651550293\n",
      "Batch 25 Epoch 1: train loss 0.03239353373646736 val loss 10.291626930236816\n",
      "Batch 26 Epoch 1: train loss 0.09000448882579803 val loss 10.411779403686523\n",
      "Batch 27 Epoch 1: train loss 0.10179946571588516 val loss 10.423674583435059\n",
      "Batch 28 Epoch 1: train loss 0.09229596704244614 val loss 10.331905364990234\n",
      "Batch 29 Epoch 1: train loss 0.08161068707704544 val loss 10.144396781921387\n",
      "Batch 30 Epoch 1: train loss 0.039850905537605286 val loss 9.895342826843262\n",
      "Batch 31 Epoch 1: train loss 0.016285764053463936 val loss 9.62258243560791\n",
      "Batch 32 Epoch 1: train loss 0.016371697187423706 val loss 9.328510284423828\n",
      "Batch 33 Epoch 1: train loss 0.203191339969635 val loss 8.863621711730957\n",
      "Batch 34 Epoch 1: train loss 0.16979171335697174 val loss 8.167478561401367\n",
      "Batch 35 Epoch 1: train loss 0.0308715607970953 val loss 7.528339385986328\n",
      "Batch 36 Epoch 1: train loss 0.0243629589676857 val loss 7.069204807281494\n",
      "Batch 37 Epoch 1: train loss 0.11050613969564438 val loss 6.470211505889893\n",
      "Batch 38 Epoch 1: train loss 0.2859567701816559 val loss 5.700221061706543\n",
      "Batch 39 Epoch 1: train loss 0.09786579012870789 val loss 4.746951580047607\n",
      "Batch 40 Epoch 1: train loss 0.017728246748447418 val loss 3.8121259212493896\n",
      "Batch 41 Epoch 1: train loss 0.03437706455588341 val loss 0.025076849386096\n",
      "Batch 42 Epoch 1: train loss 0.1387297809123993 val loss 0.0091835493221879\n",
      "Batch 43 Epoch 1: train loss 0.020188912749290466 val loss 0.025213642045855522\n",
      "Batch 44 Epoch 1: train loss 0.03571078181266785 val loss 0.018694663420319557\n",
      "Batch 45 Epoch 1: train loss 0.0749061107635498 val loss 0.022996094077825546\n",
      "Batch 46 Epoch 1: train loss 0.05124879628419876 val loss 0.03304934874176979\n",
      "Batch 47 Epoch 1: train loss 0.891996443271637 val loss 0.022169960662722588\n",
      "Batch 48 Epoch 1: train loss 0.4563300907611847 val loss 0.011102278716862202\n",
      "Batch 49 Epoch 1: train loss 0.046328939497470856 val loss 0.009792523458600044\n",
      "Batch 0 Epoch 2: train loss 2.55808424949646 val loss 5.896368503570557\n",
      "Batch 1 Epoch 2: train loss 1.7598271369934082 val loss 7.428226947784424\n",
      "Batch 2 Epoch 2: train loss 1.2571839094161987 val loss 8.719111442565918\n",
      "Batch 3 Epoch 2: train loss 0.3487611413002014 val loss 9.813572883605957\n",
      "Batch 4 Epoch 2: train loss 0.18143926560878754 val loss 10.766730308532715\n",
      "Batch 5 Epoch 2: train loss 0.1428135484457016 val loss 11.631019592285156\n",
      "Batch 6 Epoch 2: train loss 0.044207438826560974 val loss 12.410473823547363\n",
      "Batch 7 Epoch 2: train loss 0.018948065117001534 val loss 13.115577697753906\n",
      "Batch 8 Epoch 2: train loss 0.006447021849453449 val loss 13.680764198303223\n",
      "Batch 9 Epoch 2: train loss 0.033354006707668304 val loss 14.051360130310059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 Epoch 2: train loss 0.08361922204494476 val loss 14.171777725219727\n",
      "Batch 11 Epoch 2: train loss 0.07488628476858139 val loss 14.076006889343262\n",
      "Batch 12 Epoch 2: train loss 0.05967174842953682 val loss 13.808561325073242\n",
      "Batch 13 Epoch 2: train loss 0.07507988810539246 val loss 13.383956909179688\n",
      "Batch 14 Epoch 2: train loss 0.016923798248171806 val loss 12.899100303649902\n",
      "Batch 15 Epoch 2: train loss 0.0034526598174124956 val loss 12.413152694702148\n",
      "Batch 16 Epoch 2: train loss 0.08638620376586914 val loss 12.040716171264648\n",
      "Batch 17 Epoch 2: train loss 0.0934230387210846 val loss 11.79967212677002\n",
      "Batch 18 Epoch 2: train loss 0.08292943984270096 val loss 11.701533317565918\n",
      "Batch 19 Epoch 2: train loss 0.07505515217781067 val loss 11.744991302490234\n",
      "Batch 20 Epoch 2: train loss 0.06625499576330185 val loss 11.923321723937988\n",
      "Batch 21 Epoch 2: train loss 0.04075680673122406 val loss 12.204258918762207\n",
      "Batch 22 Epoch 2: train loss 0.028256721794605255 val loss 12.570080757141113\n",
      "Batch 23 Epoch 2: train loss 0.0067811342887580395 val loss 12.968688011169434\n",
      "Batch 24 Epoch 2: train loss 0.00026009834255091846 val loss 13.33747673034668\n",
      "Batch 25 Epoch 2: train loss 0.009567545726895332 val loss 13.615433692932129\n",
      "Batch 26 Epoch 2: train loss 0.045946426689624786 val loss 13.718997955322266\n",
      "Batch 27 Epoch 2: train loss 0.06115388125181198 val loss 13.624436378479004\n",
      "Batch 28 Epoch 2: train loss 0.06623990833759308 val loss 13.343160629272461\n",
      "Batch 29 Epoch 2: train loss 0.07002799212932587 val loss 12.897134780883789\n",
      "Batch 30 Epoch 2: train loss 0.04169454425573349 val loss 12.358574867248535\n",
      "Batch 31 Epoch 2: train loss 0.022176820784807205 val loss 11.787944793701172\n",
      "Batch 32 Epoch 2: train loss 0.025531215593218803 val loss 11.192052841186523\n",
      "Batch 33 Epoch 2: train loss 0.2510576844215393 val loss 10.387202262878418\n",
      "Batch 34 Epoch 2: train loss 0.22348609566688538 val loss 9.377704620361328\n",
      "Batch 35 Epoch 2: train loss 0.03708674758672714 val loss 8.460920333862305\n",
      "Batch 36 Epoch 2: train loss 0.013031433336436749 val loss 7.738155841827393\n",
      "Batch 37 Epoch 2: train loss 0.16420987248420715 val loss 6.923105239868164\n",
      "Batch 38 Epoch 2: train loss 0.6786147952079773 val loss 5.932742595672607\n",
      "Batch 39 Epoch 2: train loss 0.1933891624212265 val loss 4.865734100341797\n",
      "Batch 40 Epoch 2: train loss 0.025979531928896904 val loss 3.896434783935547\n",
      "Batch 41 Epoch 2: train loss 0.018112096935510635 val loss 0.03172963112592697\n",
      "Batch 42 Epoch 2: train loss 0.16602925956249237 val loss 0.008713727816939354\n",
      "Batch 43 Epoch 2: train loss 0.06116549298167229 val loss 0.029079968109726906\n",
      "Batch 44 Epoch 2: train loss 0.0892145112156868 val loss 0.028522059321403503\n",
      "Batch 45 Epoch 2: train loss 0.05516437441110611 val loss 0.032818086445331573\n",
      "Batch 46 Epoch 2: train loss 0.085153728723526 val loss 0.02113707922399044\n",
      "Batch 47 Epoch 2: train loss 0.9677448868751526 val loss 0.0388488844037056\n",
      "Batch 48 Epoch 2: train loss 0.5351942181587219 val loss 0.043120961636304855\n",
      "Batch 49 Epoch 2: train loss 0.0993880033493042 val loss 0.03261089697480202\n",
      "Batch 0 Epoch 3: train loss 2.143371343612671 val loss 0.02326267585158348\n",
      "Batch 1 Epoch 3: train loss 1.5083662271499634 val loss 4.402595520019531\n",
      "Batch 2 Epoch 3: train loss 1.0447593927383423 val loss 5.509442329406738\n",
      "Batch 3 Epoch 3: train loss 0.2654120624065399 val loss 6.507284641265869\n",
      "Batch 4 Epoch 3: train loss 0.10827746242284775 val loss 7.398217678070068\n",
      "Batch 5 Epoch 3: train loss 0.05998209863901138 val loss 8.190191268920898\n",
      "Batch 6 Epoch 3: train loss 0.0029644188471138477 val loss 8.85822582244873\n",
      "Batch 7 Epoch 3: train loss 0.013907830230891705 val loss 9.40287971496582\n",
      "Batch 8 Epoch 3: train loss 0.11844424903392792 val loss 9.813850402832031\n",
      "Batch 9 Epoch 3: train loss 0.1499098390340805 val loss 10.097043991088867\n",
      "Batch 10 Epoch 3: train loss 0.15150246024131775 val loss 10.261773109436035\n",
      "Batch 11 Epoch 3: train loss 0.07196284085512161 val loss 10.33246898651123\n",
      "Batch 12 Epoch 3: train loss 0.025072697550058365 val loss 10.333263397216797\n",
      "Batch 13 Epoch 3: train loss 0.024111786857247353 val loss 10.276504516601562\n",
      "Batch 14 Epoch 3: train loss 0.00011986510799033567 val loss 10.195572853088379\n",
      "Batch 15 Epoch 3: train loss 0.006333349738270044 val loss 10.113059997558594\n",
      "Batch 16 Epoch 3: train loss 0.08039579540491104 val loss 10.049635887145996\n",
      "Batch 17 Epoch 3: train loss 0.11181186139583588 val loss 10.017829895019531\n",
      "Batch 18 Epoch 3: train loss 0.10757819563150406 val loss 10.020455360412598\n",
      "Batch 19 Epoch 3: train loss 0.10750891268253326 val loss 10.06292724609375\n",
      "Batch 20 Epoch 3: train loss 0.10135192424058914 val loss 10.145790100097656\n",
      "Batch 21 Epoch 3: train loss 0.07330728322267532 val loss 10.262467384338379\n",
      "Batch 22 Epoch 3: train loss 0.06175689399242401 val loss 10.40989875793457\n",
      "Batch 23 Epoch 3: train loss 0.030413426458835602 val loss 10.57475757598877\n",
      "Batch 24 Epoch 3: train loss 0.007249728310853243 val loss 10.74317455291748\n",
      "Batch 25 Epoch 3: train loss 0.00012933049583807588 val loss 10.899977684020996\n",
      "Batch 26 Epoch 3: train loss 0.01975288987159729 val loss 11.020312309265137\n",
      "Batch 27 Epoch 3: train loss 0.04290502145886421 val loss 11.08558464050293\n",
      "Batch 28 Epoch 3: train loss 0.07033570855855942 val loss 11.08255672454834\n",
      "Batch 29 Epoch 3: train loss 0.10485385358333588 val loss 10.997564315795898\n",
      "Batch 30 Epoch 3: train loss 0.09619900584220886 val loss 10.837185859680176\n",
      "Batch 31 Epoch 3: train loss 0.08401256054639816 val loss 10.616065979003906\n",
      "Batch 32 Epoch 3: train loss 0.10487498342990875 val loss 10.332186698913574\n",
      "Batch 33 Epoch 3: train loss 0.42839479446411133 val loss 9.90257740020752\n",
      "Batch 34 Epoch 3: train loss 0.37240004539489746 val loss 9.312893867492676\n",
      "Batch 35 Epoch 3: train loss 0.11180809885263443 val loss 8.708209037780762\n",
      "Batch 36 Epoch 3: train loss 0.026531590148806572 val loss 8.168953895568848\n",
      "Batch 37 Epoch 3: train loss 0.3675938844680786 val loss 7.569215774536133\n",
      "Batch 38 Epoch 3: train loss 0.7306094169616699 val loss 6.897486209869385\n",
      "Batch 39 Epoch 3: train loss 0.35679394006729126 val loss 6.173559665679932\n",
      "Batch 40 Epoch 3: train loss 0.1954958289861679 val loss 5.4338765144348145\n",
      "Batch 41 Epoch 3: train loss 0.041981570422649384 val loss 4.757216453552246\n",
      "Batch 42 Epoch 3: train loss 0.42100292444229126 val loss 4.0219807624816895\n",
      "Batch 43 Epoch 3: train loss 0.31667959690093994 val loss 3.171210289001465\n",
      "Batch 44 Epoch 3: train loss 0.4545183479785919 val loss 0.010797174647450447\n",
      "Batch 45 Epoch 3: train loss 0.39384719729423523 val loss 0.008132786490023136\n",
      "Batch 46 Epoch 3: train loss 0.099159836769104 val loss 0.024609480053186417\n",
      "Batch 47 Epoch 3: train loss 0.5795873999595642 val loss 0.0273001529276371\n",
      "Batch 48 Epoch 3: train loss 0.5479844808578491 val loss 0.043492019176483154\n",
      "Batch 49 Epoch 3: train loss 0.2578738033771515 val loss 0.015765396878123283\n",
      "Batch 0 Epoch 4: train loss 1.9935296773910522 val loss 0.014674355275928974\n",
      "Batch 1 Epoch 4: train loss 1.5810351371765137 val loss 0.011719781905412674\n",
      "Batch 2 Epoch 4: train loss 1.2057955265045166 val loss 0.04528138414025307\n",
      "Batch 3 Epoch 4: train loss 0.4280904233455658 val loss 2.311471462249756\n",
      "Batch 4 Epoch 4: train loss 0.28123289346694946 val loss 2.981978178024292\n",
      "Batch 5 Epoch 4: train loss 0.23110289871692657 val loss 3.6546926498413086\n",
      "Batch 6 Epoch 4: train loss 0.0797029510140419 val loss 4.292710781097412\n",
      "Batch 7 Epoch 4: train loss 0.02544681914150715 val loss 4.880001544952393\n",
      "Batch 8 Epoch 4: train loss 0.008714011870324612 val loss 5.44306755065918\n",
      "Batch 9 Epoch 4: train loss 0.045964278280735016 val loss 6.013016223907471\n",
      "Batch 10 Epoch 4: train loss 0.1127268597483635 val loss 6.624837875366211\n",
      "Batch 11 Epoch 4: train loss 0.1093066930770874 val loss 7.269747734069824\n",
      "Batch 12 Epoch 4: train loss 0.08410029858350754 val loss 7.912393569946289\n",
      "Batch 13 Epoch 4: train loss 0.09021531790494919 val loss 8.517435073852539\n",
      "Batch 14 Epoch 4: train loss 0.01996764913201332 val loss 9.035659790039062\n",
      "Batch 15 Epoch 4: train loss 0.003913130145519972 val loss 9.46253490447998\n",
      "Batch 16 Epoch 4: train loss 0.05199122428894043 val loss 9.811394691467285\n",
      "Batch 17 Epoch 4: train loss 0.06440303474664688 val loss 10.09467601776123\n",
      "Batch 18 Epoch 4: train loss 0.07903291285037994 val loss 10.317972183227539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 19 Epoch 4: train loss 0.08651621639728546 val loss 10.493163108825684\n",
      "Batch 20 Epoch 4: train loss 0.09556003659963608 val loss 10.625870704650879\n",
      "Batch 21 Epoch 4: train loss 0.08066173642873764 val loss 10.723808288574219\n",
      "Batch 22 Epoch 4: train loss 0.0777539536356926 val loss 10.78750991821289\n",
      "Batch 23 Epoch 4: train loss 0.04731299355626106 val loss 10.816852569580078\n",
      "Batch 24 Epoch 4: train loss 0.016850359737873077 val loss 10.820268630981445\n",
      "Batch 25 Epoch 4: train loss 0.0013279467821121216 val loss 10.809545516967773\n",
      "Batch 26 Epoch 4: train loss 0.014455579221248627 val loss 10.808897018432617\n",
      "Batch 27 Epoch 4: train loss 0.03776482865214348 val loss 10.829625129699707\n",
      "Batch 28 Epoch 4: train loss 0.07004021853208542 val loss 10.880834579467773\n",
      "Batch 29 Epoch 4: train loss 0.10314767807722092 val loss 10.952253341674805\n",
      "Batch 30 Epoch 4: train loss 0.08909499645233154 val loss 11.020768165588379\n",
      "Batch 31 Epoch 4: train loss 0.06887239962816238 val loss 11.068992614746094\n",
      "Batch 32 Epoch 4: train loss 0.08174794167280197 val loss 11.083322525024414\n",
      "Batch 33 Epoch 4: train loss 0.3649948537349701 val loss 10.997366905212402\n",
      "Batch 34 Epoch 4: train loss 0.33085182309150696 val loss 10.737900733947754\n",
      "Batch 35 Epoch 4: train loss 0.10343718528747559 val loss 10.418503761291504\n",
      "Batch 36 Epoch 4: train loss 0.01745445281267166 val loss 10.110649108886719\n",
      "Batch 37 Epoch 4: train loss 0.33209508657455444 val loss 9.729607582092285\n",
      "Batch 38 Epoch 4: train loss 0.6981999278068542 val loss 9.14706802368164\n",
      "Batch 39 Epoch 4: train loss 0.4764455258846283 val loss 8.487590789794922\n",
      "Batch 40 Epoch 4: train loss 0.30720996856689453 val loss 7.822247505187988\n",
      "Batch 41 Epoch 4: train loss 0.11230606585741043 val loss 7.208024978637695\n",
      "Batch 42 Epoch 4: train loss 0.6008604168891907 val loss 6.574719429016113\n",
      "Batch 43 Epoch 4: train loss 0.68100905418396 val loss 5.8286261558532715\n",
      "Batch 44 Epoch 4: train loss 1.0087883472442627 val loss 4.86643648147583\n",
      "Batch 45 Epoch 4: train loss 1.16023588180542 val loss 3.586498498916626\n",
      "Batch 46 Epoch 4: train loss 0.743026614189148 val loss 0.009100208058953285\n",
      "Batch 47 Epoch 4: train loss 0.02159387618303299 val loss 0.04678766056895256\n",
      "Batch 48 Epoch 4: train loss 0.08900735527276993 val loss 0.03945044428110123\n",
      "Batch 49 Epoch 4: train loss 0.07794986665248871 val loss 0.020804625004529953\n",
      "Batch 0 Epoch 5: train loss 1.6952661275863647 val loss 0.03866547718644142\n",
      "Batch 1 Epoch 5: train loss 1.470040202140808 val loss 0.03955821692943573\n",
      "Batch 2 Epoch 5: train loss 1.1076101064682007 val loss 0.028010820969939232\n",
      "Batch 3 Epoch 5: train loss 0.32876601815223694 val loss 0.04182211682200432\n",
      "Batch 4 Epoch 5: train loss 0.21784836053848267 val loss 0.015205975621938705\n",
      "Batch 5 Epoch 5: train loss 0.08794675767421722 val loss 0.032177284359931946\n",
      "Batch 6 Epoch 5: train loss 0.02838655188679695 val loss 0.015742482617497444\n",
      "Batch 7 Epoch 5: train loss 0.23605172336101532 val loss 0.019606739282608032\n",
      "Batch 8 Epoch 5: train loss 0.5207908749580383 val loss 0.03865649923682213\n",
      "Batch 9 Epoch 5: train loss 0.3159816563129425 val loss 0.0334993377327919\n",
      "Batch 10 Epoch 5: train loss 0.15300621092319489 val loss 0.013829471543431282\n",
      "Batch 11 Epoch 5: train loss 0.02565690502524376 val loss 0.3971467912197113\n",
      "Batch 12 Epoch 5: train loss 0.0012755305506289005 val loss 0.9697206616401672\n",
      "Batch 13 Epoch 5: train loss 0.00782917719334364 val loss 1.6287246942520142\n",
      "Batch 14 Epoch 5: train loss 0.04735611751675606 val loss 2.2567899227142334\n",
      "Batch 15 Epoch 5: train loss 0.07369804382324219 val loss 2.7987992763519287\n",
      "Batch 16 Epoch 5: train loss 0.10654348134994507 val loss 3.2428340911865234\n",
      "Batch 17 Epoch 5: train loss 0.16228683292865753 val loss 3.5589749813079834\n",
      "Batch 18 Epoch 5: train loss 0.1609039604663849 val loss 3.737356662750244\n",
      "Batch 19 Epoch 5: train loss 0.16545577347278595 val loss 3.7854902744293213\n",
      "Batch 20 Epoch 5: train loss 0.15616092085838318 val loss 3.714885711669922\n",
      "Batch 21 Epoch 5: train loss 0.12054847180843353 val loss 3.5499155521392822\n",
      "Batch 22 Epoch 5: train loss 0.10804779082536697 val loss 3.303037405014038\n",
      "Batch 23 Epoch 5: train loss 0.06327041238546371 val loss 3.0015063285827637\n",
      "Batch 24 Epoch 5: train loss 0.02741861529648304 val loss 2.678886651992798\n",
      "Batch 25 Epoch 5: train loss 0.006998967844992876 val loss 2.3658218383789062\n",
      "Batch 26 Epoch 5: train loss 0.005014385096728802 val loss 2.10667085647583\n",
      "Batch 27 Epoch 5: train loss 0.016748642548918724 val loss 1.9181547164916992\n",
      "Batch 28 Epoch 5: train loss 0.04079794883728027 val loss 1.8154305219650269\n",
      "Batch 29 Epoch 5: train loss 0.08017633110284805 val loss 1.8102861642837524\n",
      "Batch 30 Epoch 5: train loss 0.08380699157714844 val loss 1.8878949880599976\n",
      "Batch 31 Epoch 5: train loss 0.09426034986972809 val loss 2.041823625564575\n",
      "Batch 32 Epoch 5: train loss 0.128554567694664 val loss 2.269352436065674\n",
      "Batch 33 Epoch 5: train loss 0.45993539690971375 val loss 2.587740182876587\n",
      "Batch 34 Epoch 5: train loss 0.2830219268798828 val loss 2.8674285411834717\n",
      "Batch 35 Epoch 5: train loss 0.07878613471984863 val loss 3.0946974754333496\n",
      "Batch 36 Epoch 5: train loss 0.05761660262942314 val loss 3.3019983768463135\n",
      "Batch 37 Epoch 5: train loss 0.4690888524055481 val loss 3.4623942375183105\n",
      "Batch 38 Epoch 5: train loss 0.5843859910964966 val loss 3.4679274559020996\n",
      "Batch 39 Epoch 5: train loss 0.34606215357780457 val loss 3.3163154125213623\n",
      "Batch 40 Epoch 5: train loss 0.25250744819641113 val loss 3.0527937412261963\n",
      "Batch 41 Epoch 5: train loss 0.11364948004484177 val loss 2.75830340385437\n",
      "Batch 42 Epoch 5: train loss 0.6529509425163269 val loss 2.3604612350463867\n",
      "Batch 43 Epoch 5: train loss 0.4773847162723541 val loss 1.8009434938430786\n",
      "Batch 44 Epoch 5: train loss 0.6799235343933105 val loss 1.0594977140426636\n",
      "Batch 45 Epoch 5: train loss 0.6388495564460754 val loss 0.017522713169455528\n",
      "Batch 46 Epoch 5: train loss 0.23055550456047058 val loss 0.013647043146193027\n",
      "Batch 47 Epoch 5: train loss 0.13846243917942047 val loss 0.01796419359743595\n",
      "Batch 48 Epoch 5: train loss 0.04472668096423149 val loss 0.008731411769986153\n",
      "Batch 49 Epoch 5: train loss 0.007037372328341007 val loss 0.03378581628203392\n",
      "Batch 0 Epoch 6: train loss 0.30358314514160156 val loss 3.9592063426971436\n",
      "Batch 1 Epoch 6: train loss 0.31671836972236633 val loss 5.989538192749023\n",
      "Batch 2 Epoch 6: train loss 0.32812368869781494 val loss 8.692193031311035\n",
      "Batch 3 Epoch 6: train loss 0.10305934399366379 val loss 12.044770240783691\n",
      "Batch 4 Epoch 6: train loss 0.19029414653778076 val loss 16.10687255859375\n",
      "Batch 5 Epoch 6: train loss 0.22450637817382812 val loss 21.197011947631836\n",
      "Batch 6 Epoch 6: train loss 0.08167877048254013 val loss 27.477252960205078\n",
      "Batch 7 Epoch 6: train loss 0.02459041029214859 val loss 34.81147003173828\n",
      "Batch 8 Epoch 6: train loss 0.00991036742925644 val loss 42.07902526855469\n",
      "Batch 9 Epoch 6: train loss 0.028728775680065155 val loss 48.48737335205078\n",
      "Batch 10 Epoch 6: train loss 0.08464445173740387 val loss 52.85860824584961\n",
      "Batch 11 Epoch 6: train loss 0.11013393104076385 val loss 54.602012634277344\n",
      "Batch 12 Epoch 6: train loss 0.11255792528390884 val loss 53.69874572753906\n",
      "Batch 13 Epoch 6: train loss 0.11982466280460358 val loss 50.50991439819336\n",
      "Batch 14 Epoch 6: train loss 0.01710681803524494 val loss 46.85178756713867\n",
      "Batch 15 Epoch 6: train loss 0.0074092005379498005 val loss 43.16147994995117\n",
      "Batch 16 Epoch 6: train loss 0.126418799161911 val loss 41.33958053588867\n",
      "Batch 17 Epoch 6: train loss 0.06356342136859894 val loss 40.68183898925781\n",
      "Batch 18 Epoch 6: train loss 0.013025161810219288 val loss 40.62744903564453\n",
      "Batch 19 Epoch 6: train loss 0.012638892978429794 val loss 41.1170768737793\n",
      "Batch 20 Epoch 6: train loss 0.013089346699416637 val loss 42.11729049682617\n",
      "Batch 21 Epoch 6: train loss 0.012625136412680149 val loss 43.604949951171875\n",
      "Batch 22 Epoch 6: train loss 0.01602928712964058 val loss 45.6750602722168\n",
      "Batch 23 Epoch 6: train loss 0.0059970528818666935 val loss 48.04502868652344\n",
      "Batch 24 Epoch 6: train loss 0.002701960736885667 val loss 50.58660888671875\n",
      "Batch 25 Epoch 6: train loss 0.0008377207559533417 val loss 53.13195037841797\n",
      "Batch 26 Epoch 6: train loss 0.004914766643196344 val loss 55.178829193115234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 27 Epoch 6: train loss 0.0033337650820612907 val loss 56.86941909790039\n",
      "Batch 28 Epoch 6: train loss 0.00542596448212862 val loss 57.94301986694336\n",
      "Batch 29 Epoch 6: train loss 0.009460976347327232 val loss 58.2602653503418\n",
      "Batch 30 Epoch 6: train loss 0.005445307586342096 val loss 58.10042953491211\n",
      "Batch 31 Epoch 6: train loss 0.0056892745196819305 val loss 57.4998779296875\n",
      "Batch 32 Epoch 6: train loss 0.015192939899861813 val loss 56.207313537597656\n",
      "Batch 33 Epoch 6: train loss 0.1223142147064209 val loss 53.36030960083008\n",
      "Batch 34 Epoch 6: train loss 0.011392812244594097 val loss 50.814083099365234\n",
      "Batch 35 Epoch 6: train loss 0.12603913247585297 val loss 48.42646026611328\n",
      "Batch 36 Epoch 6: train loss 0.03460242226719856 val loss 46.41064453125\n",
      "Batch 37 Epoch 6: train loss 0.09366012364625931 val loss 44.213932037353516\n",
      "Batch 38 Epoch 6: train loss 0.03673018515110016 val loss 41.542564392089844\n",
      "Batch 39 Epoch 6: train loss 0.1172390729188919 val loss 35.96449279785156\n",
      "Batch 40 Epoch 6: train loss 0.1838558316230774 val loss 27.498289108276367\n",
      "Batch 41 Epoch 6: train loss 0.129176527261734 val loss 19.363142013549805\n",
      "Batch 42 Epoch 6: train loss 0.17044472694396973 val loss 0.03228210285305977\n",
      "Batch 43 Epoch 6: train loss 0.04759274423122406 val loss 0.030764175578951836\n",
      "Batch 44 Epoch 6: train loss 0.019174030050635338 val loss 0.012393699958920479\n",
      "Batch 45 Epoch 6: train loss 0.006144918035715818 val loss 0.014515108428895473\n",
      "Batch 46 Epoch 6: train loss 0.14637711644172668 val loss 0.044283609837293625\n",
      "Batch 47 Epoch 6: train loss 0.09217856824398041 val loss 0.026361428201198578\n",
      "Batch 48 Epoch 6: train loss 0.08472304791212082 val loss 0.1673332303762436\n",
      "Batch 49 Epoch 6: train loss 0.2575960159301758 val loss 0.5247475504875183\n",
      "Batch 0 Epoch 7: train loss 0.11646850407123566 val loss 0.9026585221290588\n",
      "Batch 1 Epoch 7: train loss 0.1279156655073166 val loss 1.2362167835235596\n",
      "Batch 2 Epoch 7: train loss 0.1563723385334015 val loss 1.4962255954742432\n",
      "Batch 3 Epoch 7: train loss 0.0400417223572731 val loss 1.7085158824920654\n",
      "Batch 4 Epoch 7: train loss 0.05038336664438248 val loss 1.8788875341415405\n",
      "Batch 5 Epoch 7: train loss 0.13373251259326935 val loss 1.9918112754821777\n",
      "Batch 6 Epoch 7: train loss 0.1591782420873642 val loss 2.0408036708831787\n",
      "Batch 7 Epoch 7: train loss 0.17946673929691315 val loss 2.0192008018493652\n",
      "Batch 8 Epoch 7: train loss 0.10287664085626602 val loss 1.9471871852874756\n",
      "Batch 9 Epoch 7: train loss 0.08982022851705551 val loss 1.833986520767212\n",
      "Batch 10 Epoch 7: train loss 0.052434999495744705 val loss 1.6959973573684692\n",
      "Batch 11 Epoch 7: train loss 0.02918100357055664 val loss 1.5443527698516846\n",
      "Batch 12 Epoch 7: train loss 0.006849032361060381 val loss 1.3971962928771973\n",
      "Batch 13 Epoch 7: train loss 0.017629923298954964 val loss 1.2918349504470825\n",
      "Batch 14 Epoch 7: train loss 0.01564609818160534 val loss 1.2231965065002441\n",
      "Batch 15 Epoch 7: train loss 0.03231227025389671 val loss 1.1968928575515747\n",
      "Batch 16 Epoch 7: train loss 0.0953807607293129 val loss 1.2467154264450073\n",
      "Batch 17 Epoch 7: train loss 0.04954904317855835 val loss 1.3428466320037842\n",
      "Batch 18 Epoch 7: train loss 0.0337354876101017 val loss 1.4723637104034424\n",
      "Batch 19 Epoch 7: train loss 0.017905039712786674 val loss 1.6193183660507202\n",
      "Batch 20 Epoch 7: train loss 0.007978764362633228 val loss 1.7713122367858887\n",
      "Batch 21 Epoch 7: train loss 0.003217054298147559 val loss 1.919980764389038\n",
      "Batch 22 Epoch 7: train loss 0.0005268591339699924 val loss 2.0507454872131348\n",
      "Batch 23 Epoch 7: train loss 0.001652842154726386 val loss 2.1612930297851562\n",
      "Batch 24 Epoch 7: train loss 0.003545589279383421 val loss 2.248473644256592\n",
      "Batch 25 Epoch 7: train loss 0.006403980776667595 val loss 2.3107221126556396\n",
      "Batch 26 Epoch 7: train loss 0.004510651342570782 val loss 2.3579065799713135\n",
      "Batch 27 Epoch 7: train loss 0.004703848157078028 val loss 2.389700412750244\n",
      "Batch 28 Epoch 7: train loss 0.001660570502281189 val loss 2.412379264831543\n",
      "Batch 29 Epoch 7: train loss 0.0003998016763944179 val loss 2.4347355365753174\n",
      "Batch 30 Epoch 7: train loss 0.001453000702895224 val loss 2.456972360610962\n",
      "Batch 31 Epoch 7: train loss 0.002058969344943762 val loss 2.483170747756958\n",
      "Batch 32 Epoch 7: train loss 0.018236491829156876 val loss 2.5254411697387695\n",
      "Batch 33 Epoch 7: train loss 0.2192022204399109 val loss 2.614178419113159\n",
      "Batch 34 Epoch 7: train loss 0.1371021270751953 val loss 2.7022364139556885\n",
      "Batch 35 Epoch 7: train loss 0.049781788140535355 val loss 2.7788808345794678\n",
      "Batch 36 Epoch 7: train loss 0.03846185281872749 val loss 2.8568999767303467\n",
      "Batch 37 Epoch 7: train loss 0.3648412227630615 val loss 2.9387707710266113\n",
      "Batch 38 Epoch 7: train loss 0.2633255422115326 val loss 2.9847915172576904\n",
      "Batch 39 Epoch 7: train loss 0.28782686591148376 val loss 2.9624340534210205\n",
      "Batch 40 Epoch 7: train loss 0.2664034068584442 val loss 2.8735368251800537\n",
      "Batch 41 Epoch 7: train loss 0.14406348764896393 val loss 2.754366159439087\n",
      "Batch 42 Epoch 7: train loss 0.6695680022239685 val loss 2.570690393447876\n",
      "Batch 43 Epoch 7: train loss 0.49343374371528625 val loss 2.2714502811431885\n",
      "Batch 44 Epoch 7: train loss 0.7492918372154236 val loss 1.8078112602233887\n",
      "Batch 45 Epoch 7: train loss 0.8522353768348694 val loss 1.1605463027954102\n",
      "Batch 46 Epoch 7: train loss 0.4725019633769989 val loss 0.03663475811481476\n",
      "Batch 47 Epoch 7: train loss 0.0053670066408813 val loss 0.03404791280627251\n",
      "Batch 48 Epoch 7: train loss 0.02274375408887863 val loss 0.01188282947987318\n",
      "Batch 49 Epoch 7: train loss 0.016327781602740288 val loss 0.02059042453765869\n",
      "Batch 0 Epoch 8: train loss 0.18636444211006165 val loss 0.018070468679070473\n",
      "Batch 1 Epoch 8: train loss 0.1828792691230774 val loss 2.1389007568359375\n",
      "Batch 2 Epoch 8: train loss 0.18586283922195435 val loss 3.9511334896087646\n",
      "Batch 3 Epoch 8: train loss 0.040034279227256775 val loss 6.39837121963501\n",
      "Batch 4 Epoch 8: train loss 0.10975402593612671 val loss 9.548291206359863\n",
      "Batch 5 Epoch 8: train loss 0.12875880300998688 val loss 13.57740306854248\n",
      "Batch 6 Epoch 8: train loss 0.03242417797446251 val loss 18.526813507080078\n",
      "Batch 7 Epoch 8: train loss 0.004648760426789522 val loss 24.176925659179688\n",
      "Batch 8 Epoch 8: train loss 0.031691111624240875 val loss 29.646644592285156\n",
      "Batch 9 Epoch 8: train loss 0.04560059309005737 val loss 34.39186477661133\n",
      "Batch 10 Epoch 8: train loss 0.08217301219701767 val loss 37.722259521484375\n",
      "Batch 11 Epoch 8: train loss 0.09083816409111023 val loss 39.30692672729492\n",
      "Batch 12 Epoch 8: train loss 0.0795484408736229 val loss 39.239097595214844\n",
      "Batch 13 Epoch 8: train loss 0.08622276037931442 val loss 37.75031280517578\n",
      "Batch 14 Epoch 8: train loss 0.0057193199172616005 val loss 36.06587600708008\n",
      "Batch 15 Epoch 8: train loss 0.0011776909232139587 val loss 34.45555114746094\n",
      "Batch 16 Epoch 8: train loss 0.0792970284819603 val loss 33.7486686706543\n",
      "Batch 17 Epoch 8: train loss 0.03948241472244263 val loss 33.67212677001953\n",
      "Batch 18 Epoch 8: train loss 0.011591928079724312 val loss 33.986724853515625\n",
      "Batch 19 Epoch 8: train loss 0.01566297560930252 val loss 34.7445182800293\n",
      "Batch 20 Epoch 8: train loss 0.01692192442715168 val loss 35.95428466796875\n",
      "Batch 21 Epoch 8: train loss 0.014315077103674412 val loss 37.571441650390625\n",
      "Batch 22 Epoch 8: train loss 0.013977527618408203 val loss 39.622314453125\n",
      "Batch 23 Epoch 8: train loss 0.0033669101539999247 val loss 41.809471130371094\n",
      "Batch 24 Epoch 8: train loss 0.0004379009478725493 val loss 43.95856475830078\n",
      "Batch 25 Epoch 8: train loss 0.000412056251661852 val loss 45.901390075683594\n",
      "Batch 26 Epoch 8: train loss 0.008816124871373177 val loss 47.231117248535156\n",
      "Batch 27 Epoch 8: train loss 0.005101766437292099 val loss 48.183902740478516\n",
      "Batch 28 Epoch 8: train loss 0.006232221610844135 val loss 48.61623764038086\n",
      "Batch 29 Epoch 8: train loss 0.007802749518305063 val loss 48.539058685302734\n",
      "Batch 30 Epoch 8: train loss 0.002873073099181056 val loss 48.259708404541016\n",
      "Batch 31 Epoch 8: train loss 0.002672590548172593 val loss 47.836578369140625\n",
      "Batch 32 Epoch 8: train loss 0.0054651228711009026 val loss 47.1411247253418\n",
      "Batch 33 Epoch 8: train loss 0.07539474964141846 val loss 45.67214584350586\n",
      "Batch 34 Epoch 8: train loss 0.05921047925949097 val loss 44.01332473754883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 35 Epoch 8: train loss 0.27490365505218506 val loss 41.64023208618164\n",
      "Batch 36 Epoch 8: train loss 0.059590961784124374 val loss 39.63991165161133\n",
      "Batch 37 Epoch 8: train loss 0.057327400892972946 val loss 37.606510162353516\n",
      "Batch 38 Epoch 8: train loss 0.08241593092679977 val loss 33.890159606933594\n",
      "Batch 39 Epoch 8: train loss 0.24127882719039917 val loss 26.748693466186523\n",
      "Batch 40 Epoch 8: train loss 0.355132520198822 val loss 17.607269287109375\n",
      "Batch 41 Epoch 8: train loss 0.19307172298431396 val loss 10.172204971313477\n",
      "Batch 42 Epoch 8: train loss 0.16321532428264618 val loss 0.01826012320816517\n",
      "Batch 43 Epoch 8: train loss 0.009307396598160267 val loss 0.014956760220229626\n",
      "Batch 44 Epoch 8: train loss 0.031047264114022255 val loss 0.0118170864880085\n",
      "Batch 45 Epoch 8: train loss 0.014607779681682587 val loss 0.02229023352265358\n",
      "Batch 46 Epoch 8: train loss 0.0970897376537323 val loss 0.016379080712795258\n",
      "Batch 47 Epoch 8: train loss 0.07962475717067719 val loss 0.0337384007871151\n",
      "Batch 48 Epoch 8: train loss 0.10018587112426758 val loss 0.06529868394136429\n",
      "Batch 49 Epoch 8: train loss 0.2703321874141693 val loss 0.16086475551128387\n",
      "Batch 0 Epoch 9: train loss 0.00852962862700224 val loss 0.2669968903064728\n",
      "Batch 1 Epoch 9: train loss 0.013579539023339748 val loss 0.36580103635787964\n",
      "Batch 2 Epoch 9: train loss 0.01839321479201317 val loss 0.4435316026210785\n",
      "Batch 3 Epoch 9: train loss 0.0025381094310432673 val loss 0.5219706296920776\n",
      "Batch 4 Epoch 9: train loss 0.0038058843929320574 val loss 0.5976770520210266\n",
      "Batch 5 Epoch 9: train loss 0.02525261789560318 val loss 0.6437653303146362\n",
      "Batch 6 Epoch 9: train loss 0.05406760051846504 val loss 0.6435868144035339\n",
      "Batch 7 Epoch 9: train loss 0.06129404902458191 val loss 0.5931071639060974\n",
      "Batch 8 Epoch 9: train loss 0.02510404959321022 val loss 0.5178002119064331\n",
      "Batch 9 Epoch 9: train loss 0.0240899920463562 val loss 0.42561084032058716\n",
      "Batch 10 Epoch 9: train loss 0.011403939686715603 val loss 0.33349859714508057\n",
      "Batch 11 Epoch 9: train loss 0.00412118062376976 val loss 0.25061875581741333\n",
      "Batch 12 Epoch 9: train loss 0.0008672223193570971 val loss 0.1869603842496872\n",
      "Batch 13 Epoch 9: train loss 0.038205586373806 val loss 0.15967053174972534\n",
      "Batch 14 Epoch 9: train loss 0.022941788658499718 val loss 0.15360315144062042\n",
      "Batch 15 Epoch 9: train loss 0.02121039293706417 val loss 0.16476477682590485\n",
      "Batch 16 Epoch 9: train loss 0.3478684425354004 val loss 0.2616121470928192\n",
      "Batch 17 Epoch 9: train loss 0.08554445207118988 val loss 0.4113292098045349\n",
      "Batch 18 Epoch 9: train loss 0.005505971610546112 val loss 0.5814498066902161\n",
      "Batch 19 Epoch 9: train loss 0.0070891911163926125 val loss 0.7351678609848022\n",
      "Batch 20 Epoch 9: train loss 0.026230184361338615 val loss 0.8479418754577637\n",
      "Batch 21 Epoch 9: train loss 0.03625144064426422 val loss 0.9117203950881958\n",
      "Batch 22 Epoch 9: train loss 0.057589855045080185 val loss 0.9178050756454468\n",
      "Batch 23 Epoch 9: train loss 0.05533560737967491 val loss 0.872379720211029\n",
      "Batch 24 Epoch 9: train loss 0.05004391074180603 val loss 0.7865139842033386\n",
      "Batch 25 Epoch 9: train loss 0.04189988970756531 val loss 0.673463761806488\n",
      "Batch 26 Epoch 9: train loss 0.01956033892929554 val loss 0.5552924871444702\n",
      "Batch 27 Epoch 9: train loss 0.012052268721163273 val loss 0.44199860095977783\n",
      "Batch 28 Epoch 9: train loss 0.002652018563821912 val loss 0.3439333438873291\n",
      "Batch 29 Epoch 9: train loss 0.0012440936407074332 val loss 0.2688577473163605\n",
      "Batch 30 Epoch 9: train loss 0.004452669061720371 val loss 0.21453271806240082\n",
      "Batch 31 Epoch 9: train loss 0.012087542563676834 val loss 0.18065859377384186\n",
      "Batch 32 Epoch 9: train loss 0.04094002768397331 val loss 0.1695738136768341\n",
      "Batch 33 Epoch 9: train loss 0.2884403467178345 val loss 0.19653968513011932\n",
      "Batch 34 Epoch 9: train loss 0.14968305826187134 val loss 0.2315400242805481\n",
      "Batch 35 Epoch 9: train loss 0.03848845139145851 val loss 0.2658885419368744\n",
      "Batch 36 Epoch 9: train loss 0.055830515921115875 val loss 0.3099507987499237\n",
      "Batch 37 Epoch 9: train loss 0.37111449241638184 val loss 0.3674668073654175\n",
      "Batch 38 Epoch 9: train loss 0.37569183111190796 val loss 0.3890638053417206\n",
      "Batch 39 Epoch 9: train loss 0.19603541493415833 val loss 0.3648907542228699\n",
      "Batch 40 Epoch 9: train loss 0.10465165227651596 val loss 0.31235775351524353\n",
      "Batch 41 Epoch 9: train loss 0.04452155530452728 val loss 0.2536981999874115\n",
      "Batch 42 Epoch 9: train loss 0.4236591160297394 val loss 0.035800471901893616\n",
      "Batch 43 Epoch 9: train loss 0.16246090829372406 val loss 0.018598085269331932\n",
      "Batch 44 Epoch 9: train loss 0.21436557173728943 val loss 0.04702085256576538\n",
      "Batch 45 Epoch 9: train loss 0.159376859664917 val loss 0.018742162734270096\n",
      "Batch 46 Epoch 9: train loss 0.05729609355330467 val loss 0.011201123706996441\n",
      "Batch 47 Epoch 9: train loss 0.23308272659778595 val loss 0.013113274239003658\n",
      "Batch 48 Epoch 9: train loss 0.025021348148584366 val loss 0.016243936493992805\n",
      "Batch 49 Epoch 9: train loss 0.0013292576186358929 val loss 0.020347723737359047\n",
      "Batch 0 Epoch 10: train loss 0.15401600301265717 val loss 0.025815073400735855\n",
      "Batch 1 Epoch 10: train loss 0.15055356919765472 val loss 0.03262973204255104\n",
      "Batch 2 Epoch 10: train loss 0.16638819873332977 val loss 0.04117196798324585\n",
      "Batch 3 Epoch 10: train loss 0.04249729961156845 val loss 0.023132728412747383\n",
      "Batch 4 Epoch 10: train loss 0.09875471889972687 val loss 0.03224074840545654\n",
      "Batch 5 Epoch 10: train loss 0.13316264748573303 val loss 0.0436604768037796\n",
      "Batch 6 Epoch 10: train loss 0.07854316383600235 val loss 4.345003128051758\n",
      "Batch 7 Epoch 10: train loss 0.05192303657531738 val loss 5.392899513244629\n",
      "Batch 8 Epoch 10: train loss 0.00900235865265131 val loss 6.553049087524414\n",
      "Batch 9 Epoch 10: train loss 0.0032571665942668915 val loss 7.788389205932617\n",
      "Batch 10 Epoch 10: train loss 0.004504361189901829 val loss 8.979007720947266\n",
      "Batch 11 Epoch 10: train loss 0.009935183450579643 val loss 9.992264747619629\n",
      "Batch 12 Epoch 10: train loss 0.023492813110351562 val loss 10.696725845336914\n",
      "Batch 13 Epoch 10: train loss 0.06965567171573639 val loss 10.907270431518555\n",
      "Batch 14 Epoch 10: train loss 0.029700640588998795 val loss 10.802083969116211\n",
      "Batch 15 Epoch 10: train loss 0.035951755940914154 val loss 10.387187957763672\n",
      "Batch 16 Epoch 10: train loss 0.039298027753829956 val loss 10.235355377197266\n",
      "Batch 17 Epoch 10: train loss 0.007242607418447733 val loss 10.041220664978027\n",
      "Batch 18 Epoch 10: train loss 0.011730242520570755 val loss 9.69300365447998\n",
      "Batch 19 Epoch 10: train loss 0.014645087532699108 val loss 9.194099426269531\n",
      "Batch 20 Epoch 10: train loss 0.012085373513400555 val loss 8.595406532287598\n",
      "Batch 21 Epoch 10: train loss 0.005472694989293814 val loss 7.979647636413574\n",
      "Batch 22 Epoch 10: train loss 0.0003705148701556027 val loss 7.434838771820068\n",
      "Batch 23 Epoch 10: train loss 0.0005674732383340597 val loss 6.949841022491455\n",
      "Batch 24 Epoch 10: train loss 0.0007643879507668316 val loss 6.56107759475708\n",
      "Batch 25 Epoch 10: train loss 0.002542394446209073 val loss 6.278459072113037\n",
      "Batch 26 Epoch 10: train loss 0.0028354297392070293 val loss 6.050685405731201\n",
      "Batch 27 Epoch 10: train loss 0.007590726483613253 val loss 5.9263482093811035\n",
      "Batch 28 Epoch 10: train loss 0.0027582573238760233 val loss 5.862788677215576\n",
      "Batch 29 Epoch 10: train loss 0.0017962916754186153 val loss 5.840040683746338\n",
      "Batch 30 Epoch 10: train loss 0.0036126344930380583 val loss 5.864227294921875\n",
      "Batch 31 Epoch 10: train loss 0.002619932172819972 val loss 5.925043106079102\n",
      "Batch 32 Epoch 10: train loss 0.0025820897426456213 val loss 5.948586463928223\n",
      "Batch 33 Epoch 10: train loss 0.06123308464884758 val loss 5.8210015296936035\n",
      "Batch 34 Epoch 10: train loss 0.0035739948507398367 val loss 5.708944320678711\n",
      "Batch 35 Epoch 10: train loss 0.07136263698339462 val loss 5.623730182647705\n",
      "Batch 36 Epoch 10: train loss 0.021674394607543945 val loss 0.049709100276231766\n",
      "Batch 37 Epoch 10: train loss 0.09418651461601257 val loss 0.046936340630054474\n",
      "Batch 38 Epoch 10: train loss 0.1900220513343811 val loss 0.03298172727227211\n",
      "Batch 39 Epoch 10: train loss 0.04024307057261467 val loss 0.039979416877031326\n",
      "Batch 40 Epoch 10: train loss 0.034212104976177216 val loss 0.02764955163002014\n",
      "Batch 41 Epoch 10: train loss 0.02047751471400261 val loss 0.018476391211152077\n",
      "Batch 42 Epoch 10: train loss 0.22483372688293457 val loss 0.01399258617311716\n",
      "Batch 43 Epoch 10: train loss 0.01938420720398426 val loss 0.011375700123608112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 44 Epoch 10: train loss 0.016192132607102394 val loss 0.010865534655749798\n",
      "Batch 45 Epoch 10: train loss 0.003109475364908576 val loss 0.01147445384413004\n",
      "Batch 46 Epoch 10: train loss 0.13275255262851715 val loss 0.017061756923794746\n",
      "Batch 47 Epoch 10: train loss 0.14394378662109375 val loss 0.03628866747021675\n",
      "Batch 48 Epoch 10: train loss 0.03171810507774353 val loss 0.028667163103818893\n",
      "Batch 49 Epoch 10: train loss 0.09741930663585663 val loss 0.02911754511296749\n",
      "Batch 0 Epoch 11: train loss 0.06423018872737885 val loss 0.045025765895843506\n",
      "Batch 1 Epoch 11: train loss 0.06923496723175049 val loss 0.2261558175086975\n",
      "Batch 2 Epoch 11: train loss 0.08426883816719055 val loss 0.30111607909202576\n",
      "Batch 3 Epoch 11: train loss 0.014758226461708546 val loss 0.3646329641342163\n",
      "Batch 4 Epoch 11: train loss 0.030445890501141548 val loss 0.41279181838035583\n",
      "Batch 5 Epoch 11: train loss 0.08191413432359695 val loss 0.4310757517814636\n",
      "Batch 6 Epoch 11: train loss 0.09198710322380066 val loss 0.41317644715309143\n",
      "Batch 7 Epoch 11: train loss 0.09349044412374496 val loss 0.35929688811302185\n",
      "Batch 8 Epoch 11: train loss 0.044489048421382904 val loss 0.2883630394935608\n",
      "Batch 9 Epoch 11: train loss 0.037263449281454086 val loss 0.2098749727010727\n",
      "Batch 10 Epoch 11: train loss 0.018384581431746483 val loss 0.1374681442975998\n",
      "Batch 11 Epoch 11: train loss 0.005982416681945324 val loss 0.047160737216472626\n",
      "Batch 12 Epoch 11: train loss 0.0010565097909420729 val loss 0.039236582815647125\n",
      "Batch 13 Epoch 11: train loss 0.031860239803791046 val loss 0.03599754348397255\n",
      "Batch 14 Epoch 11: train loss 0.019051697105169296 val loss 0.03488142043352127\n",
      "Batch 15 Epoch 11: train loss 0.02875356934964657 val loss 0.035200558602809906\n",
      "Batch 16 Epoch 11: train loss 0.09375935047864914 val loss 0.037707891315221786\n",
      "Batch 17 Epoch 11: train loss 0.032789528369903564 val loss 0.04299398511648178\n",
      "Batch 18 Epoch 11: train loss 0.019083496183156967 val loss 0.09383438527584076\n",
      "Batch 19 Epoch 11: train loss 0.004529036581516266 val loss 0.14475084841251373\n",
      "Batch 20 Epoch 11: train loss 0.0010874869767576456 val loss 0.20052236318588257\n",
      "Batch 21 Epoch 11: train loss 0.0009183914517052472 val loss 0.254693865776062\n",
      "Batch 22 Epoch 11: train loss 0.0055822208523750305 val loss 0.29807186126708984\n",
      "Batch 23 Epoch 11: train loss 0.006103368941694498 val loss 0.32875683903694153\n",
      "Batch 24 Epoch 11: train loss 0.008707384578883648 val loss 0.34431976079940796\n",
      "Batch 25 Epoch 11: train loss 0.009934412315487862 val loss 0.34488821029663086\n",
      "Batch 26 Epoch 11: train loss 0.00520497327670455 val loss 0.33824461698532104\n",
      "Batch 27 Epoch 11: train loss 0.00633455254137516 val loss 0.32367053627967834\n",
      "Batch 28 Epoch 11: train loss 0.001541321980766952 val loss 0.3070856034755707\n",
      "Batch 29 Epoch 11: train loss 0.00047075096517801285 val loss 0.29329073429107666\n",
      "Batch 30 Epoch 11: train loss 0.0015616369200870395 val loss 0.28258341550827026\n",
      "Batch 31 Epoch 11: train loss 0.0020587905310094357 val loss 0.27687227725982666\n",
      "Batch 32 Epoch 11: train loss 0.015649747103452682 val loss 0.2837246358394623\n",
      "Batch 33 Epoch 11: train loss 0.17244426906108856 val loss 0.32217419147491455\n",
      "Batch 34 Epoch 11: train loss 0.05776128172874451 val loss 0.3651949167251587\n",
      "Batch 35 Epoch 11: train loss 0.021695494651794434 val loss 0.40580710768699646\n",
      "Batch 36 Epoch 11: train loss 0.028733570128679276 val loss 0.452261358499527\n",
      "Batch 37 Epoch 11: train loss 0.24195106327533722 val loss 0.5107056498527527\n",
      "Batch 38 Epoch 11: train loss 0.18336248397827148 val loss 0.5378409028053284\n",
      "Batch 39 Epoch 11: train loss 0.11590858548879623 val loss 0.5326444506645203\n",
      "Batch 40 Epoch 11: train loss 0.07834715396165848 val loss 0.5037391781806946\n",
      "Batch 41 Epoch 11: train loss 0.038744036108255386 val loss 0.4654335081577301\n",
      "Batch 42 Epoch 11: train loss 0.412541002035141 val loss 0.3942573070526123\n",
      "Batch 43 Epoch 11: train loss 0.17187079787254333 val loss 0.04619889333844185\n",
      "Batch 44 Epoch 11: train loss 0.25239527225494385 val loss 0.021822327747941017\n",
      "Batch 45 Epoch 11: train loss 0.22850507497787476 val loss 0.03675466403365135\n",
      "Batch 46 Epoch 11: train loss 0.06703122705221176 val loss 0.01597771793603897\n",
      "Batch 47 Epoch 11: train loss 0.1077868640422821 val loss 0.011029274202883244\n",
      "Batch 48 Epoch 11: train loss 0.015484431758522987 val loss 0.012259364128112793\n",
      "Batch 49 Epoch 11: train loss 0.002792057115584612 val loss 0.01859583891928196\n",
      "Batch 0 Epoch 12: train loss 0.06350493431091309 val loss 0.030066095292568207\n",
      "Batch 1 Epoch 12: train loss 0.05544783174991608 val loss 0.04661944881081581\n",
      "Batch 2 Epoch 12: train loss 0.05732537433505058 val loss 0.04266415536403656\n",
      "Batch 3 Epoch 12: train loss 0.0059365141205489635 val loss 2.8535220623016357\n",
      "Batch 4 Epoch 12: train loss 0.035832349210977554 val loss 3.640085220336914\n",
      "Batch 5 Epoch 12: train loss 0.04803383722901344 val loss 4.578408718109131\n",
      "Batch 6 Epoch 12: train loss 0.015161778777837753 val loss 5.654519081115723\n",
      "Batch 7 Epoch 12: train loss 0.004239299334585667 val loss 6.8152079582214355\n",
      "Batch 8 Epoch 12: train loss 0.006666433531790972 val loss 7.894737720489502\n",
      "Batch 9 Epoch 12: train loss 0.006765664555132389 val loss 8.844417572021484\n",
      "Batch 10 Epoch 12: train loss 0.01714899204671383 val loss 9.571358680725098\n",
      "Batch 11 Epoch 12: train loss 0.02285792864859104 val loss 9.996356010437012\n",
      "Batch 12 Epoch 12: train loss 0.022147348150610924 val loss 10.126721382141113\n",
      "Batch 13 Epoch 12: train loss 0.04646584764122963 val loss 9.912633895874023\n",
      "Batch 14 Epoch 12: train loss 0.006426006555557251 val loss 9.595032691955566\n",
      "Batch 15 Epoch 12: train loss 0.005278006661683321 val loss 9.210372924804688\n",
      "Batch 16 Epoch 12: train loss 0.06196872517466545 val loss 9.162035942077637\n",
      "Batch 17 Epoch 12: train loss 0.010624924674630165 val loss 9.225469589233398\n",
      "Batch 18 Epoch 12: train loss 0.0009725466370582581 val loss 9.317313194274902\n",
      "Batch 19 Epoch 12: train loss 0.00021865643793717027 val loss 9.401376724243164\n",
      "Batch 20 Epoch 12: train loss 0.0009243832319043577 val loss 9.459861755371094\n",
      "Batch 21 Epoch 12: train loss 0.0007072635344229639 val loss 9.515832901000977\n",
      "Batch 22 Epoch 12: train loss 0.000840257853269577 val loss 9.607704162597656\n",
      "Batch 23 Epoch 12: train loss 0.0006947079091332853 val loss 9.673492431640625\n",
      "Batch 24 Epoch 12: train loss 0.0001582121622050181 val loss 9.740727424621582\n",
      "Batch 25 Epoch 12: train loss 0.00025114553864113986 val loss 9.808671951293945\n",
      "Batch 26 Epoch 12: train loss 0.004241740796715021 val loss 9.796716690063477\n",
      "Batch 27 Epoch 12: train loss 0.0026131265331059694 val loss 9.818136215209961\n",
      "Batch 28 Epoch 12: train loss 0.000864654837641865 val loss 9.806591033935547\n",
      "Batch 29 Epoch 12: train loss 0.0010666324524208903 val loss 9.77293872833252\n",
      "Batch 30 Epoch 12: train loss 0.0014882896794006228 val loss 9.752437591552734\n",
      "Batch 31 Epoch 12: train loss 0.0012674930039793253 val loss 9.753318786621094\n",
      "Batch 32 Epoch 12: train loss 0.003564659273251891 val loss 9.683549880981445\n",
      "Batch 33 Epoch 12: train loss 0.053700096905231476 val loss 9.401983261108398\n",
      "Batch 34 Epoch 12: train loss 0.026009466499090195 val loss 9.149735450744629\n",
      "Batch 35 Epoch 12: train loss 0.12838111817836761 val loss 8.90510368347168\n",
      "Batch 36 Epoch 12: train loss 0.025549033656716347 val loss 8.704840660095215\n",
      "Batch 37 Epoch 12: train loss 0.06781863421201706 val loss 8.401909828186035\n",
      "Batch 38 Epoch 12: train loss 0.18449684977531433 val loss 7.490190029144287\n",
      "Batch 39 Epoch 12: train loss 0.11974643170833588 val loss 6.185885429382324\n",
      "Batch 40 Epoch 12: train loss 0.12478993088006973 val loss 4.667522430419922\n",
      "Batch 41 Epoch 12: train loss 0.07048722356557846 val loss 0.04085925966501236\n",
      "Batch 42 Epoch 12: train loss 0.20059075951576233 val loss 0.03321167454123497\n",
      "Batch 43 Epoch 12: train loss 0.041614316403865814 val loss 0.015285355970263481\n",
      "Batch 44 Epoch 12: train loss 0.017692701891064644 val loss 0.01033754087984562\n",
      "Batch 45 Epoch 12: train loss 0.0035214542876929045 val loss 0.014010357670485973\n",
      "Batch 46 Epoch 12: train loss 0.08585883677005768 val loss 0.028539462015032768\n",
      "Batch 47 Epoch 12: train loss 0.053712502121925354 val loss 0.030972732231020927\n",
      "Batch 48 Epoch 12: train loss 0.05246758460998535 val loss 0.11406023800373077\n",
      "Batch 49 Epoch 12: train loss 0.12564611434936523 val loss 0.25271692872047424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 Epoch 13: train loss 0.0635736808180809 val loss 0.3826011121273041\n",
      "Batch 1 Epoch 13: train loss 0.06130364164710045 val loss 0.48357272148132324\n",
      "Batch 2 Epoch 13: train loss 0.07158634066581726 val loss 0.5415409803390503\n",
      "Batch 3 Epoch 13: train loss 0.010511351749300957 val loss 0.5817829966545105\n",
      "Batch 4 Epoch 13: train loss 0.02190493606030941 val loss 0.6029480695724487\n",
      "Batch 5 Epoch 13: train loss 0.06127238646149635 val loss 0.5901483297348022\n",
      "Batch 6 Epoch 13: train loss 0.07050265371799469 val loss 0.5392674803733826\n",
      "Batch 7 Epoch 13: train loss 0.06630294770002365 val loss 0.45447230339050293\n",
      "Batch 8 Epoch 13: train loss 0.02808140031993389 val loss 0.3589210510253906\n",
      "Batch 9 Epoch 13: train loss 0.021404985338449478 val loss 0.26269084215164185\n",
      "Batch 10 Epoch 13: train loss 0.00922742672264576 val loss 0.17903876304626465\n",
      "Batch 11 Epoch 13: train loss 0.001531262998469174 val loss 0.11461770534515381\n",
      "Batch 12 Epoch 13: train loss 0.0025101425126194954 val loss 0.07188096642494202\n",
      "Batch 13 Epoch 13: train loss 0.03994594141840935 val loss 0.05412379652261734\n",
      "Batch 14 Epoch 13: train loss 0.023090757429599762 val loss 0.04937630519270897\n",
      "Batch 15 Epoch 13: train loss 0.029511379078030586 val loss 0.055999498814344406\n",
      "Batch 16 Epoch 13: train loss 0.08343975991010666 val loss 0.08493837714195251\n",
      "Batch 17 Epoch 13: train loss 0.021937908604741096 val loss 0.1306820511817932\n",
      "Batch 18 Epoch 13: train loss 0.012776758521795273 val loss 0.1930934339761734\n",
      "Batch 19 Epoch 13: train loss 0.0016784088220447302 val loss 0.2628333568572998\n",
      "Batch 20 Epoch 13: train loss 0.0008107820176519454 val loss 0.33153843879699707\n",
      "Batch 21 Epoch 13: train loss 0.002632897114381194 val loss 0.39153775572776794\n",
      "Batch 22 Epoch 13: train loss 0.009428472258150578 val loss 0.432999849319458\n",
      "Batch 23 Epoch 13: train loss 0.009520487859845161 val loss 0.45525291562080383\n",
      "Batch 24 Epoch 13: train loss 0.01179580856114626 val loss 0.4571964144706726\n",
      "Batch 25 Epoch 13: train loss 0.011138989590108395 val loss 0.44149070978164673\n",
      "Batch 26 Epoch 13: train loss 0.005865104030817747 val loss 0.41831037402153015\n",
      "Batch 27 Epoch 13: train loss 0.006304734852164984 val loss 0.3878306746482849\n",
      "Batch 28 Epoch 13: train loss 0.001084246439859271 val loss 0.3580572009086609\n",
      "Batch 29 Epoch 13: train loss 0.0006780096446163952 val loss 0.3343529999256134\n",
      "Batch 30 Epoch 13: train loss 0.002200064016506076 val loss 0.31716683506965637\n",
      "Batch 31 Epoch 13: train loss 0.003599646035581827 val loss 0.3083648085594177\n",
      "Batch 32 Epoch 13: train loss 0.01926860399544239 val loss 0.3159349858760834\n",
      "Batch 33 Epoch 13: train loss 0.1827751249074936 val loss 0.3621247410774231\n",
      "Batch 34 Epoch 13: train loss 0.060581695288419724 val loss 0.4145374894142151\n",
      "Batch 35 Epoch 13: train loss 0.02163570374250412 val loss 0.46446293592453003\n",
      "Batch 36 Epoch 13: train loss 0.031844086945056915 val loss 0.5222356915473938\n",
      "Batch 37 Epoch 13: train loss 0.2437991499900818 val loss 0.5962101817131042\n",
      "Batch 38 Epoch 13: train loss 0.2296638786792755 val loss 0.6440747976303101\n",
      "Batch 39 Epoch 13: train loss 0.12445373088121414 val loss 0.6507273316383362\n",
      "Batch 40 Epoch 13: train loss 0.07945340871810913 val loss 0.6236937642097473\n",
      "Batch 41 Epoch 13: train loss 0.042896680533885956 val loss 0.5804407596588135\n",
      "Batch 42 Epoch 13: train loss 0.41812682151794434 val loss 0.49436885118484497\n",
      "Batch 43 Epoch 13: train loss 0.19197343289852142 val loss 0.34607619047164917\n",
      "Batch 44 Epoch 13: train loss 0.2673841118812561 val loss 0.03699743002653122\n",
      "Batch 45 Epoch 13: train loss 0.2308225780725479 val loss 0.033660851418972015\n",
      "Batch 46 Epoch 13: train loss 0.06142985820770264 val loss 0.013515245169401169\n",
      "Batch 47 Epoch 13: train loss 0.09276097267866135 val loss 0.010258304886519909\n",
      "Batch 48 Epoch 13: train loss 0.013423364609479904 val loss 0.014142596162855625\n",
      "Batch 49 Epoch 13: train loss 0.0014436017954722047 val loss 0.023600509390234947\n",
      "Batch 0 Epoch 14: train loss 0.06038139387965202 val loss 0.03890126571059227\n",
      "Batch 1 Epoch 14: train loss 0.04850838705897331 val loss 0.0374310240149498\n",
      "Batch 2 Epoch 14: train loss 0.046149689704179764 val loss 2.7547109127044678\n",
      "Batch 3 Epoch 14: train loss 0.005325160920619965 val loss 3.5936262607574463\n",
      "Batch 4 Epoch 14: train loss 0.027546413242816925 val loss 4.557593822479248\n",
      "Batch 5 Epoch 14: train loss 0.037145230919122696 val loss 5.694602966308594\n",
      "Batch 6 Epoch 14: train loss 0.01347370631992817 val loss 6.990437030792236\n",
      "Batch 7 Epoch 14: train loss 0.0028862212784588337 val loss 8.367369651794434\n",
      "Batch 8 Epoch 14: train loss 0.007355924695730209 val loss 9.62886905670166\n",
      "Batch 9 Epoch 14: train loss 0.007763983216136694 val loss 10.716022491455078\n",
      "Batch 10 Epoch 14: train loss 0.017745204269886017 val loss 11.525857925415039\n",
      "Batch 11 Epoch 14: train loss 0.02337687462568283 val loss 11.96597957611084\n",
      "Batch 12 Epoch 14: train loss 0.022950004786252975 val loss 12.046167373657227\n",
      "Batch 13 Epoch 14: train loss 0.04628258943557739 val loss 11.716788291931152\n",
      "Batch 14 Epoch 14: train loss 0.006647361908107996 val loss 11.268754005432129\n",
      "Batch 15 Epoch 14: train loss 0.003628172678872943 val loss 10.773852348327637\n",
      "Batch 16 Epoch 14: train loss 0.0132913738489151 val loss 10.516607284545898\n",
      "Batch 17 Epoch 14: train loss 0.004145812708884478 val loss 10.374822616577148\n",
      "Batch 18 Epoch 14: train loss 0.0014280591858550906 val loss 10.30461597442627\n",
      "Batch 19 Epoch 14: train loss 0.0021430535707622766 val loss 10.316681861877441\n",
      "Batch 20 Epoch 14: train loss 0.002874573925510049 val loss 10.406935691833496\n",
      "Batch 21 Epoch 14: train loss 0.003940933849662542 val loss 10.591994285583496\n",
      "Batch 22 Epoch 14: train loss 0.004858615808188915 val loss 10.890305519104004\n",
      "Batch 23 Epoch 14: train loss 0.0014042466646060348 val loss 11.223567008972168\n",
      "Batch 24 Epoch 14: train loss 0.0010626771254464984 val loss 11.592616081237793\n",
      "Batch 25 Epoch 14: train loss 0.0006210195715539157 val loss 11.969782829284668\n",
      "Batch 26 Epoch 14: train loss 0.0040603638626635075 val loss 12.228094100952148\n",
      "Batch 27 Epoch 14: train loss 0.0024209122639149427 val loss 12.4830322265625\n",
      "Batch 28 Epoch 14: train loss 0.0019401080207899213 val loss 12.632265090942383\n",
      "Batch 29 Epoch 14: train loss 0.0022180010564625263 val loss 12.689009666442871\n",
      "Batch 30 Epoch 14: train loss 0.0020664797630161047 val loss 12.69786262512207\n",
      "Batch 31 Epoch 14: train loss 0.0013587619177997112 val loss 12.676816940307617\n",
      "Batch 32 Epoch 14: train loss 0.006328072864562273 val loss 12.522330284118652\n",
      "Batch 33 Epoch 14: train loss 0.062024686485528946 val loss 12.066123008728027\n",
      "Batch 34 Epoch 14: train loss 0.02586570754647255 val loss 11.666482925415039\n",
      "Batch 35 Epoch 14: train loss 0.12922361493110657 val loss 11.289079666137695\n",
      "Batch 36 Epoch 14: train loss 0.0210970938205719 val loss 10.97154426574707\n",
      "Batch 37 Epoch 14: train loss 0.06033368781208992 val loss 10.5473051071167\n",
      "Batch 38 Epoch 14: train loss 0.10389886796474457 val loss 9.699326515197754\n",
      "Batch 39 Epoch 14: train loss 0.1258944422006607 val loss 8.233831405639648\n",
      "Batch 40 Epoch 14: train loss 0.16265207529067993 val loss 6.267296314239502\n",
      "Batch 41 Epoch 14: train loss 0.0878288596868515 val loss 4.447097301483154\n",
      "Batch 42 Epoch 14: train loss 0.19757573306560516 val loss 0.04819377884268761\n",
      "Batch 43 Epoch 14: train loss 0.05464940518140793 val loss 0.020184550434350967\n",
      "Batch 44 Epoch 14: train loss 0.02428334206342697 val loss 0.010180170647799969\n",
      "Batch 45 Epoch 14: train loss 0.0029032507445663214 val loss 0.014379426836967468\n",
      "Batch 46 Epoch 14: train loss 0.0873972475528717 val loss 0.03432720527052879\n",
      "Batch 47 Epoch 14: train loss 0.031730055809020996 val loss 0.04011448845267296\n",
      "Batch 48 Epoch 14: train loss 0.0652494728565216 val loss 0.21956533193588257\n",
      "Batch 49 Epoch 14: train loss 0.1393176168203354 val loss 0.4134693443775177\n",
      "Batch 0 Epoch 15: train loss 0.07897491008043289 val loss 0.5806367993354797\n",
      "Batch 1 Epoch 15: train loss 0.07234834879636765 val loss 0.70407634973526\n",
      "Batch 2 Epoch 15: train loss 0.08347266912460327 val loss 0.7713282704353333\n",
      "Batch 3 Epoch 15: train loss 0.01598067209124565 val loss 0.8131157159805298\n",
      "Batch 4 Epoch 15: train loss 0.02732953056693077 val loss 0.8308573365211487\n",
      "Batch 5 Epoch 15: train loss 0.06636717170476913 val loss 0.8098061680793762\n",
      "Batch 6 Epoch 15: train loss 0.07768437266349792 val loss 0.7453737854957581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7 Epoch 15: train loss 0.07392704486846924 val loss 0.6417286396026611\n",
      "Batch 8 Epoch 15: train loss 0.03390955552458763 val loss 0.5236459970474243\n",
      "Batch 9 Epoch 15: train loss 0.024565160274505615 val loss 0.40304118394851685\n",
      "Batch 10 Epoch 15: train loss 0.010963759385049343 val loss 0.2947888672351837\n",
      "Batch 11 Epoch 15: train loss 0.0016574396286159754 val loss 0.20758868753910065\n",
      "Batch 12 Epoch 15: train loss 0.002032824559137225 val loss 0.14531958103179932\n",
      "Batch 13 Epoch 15: train loss 0.038403332233428955 val loss 0.11564420908689499\n",
      "Batch 14 Epoch 15: train loss 0.02442186325788498 val loss 0.10535331815481186\n",
      "Batch 15 Epoch 15: train loss 0.03274207562208176 val loss 0.11248651891946793\n",
      "Batch 16 Epoch 15: train loss 0.06831197440624237 val loss 0.14550459384918213\n",
      "Batch 17 Epoch 15: train loss 0.02463645488023758 val loss 0.19692522287368774\n",
      "Batch 18 Epoch 15: train loss 0.01524013839662075 val loss 0.26572707295417786\n",
      "Batch 19 Epoch 15: train loss 0.0031492155976593494 val loss 0.34255123138427734\n",
      "Batch 20 Epoch 15: train loss 0.0008067722665145993 val loss 0.4200100004673004\n",
      "Batch 21 Epoch 15: train loss 0.0014595078537240624 val loss 0.4898627698421478\n",
      "Batch 22 Epoch 15: train loss 0.006161975208669901 val loss 0.5424422025680542\n",
      "Batch 23 Epoch 15: train loss 0.007500543259084225 val loss 0.57598477602005\n",
      "Batch 24 Epoch 15: train loss 0.010306734591722488 val loss 0.5882721543312073\n",
      "Batch 25 Epoch 15: train loss 0.01097990944981575 val loss 0.5806953310966492\n",
      "Batch 26 Epoch 15: train loss 0.005642295349389315 val loss 0.5640912652015686\n",
      "Batch 27 Epoch 15: train loss 0.006670662667602301 val loss 0.5375993847846985\n",
      "Batch 28 Epoch 15: train loss 0.0013061995850875974 val loss 0.5098753571510315\n",
      "Batch 29 Epoch 15: train loss 0.0004845444636885077 val loss 0.4868800640106201\n",
      "Batch 30 Epoch 15: train loss 0.0019311828073114157 val loss 0.46966585516929626\n",
      "Batch 31 Epoch 15: train loss 0.002759129973128438 val loss 0.46047526597976685\n",
      "Batch 32 Epoch 15: train loss 0.017701707780361176 val loss 0.46924057602882385\n",
      "Batch 33 Epoch 15: train loss 0.169972762465477 val loss 0.5199065208435059\n",
      "Batch 34 Epoch 15: train loss 0.05870622396469116 val loss 0.5769990682601929\n",
      "Batch 35 Epoch 15: train loss 0.022599054500460625 val loss 0.6312157511711121\n",
      "Batch 36 Epoch 15: train loss 0.03252707049250603 val loss 0.6942799091339111\n",
      "Batch 37 Epoch 15: train loss 0.2313719540834427 val loss 0.7742463946342468\n",
      "Batch 38 Epoch 15: train loss 0.2072446048259735 val loss 0.8281073570251465\n",
      "Batch 39 Epoch 15: train loss 0.1340004801750183 val loss 0.8403133153915405\n",
      "Batch 40 Epoch 15: train loss 0.09000783413648605 val loss 0.8169541358947754\n",
      "Batch 41 Epoch 15: train loss 0.0517682284116745 val loss 0.7755741477012634\n",
      "Batch 42 Epoch 15: train loss 0.4285157322883606 val loss 0.6885626912117004\n",
      "Batch 43 Epoch 15: train loss 0.21491187810897827 val loss 0.5268808007240295\n",
      "Batch 44 Epoch 15: train loss 0.2977546155452728 val loss 0.2878740727901459\n",
      "Batch 45 Epoch 15: train loss 0.2790672481060028 val loss 0.043277010321617126\n",
      "Batch 46 Epoch 15: train loss 0.0763462632894516 val loss 0.01640685461461544\n",
      "Batch 47 Epoch 15: train loss 0.06405172497034073 val loss 0.010137147270143032\n",
      "Batch 48 Epoch 15: train loss 0.012255309149622917 val loss 0.01492557767778635\n",
      "Batch 49 Epoch 15: train loss 0.001286732847802341 val loss 0.028530839830636978\n",
      "Batch 0 Epoch 16: train loss 0.06525588035583496 val loss 0.031114932149648666\n",
      "Batch 1 Epoch 16: train loss 0.04933437332510948 val loss 2.076343297958374\n",
      "Batch 2 Epoch 16: train loss 0.043352287262678146 val loss 2.9767515659332275\n",
      "Batch 3 Epoch 16: train loss 0.005667445715516806 val loss 3.9921021461486816\n",
      "Batch 4 Epoch 16: train loss 0.027393659576773643 val loss 5.169043064117432\n",
      "Batch 5 Epoch 16: train loss 0.030355174094438553 val loss 6.542664527893066\n",
      "Batch 6 Epoch 16: train loss 0.006681178230792284 val loss 8.069108009338379\n",
      "Batch 7 Epoch 16: train loss 0.002612627577036619 val loss 9.602823257446289\n",
      "Batch 8 Epoch 16: train loss 0.019130071625113487 val loss 10.885371208190918\n",
      "Batch 9 Epoch 16: train loss 0.018614113330841064 val loss 11.852558135986328\n",
      "Batch 10 Epoch 16: train loss 0.027491921558976173 val loss 12.425134658813477\n",
      "Batch 11 Epoch 16: train loss 0.029261430725455284 val loss 12.564695358276367\n",
      "Batch 12 Epoch 16: train loss 0.020934496074914932 val loss 12.364630699157715\n",
      "Batch 13 Epoch 16: train loss 0.03645520657300949 val loss 11.839649200439453\n",
      "Batch 14 Epoch 16: train loss 0.0013556769117712975 val loss 11.306764602661133\n",
      "Batch 15 Epoch 16: train loss 0.0009604591177776456 val loss 10.831135749816895\n",
      "Batch 16 Epoch 16: train loss 0.04079006239771843 val loss 10.715520858764648\n",
      "Batch 17 Epoch 16: train loss 0.01320570893585682 val loss 10.788816452026367\n",
      "Batch 18 Epoch 16: train loss 0.006636174861341715 val loss 11.000691413879395\n",
      "Batch 19 Epoch 16: train loss 0.006059857551008463 val loss 11.338479042053223\n",
      "Batch 20 Epoch 16: train loss 0.004263675305992365 val loss 11.766380310058594\n",
      "Batch 21 Epoch 16: train loss 0.004031689837574959 val loss 12.28274154663086\n",
      "Batch 22 Epoch 16: train loss 0.0031769948545843363 val loss 12.88036823272705\n",
      "Batch 23 Epoch 16: train loss 0.0005945291486568749 val loss 13.432426452636719\n",
      "Batch 24 Epoch 16: train loss 0.000317371916025877 val loss 13.916036605834961\n",
      "Batch 25 Epoch 16: train loss 0.000967114232480526 val loss 14.295947074890137\n",
      "Batch 26 Epoch 16: train loss 0.009379839524626732 val loss 14.42389965057373\n",
      "Batch 27 Epoch 16: train loss 0.0029515489004552364 val loss 14.489575386047363\n",
      "Batch 28 Epoch 16: train loss 0.003842655336484313 val loss 14.403237342834473\n",
      "Batch 29 Epoch 16: train loss 0.0028855453711003065 val loss 14.223516464233398\n",
      "Batch 30 Epoch 16: train loss 0.0019525941461324692 val loss 14.024517059326172\n",
      "Batch 31 Epoch 16: train loss 0.001413538004271686 val loss 13.845171928405762\n",
      "Batch 32 Epoch 16: train loss 0.0030726944096386433 val loss 13.591986656188965\n",
      "Batch 33 Epoch 16: train loss 0.048116911202669144 val loss 13.084946632385254\n",
      "Batch 34 Epoch 16: train loss 0.045758266001939774 val loss 12.630911827087402\n",
      "Batch 35 Epoch 16: train loss 0.16714605689048767 val loss 12.168718338012695\n",
      "Batch 36 Epoch 16: train loss 0.025532664731144905 val loss 11.800643920898438\n",
      "Batch 37 Epoch 16: train loss 0.053077854216098785 val loss 11.32951831817627\n",
      "Batch 38 Epoch 16: train loss 0.10890286415815353 val loss 10.426187515258789\n",
      "Batch 39 Epoch 16: train loss 0.15751393139362335 val loss 8.789716720581055\n",
      "Batch 40 Epoch 16: train loss 0.2065928727388382 val loss 6.56898832321167\n",
      "Batch 41 Epoch 16: train loss 0.10513163357973099 val loss 4.55433988571167\n",
      "Batch 42 Epoch 16: train loss 0.1981414556503296 val loss 0.04168982431292534\n",
      "Batch 43 Epoch 16: train loss 0.05995757877826691 val loss 0.020894696936011314\n",
      "Batch 44 Epoch 16: train loss 0.024625742807984352 val loss 0.009878861717879772\n",
      "Batch 45 Epoch 16: train loss 0.002855387283489108 val loss 0.015775170177221298\n",
      "Batch 46 Epoch 16: train loss 0.08165126293897629 val loss 0.03848438709974289\n",
      "Batch 47 Epoch 16: train loss 0.021331539377570152 val loss 0.03632166609168053\n",
      "Batch 48 Epoch 16: train loss 0.07294543087482452 val loss 0.19655440747737885\n",
      "Batch 49 Epoch 16: train loss 0.14138880372047424 val loss 0.36449873447418213\n",
      "Batch 0 Epoch 17: train loss 0.07180076092481613 val loss 0.5061910152435303\n",
      "Batch 1 Epoch 17: train loss 0.06404846161603928 val loss 0.6074880361557007\n",
      "Batch 2 Epoch 17: train loss 0.0729621946811676 val loss 0.6577005386352539\n",
      "Batch 3 Epoch 17: train loss 0.01246163435280323 val loss 0.6865235567092896\n",
      "Batch 4 Epoch 17: train loss 0.021351031959056854 val loss 0.6949027180671692\n",
      "Batch 5 Epoch 17: train loss 0.05543402582406998 val loss 0.6689660549163818\n",
      "Batch 6 Epoch 17: train loss 0.06814075261354446 val loss 0.6048176288604736\n",
      "Batch 7 Epoch 17: train loss 0.06197512149810791 val loss 0.5084546208381653\n",
      "Batch 8 Epoch 17: train loss 0.027314109727740288 val loss 0.40294352173805237\n",
      "Batch 9 Epoch 17: train loss 0.0189614649862051 val loss 0.299028605222702\n",
      "Batch 10 Epoch 17: train loss 0.007948361337184906 val loss 0.20956265926361084\n",
      "Batch 11 Epoch 17: train loss 0.0009480885928496718 val loss 0.14117960631847382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12 Epoch 17: train loss 0.003366591176018119 val loss 0.09552539139986038\n",
      "Batch 13 Epoch 17: train loss 0.04186215251684189 val loss 0.07682482898235321\n",
      "Batch 14 Epoch 17: train loss 0.025542596355080605 val loss 0.07333232462406158\n",
      "Batch 15 Epoch 17: train loss 0.03172245994210243 val loss 0.08384022116661072\n",
      "Batch 16 Epoch 17: train loss 0.08108177036046982 val loss 0.1200290396809578\n",
      "Batch 17 Epoch 17: train loss 0.02403726987540722 val loss 0.17509128153324127\n",
      "Batch 18 Epoch 17: train loss 0.011477748863399029 val loss 0.24659772217273712\n",
      "Batch 19 Epoch 17: train loss 0.001157321035861969 val loss 0.32339292764663696\n",
      "Batch 20 Epoch 17: train loss 0.0012068282812833786 val loss 0.39629173278808594\n",
      "Batch 21 Epoch 17: train loss 0.003903021337464452 val loss 0.4572015106678009\n",
      "Batch 22 Epoch 17: train loss 0.010756760835647583 val loss 0.49719953536987305\n",
      "Batch 23 Epoch 17: train loss 0.011716153472661972 val loss 0.5155260562896729\n",
      "Batch 24 Epoch 17: train loss 0.014272398315370083 val loss 0.5114302039146423\n",
      "Batch 25 Epoch 17: train loss 0.01351404283195734 val loss 0.4881385862827301\n",
      "Batch 26 Epoch 17: train loss 0.0067175403237342834 val loss 0.45704492926597595\n",
      "Batch 27 Epoch 17: train loss 0.00663695577532053 val loss 0.41897791624069214\n",
      "Batch 28 Epoch 17: train loss 0.0008537046960555017 val loss 0.38301631808280945\n",
      "Batch 29 Epoch 17: train loss 0.0007316112169064581 val loss 0.354245662689209\n",
      "Batch 30 Epoch 17: train loss 0.0026364338118582964 val loss 0.3337614834308624\n",
      "Batch 31 Epoch 17: train loss 0.004430426750332117 val loss 0.3231712281703949\n",
      "Batch 32 Epoch 17: train loss 0.021425871178507805 val loss 0.33073240518569946\n",
      "Batch 33 Epoch 17: train loss 0.1819877028465271 val loss 0.379292756319046\n",
      "Batch 34 Epoch 17: train loss 0.06438969820737839 val loss 0.43744999170303345\n",
      "Batch 35 Epoch 17: train loss 0.023344015702605247 val loss 0.4946013391017914\n",
      "Batch 36 Epoch 17: train loss 0.03567212447524071 val loss 0.5624433159828186\n",
      "Batch 37 Epoch 17: train loss 0.23402319848537445 val loss 0.652377724647522\n",
      "Batch 38 Epoch 17: train loss 0.2325829714536667 val loss 0.7256309390068054\n",
      "Batch 39 Epoch 17: train loss 0.13899612426757812 val loss 0.763880729675293\n",
      "Batch 40 Epoch 17: train loss 0.08758676052093506 val loss 0.7698675990104675\n",
      "Batch 41 Epoch 17: train loss 0.05243844911456108 val loss 0.7568274140357971\n",
      "Batch 42 Epoch 17: train loss 0.4239518940448761 val loss 0.7003161907196045\n",
      "Batch 43 Epoch 17: train loss 0.2156979739665985 val loss 0.5638323426246643\n",
      "Batch 44 Epoch 17: train loss 0.3014378547668457 val loss 0.33356818556785583\n",
      "Batch 45 Epoch 17: train loss 0.29243242740631104 val loss 0.0465228445827961\n",
      "Batch 46 Epoch 17: train loss 0.07724449038505554 val loss 0.01740032434463501\n",
      "Batch 47 Epoch 17: train loss 0.04934350773692131 val loss 0.010050726123154163\n",
      "Batch 48 Epoch 17: train loss 0.01182707492262125 val loss 0.015741078183054924\n",
      "Batch 49 Epoch 17: train loss 0.001657534739933908 val loss 0.03206414729356766\n",
      "Batch 0 Epoch 18: train loss 0.07949912548065186 val loss 0.041437357664108276\n",
      "Batch 1 Epoch 18: train loss 0.05798536166548729 val loss 2.4258248805999756\n",
      "Batch 2 Epoch 18: train loss 0.050735119730234146 val loss 3.581704616546631\n",
      "Batch 3 Epoch 18: train loss 0.006591434124857187 val loss 4.9222307205200195\n",
      "Batch 4 Epoch 18: train loss 0.02881573885679245 val loss 6.502867698669434\n",
      "Batch 5 Epoch 18: train loss 0.027957506477832794 val loss 8.358426094055176\n",
      "Batch 6 Epoch 18: train loss 0.004328567069023848 val loss 10.406935691833496\n",
      "Batch 7 Epoch 18: train loss 0.004855868872255087 val loss 12.418783187866211\n",
      "Batch 8 Epoch 18: train loss 0.02839585766196251 val loss 14.023656845092773\n",
      "Batch 9 Epoch 18: train loss 0.02764040417969227 val loss 15.132854461669922\n",
      "Batch 10 Epoch 18: train loss 0.03422679007053375 val loss 15.670635223388672\n",
      "Batch 11 Epoch 18: train loss 0.03493084758520126 val loss 15.618729591369629\n",
      "Batch 12 Epoch 18: train loss 0.02029350958764553 val loss 15.169635772705078\n",
      "Batch 13 Epoch 18: train loss 0.0314127579331398 val loss 14.393057823181152\n",
      "Batch 14 Epoch 18: train loss 0.0003623422235250473 val loss 13.681381225585938\n",
      "Batch 15 Epoch 18: train loss 0.0016953893937170506 val loss 13.110323905944824\n",
      "Batch 16 Epoch 18: train loss 0.05335419625043869 val loss 13.027251243591309\n",
      "Batch 17 Epoch 18: train loss 0.02136431820690632 val loss 13.228154182434082\n",
      "Batch 18 Epoch 18: train loss 0.011531145311892033 val loss 13.646394729614258\n",
      "Batch 19 Epoch 18: train loss 0.008845102973282337 val loss 14.246936798095703\n",
      "Batch 20 Epoch 18: train loss 0.005830551031976938 val loss 14.97815990447998\n",
      "Batch 21 Epoch 18: train loss 0.004362086765468121 val loss 15.813156127929688\n",
      "Batch 22 Epoch 18: train loss 0.0024419697001576424 val loss 16.722423553466797\n",
      "Batch 23 Epoch 18: train loss 0.0009560483740642667 val loss 17.518020629882812\n",
      "Batch 24 Epoch 18: train loss 0.0009429162601009011 val loss 18.171680450439453\n",
      "Batch 25 Epoch 18: train loss 0.00300705898553133 val loss 18.604097366333008\n",
      "Batch 26 Epoch 18: train loss 0.01273386087268591 val loss 18.66222381591797\n",
      "Batch 27 Epoch 18: train loss 0.0036079813726246357 val loss 18.592378616333008\n",
      "Batch 28 Epoch 18: train loss 0.005847382824867964 val loss 18.30153465270996\n",
      "Batch 29 Epoch 18: train loss 0.0035149322357028723 val loss 17.894399642944336\n",
      "Batch 30 Epoch 18: train loss 0.0019715672824531794 val loss 17.479816436767578\n",
      "Batch 31 Epoch 18: train loss 0.0013763238675892353 val loss 17.116567611694336\n",
      "Batch 32 Epoch 18: train loss 0.0026426881086081266 val loss 16.695228576660156\n",
      "Batch 33 Epoch 18: train loss 0.04578790441155434 val loss 15.999515533447266\n",
      "Batch 34 Epoch 18: train loss 0.046487707644701004 val loss 15.359593391418457\n",
      "Batch 35 Epoch 18: train loss 0.16567781567573547 val loss 14.685083389282227\n",
      "Batch 36 Epoch 18: train loss 0.0247743371874094 val loss 14.138280868530273\n",
      "Batch 37 Epoch 18: train loss 0.04948153346776962 val loss 13.491073608398438\n",
      "Batch 38 Epoch 18: train loss 0.09654916822910309 val loss 12.343969345092773\n",
      "Batch 39 Epoch 18: train loss 0.15614886581897736 val loss 10.286870956420898\n",
      "Batch 40 Epoch 18: train loss 0.20239543914794922 val loss 7.56506872177124\n",
      "Batch 41 Epoch 18: train loss 0.09728353470563889 val loss 5.171013355255127\n",
      "Batch 42 Epoch 18: train loss 0.20005321502685547 val loss 0.04918508976697922\n",
      "Batch 43 Epoch 18: train loss 0.05778088420629501 val loss 0.01780712977051735\n",
      "Batch 44 Epoch 18: train loss 0.027801575139164925 val loss 0.0099561782553792\n",
      "Batch 45 Epoch 18: train loss 0.0027998024597764015 val loss 0.018914539366960526\n",
      "Batch 46 Epoch 18: train loss 0.08098743110895157 val loss 0.04538194462656975\n",
      "Batch 47 Epoch 18: train loss 0.015455333516001701 val loss 0.03470495715737343\n",
      "Batch 48 Epoch 18: train loss 0.08367332071065903 val loss 0.18646825850009918\n",
      "Batch 49 Epoch 18: train loss 0.14946086704730988 val loss 0.3342697024345398\n",
      "Batch 0 Epoch 19: train loss 0.0684945359826088 val loss 0.45290860533714294\n",
      "Batch 1 Epoch 19: train loss 0.05929948762059212 val loss 0.5316363573074341\n",
      "Batch 2 Epoch 19: train loss 0.06696083396673203 val loss 0.5619304776191711\n",
      "Batch 3 Epoch 19: train loss 0.010311421006917953 val loss 0.5738415122032166\n",
      "Batch 4 Epoch 19: train loss 0.01797552779316902 val loss 0.5685498118400574\n",
      "Batch 5 Epoch 19: train loss 0.04893411695957184 val loss 0.533172070980072\n",
      "Batch 6 Epoch 19: train loss 0.06037319824099541 val loss 0.46515655517578125\n",
      "Batch 7 Epoch 19: train loss 0.054961223155260086 val loss 0.3711920976638794\n",
      "Batch 8 Epoch 19: train loss 0.022450288757681847 val loss 0.2740591764450073\n",
      "Batch 9 Epoch 19: train loss 0.014128252863883972 val loss 0.18446235358715057\n",
      "Batch 10 Epoch 19: train loss 0.005789384711533785 val loss 0.1132117435336113\n",
      "Batch 11 Epoch 19: train loss 0.0007381418836303055 val loss 0.06431422382593155\n",
      "Batch 12 Epoch 19: train loss 0.004687158856540918 val loss 0.03614617511630058\n",
      "Batch 13 Epoch 19: train loss 0.04615852236747742 val loss 0.027148036286234856\n",
      "Batch 14 Epoch 19: train loss 0.0286707803606987 val loss 0.027399230748414993\n",
      "Batch 15 Epoch 19: train loss 0.03313649445772171 val loss 0.03660031408071518\n",
      "Batch 16 Epoch 19: train loss 0.08271054178476334 val loss 0.06639866530895233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 17 Epoch 19: train loss 0.02145186811685562 val loss 0.1151658222079277\n",
      "Batch 18 Epoch 19: train loss 0.007516919169574976 val loss 0.18003463745117188\n",
      "Batch 19 Epoch 19: train loss 0.000590217940043658 val loss 0.24975961446762085\n",
      "Batch 20 Epoch 19: train loss 0.002739488612860441 val loss 0.313818097114563\n",
      "Batch 21 Epoch 19: train loss 0.007436037063598633 val loss 0.3634464144706726\n",
      "Batch 22 Epoch 19: train loss 0.016369985416531563 val loss 0.39009028673171997\n",
      "Batch 23 Epoch 19: train loss 0.01633279211819172 val loss 0.39397796988487244\n",
      "Batch 24 Epoch 19: train loss 0.017335664480924606 val loss 0.3765258193016052\n",
      "Batch 25 Epoch 19: train loss 0.015853023156523705 val loss 0.3420376777648926\n",
      "Batch 26 Epoch 19: train loss 0.006985792890191078 val loss 0.3026685416698456\n",
      "Batch 27 Epoch 19: train loss 0.0061648753471672535 val loss 0.2602870464324951\n",
      "Batch 28 Epoch 19: train loss 0.0005752781871706247 val loss 0.22327028214931488\n",
      "Batch 29 Epoch 19: train loss 0.001271411427296698 val loss 0.19552966952323914\n",
      "Batch 30 Epoch 19: train loss 0.003999515902251005 val loss 0.1772317886352539\n",
      "Batch 31 Epoch 19: train loss 0.006848631892353296 val loss 0.16902753710746765\n",
      "Batch 32 Epoch 19: train loss 0.027041707187891006 val loss 0.17699284851551056\n",
      "Batch 33 Epoch 19: train loss 0.19323793053627014 val loss 0.21959136426448822\n",
      "Batch 34 Epoch 19: train loss 0.06855661422014236 val loss 0.2719511389732361\n",
      "Batch 35 Epoch 19: train loss 0.024265604093670845 val loss 0.32498329877853394\n",
      "Batch 36 Epoch 19: train loss 0.037544362246990204 val loss 0.3889594078063965\n",
      "Batch 37 Epoch 19: train loss 0.23052242398262024 val loss 0.4734237790107727\n",
      "Batch 38 Epoch 19: train loss 0.22948592901229858 val loss 0.5453171133995056\n",
      "Batch 39 Epoch 19: train loss 0.13340558111667633 val loss 0.585444986820221\n",
      "Batch 40 Epoch 19: train loss 0.08128587901592255 val loss 0.595930814743042\n",
      "Batch 41 Epoch 19: train loss 0.048454780131578445 val loss 0.5891310572624207\n",
      "Batch 42 Epoch 19: train loss 0.40940913558006287 val loss 0.5431711673736572\n",
      "Batch 43 Epoch 19: train loss 0.20760329067707062 val loss 0.42659643292427063\n",
      "Batch 44 Epoch 19: train loss 0.2787652909755707 val loss 0.2336317002773285\n",
      "Batch 45 Epoch 19: train loss 0.2655622363090515 val loss 0.04548736289143562\n",
      "Batch 46 Epoch 19: train loss 0.06705944240093231 val loss 0.018593275919556618\n",
      "Batch 47 Epoch 19: train loss 0.042612459510564804 val loss 0.010244537144899368\n",
      "Batch 48 Epoch 19: train loss 0.011339045129716396 val loss 0.012721369042992592\n",
      "Batch 49 Epoch 19: train loss 0.001230319612659514 val loss 0.024444246664643288\n",
      "Batch 0 Epoch 20: train loss 0.09060714393854141 val loss 0.045678623020648956\n",
      "Batch 1 Epoch 20: train loss 0.0672227069735527 val loss 2.4957401752471924\n",
      "Batch 2 Epoch 20: train loss 0.05845324322581291 val loss 3.64060115814209\n",
      "Batch 3 Epoch 20: train loss 0.008963892236351967 val loss 4.974674701690674\n",
      "Batch 4 Epoch 20: train loss 0.030373167246580124 val loss 6.539769172668457\n",
      "Batch 5 Epoch 20: train loss 0.031193990260362625 val loss 8.375887870788574\n",
      "Batch 6 Epoch 20: train loss 0.008413195610046387 val loss 10.437128067016602\n",
      "Batch 7 Epoch 20: train loss 0.0019068842520937324 val loss 12.543645858764648\n",
      "Batch 8 Epoch 20: train loss 0.017993690446019173 val loss 14.36143970489502\n",
      "Batch 9 Epoch 20: train loss 0.021136363968253136 val loss 15.764463424682617\n",
      "Batch 10 Epoch 20: train loss 0.0319490060210228 val loss 16.621868133544922\n",
      "Batch 11 Epoch 20: train loss 0.03525064140558243 val loss 16.868587493896484\n",
      "Batch 12 Epoch 20: train loss 0.02563505619764328 val loss 16.62997055053711\n",
      "Batch 13 Epoch 20: train loss 0.04114210978150368 val loss 15.93499755859375\n",
      "Batch 14 Epoch 20: train loss 0.0034591949079185724 val loss 15.180060386657715\n",
      "Batch 15 Epoch 20: train loss 0.0011066652368754148 val loss 14.472661972045898\n",
      "Batch 16 Epoch 20: train loss 0.024387935176491737 val loss 14.130219459533691\n",
      "Batch 17 Epoch 20: train loss 0.00879716221243143 val loss 13.994426727294922\n",
      "Batch 18 Epoch 20: train loss 0.007412849925458431 val loss 14.049460411071777\n",
      "Batch 19 Epoch 20: train loss 0.008225692436099052 val loss 14.295564651489258\n",
      "Batch 20 Epoch 20: train loss 0.007931548170745373 val loss 14.713326454162598\n",
      "Batch 21 Epoch 20: train loss 0.007723044138401747 val loss 15.296372413635254\n",
      "Batch 22 Epoch 20: train loss 0.0075436425395309925 val loss 16.0478458404541\n",
      "Batch 23 Epoch 20: train loss 0.0014421343803405762 val loss 16.825716018676758\n",
      "Batch 24 Epoch 20: train loss 0.0004684806917794049 val loss 17.593101501464844\n",
      "Batch 25 Epoch 20: train loss 0.000417462142650038 val loss 18.28940773010254\n",
      "Batch 26 Epoch 20: train loss 0.007574047893285751 val loss 18.70998191833496\n",
      "Batch 27 Epoch 20: train loss 0.0026292256079614162 val loss 19.020870208740234\n",
      "Batch 28 Epoch 20: train loss 0.005209389608353376 val loss 19.09260368347168\n",
      "Batch 29 Epoch 20: train loss 0.004883090499788523 val loss 18.975261688232422\n",
      "Batch 30 Epoch 20: train loss 0.0037760857958346605 val loss 18.74386215209961\n",
      "Batch 31 Epoch 20: train loss 0.0020593306981027126 val loss 18.4613037109375\n",
      "Batch 32 Epoch 20: train loss 0.00686154467985034 val loss 18.019105911254883\n",
      "Batch 33 Epoch 20: train loss 0.057771071791648865 val loss 17.239112854003906\n",
      "Batch 34 Epoch 20: train loss 0.03237605094909668 val loss 16.53049087524414\n",
      "Batch 35 Epoch 20: train loss 0.13776536285877228 val loss 15.80978012084961\n",
      "Batch 36 Epoch 20: train loss 0.018851114436984062 val loss 15.192919731140137\n",
      "Batch 37 Epoch 20: train loss 0.0525498241186142 val loss 14.471536636352539\n",
      "Batch 38 Epoch 20: train loss 0.08532599359750748 val loss 13.23843002319336\n",
      "Batch 39 Epoch 20: train loss 0.1387113481760025 val loss 11.072236061096191\n",
      "Batch 40 Epoch 20: train loss 0.1949087232351303 val loss 8.160177230834961\n",
      "Batch 41 Epoch 20: train loss 0.09062686562538147 val loss 5.575906276702881\n",
      "Batch 42 Epoch 20: train loss 0.20768161118030548 val loss 0.04898195341229439\n",
      "Batch 43 Epoch 20: train loss 0.059712003916502 val loss 0.01700098067522049\n",
      "Batch 44 Epoch 20: train loss 0.028068525716662407 val loss 0.01023229118436575\n",
      "Batch 45 Epoch 20: train loss 0.0027294878382235765 val loss 0.022061161696910858\n",
      "Batch 46 Epoch 20: train loss 0.07452921569347382 val loss 0.04920141026377678\n",
      "Batch 47 Epoch 20: train loss 0.007975096814334393 val loss 0.09383964538574219\n",
      "Batch 48 Epoch 20: train loss 0.10237932205200195 val loss 0.3048037886619568\n",
      "Batch 49 Epoch 20: train loss 0.16232524812221527 val loss 0.4372611343860626\n",
      "Batch 0 Epoch 21: train loss 0.06509610265493393 val loss 0.5208402872085571\n",
      "Batch 1 Epoch 21: train loss 0.054609280079603195 val loss 0.5574601292610168\n",
      "Batch 2 Epoch 21: train loss 0.05872604623436928 val loss 0.5455848574638367\n",
      "Batch 3 Epoch 21: train loss 0.0076869758777320385 val loss 0.5210790038108826\n",
      "Batch 4 Epoch 21: train loss 0.013467988930642605 val loss 0.48622414469718933\n",
      "Batch 5 Epoch 21: train loss 0.03755943104624748 val loss 0.42907753586769104\n",
      "Batch 6 Epoch 21: train loss 0.04838068038225174 val loss 0.34723129868507385\n",
      "Batch 7 Epoch 21: train loss 0.04097001999616623 val loss 0.25016549229621887\n",
      "Batch 8 Epoch 21: train loss 0.013348381966352463 val loss 0.16121982038021088\n",
      "Batch 9 Epoch 21: train loss 0.007735288701951504 val loss 0.08922062814235687\n",
      "Batch 10 Epoch 21: train loss 0.0031241695396602154 val loss 0.04185361787676811\n",
      "Batch 11 Epoch 21: train loss 0.002112454967573285 val loss 0.01788959465920925\n",
      "Batch 12 Epoch 21: train loss 0.008753091096878052 val loss 0.009851532056927681\n",
      "Batch 13 Epoch 21: train loss 0.0537000447511673 val loss 0.009084386751055717\n",
      "Batch 14 Epoch 21: train loss 0.03137197345495224 val loss 0.010354198515415192\n",
      "Batch 15 Epoch 21: train loss 0.02994781918823719 val loss 0.016796359792351723\n",
      "Batch 16 Epoch 21: train loss 0.07420565187931061 val loss 0.04075547680258751\n",
      "Batch 17 Epoch 21: train loss 0.016745584085583687 val loss 0.08539558947086334\n",
      "Batch 18 Epoch 21: train loss 0.002936030039563775 val loss 0.1454528272151947\n",
      "Batch 19 Epoch 21: train loss 0.0016317948466166854 val loss 0.20693238079547882\n",
      "Batch 20 Epoch 21: train loss 0.007858110591769218 val loss 0.2574567496776581\n",
      "Batch 21 Epoch 21: train loss 0.014413123950362206 val loss 0.28878462314605713\n",
      "Batch 22 Epoch 21: train loss 0.025049729272723198 val loss 0.29432013630867004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 23 Epoch 21: train loss 0.02326137199997902 val loss 0.27671048045158386\n",
      "Batch 24 Epoch 21: train loss 0.02256847359240055 val loss 0.2403712421655655\n",
      "Batch 25 Epoch 21: train loss 0.017685143277049065 val loss 0.19306640326976776\n",
      "Batch 26 Epoch 21: train loss 0.0067889331839978695 val loss 0.14753657579421997\n",
      "Batch 27 Epoch 21: train loss 0.005368488375097513 val loss 0.1059863269329071\n",
      "Batch 28 Epoch 21: train loss 0.00036985031329095364 val loss 0.07454074174165726\n",
      "Batch 29 Epoch 21: train loss 0.0026788278482854366 val loss 0.05419914796948433\n",
      "Batch 30 Epoch 21: train loss 0.006748446263372898 val loss 0.042713429778814316\n",
      "Batch 31 Epoch 21: train loss 0.010380681604146957 val loss 0.038315385580062866\n",
      "Batch 32 Epoch 21: train loss 0.032571692019701004 val loss 0.042840711772441864\n",
      "Batch 33 Epoch 21: train loss 0.1990305632352829 val loss 0.06679938733577728\n",
      "Batch 34 Epoch 21: train loss 0.06643632054328918 val loss 0.0983349084854126\n",
      "Batch 35 Epoch 21: train loss 0.02278737910091877 val loss 0.13212700188159943\n",
      "Batch 36 Epoch 21: train loss 0.035655293613672256 val loss 0.17450545728206635\n",
      "Batch 37 Epoch 21: train loss 0.2120053768157959 val loss 0.22924216091632843\n",
      "Batch 38 Epoch 21: train loss 0.2114267349243164 val loss 0.2646183967590332\n",
      "Batch 39 Epoch 21: train loss 0.10741814970970154 val loss 0.2714463174343109\n",
      "Batch 40 Epoch 21: train loss 0.05279795080423355 val loss 0.25793540477752686\n",
      "Batch 41 Epoch 21: train loss 0.02804907038807869 val loss 0.23484604060649872\n",
      "Batch 42 Epoch 21: train loss 0.347202867269516 val loss 0.18646469712257385\n",
      "Batch 43 Epoch 21: train loss 0.1314338594675064 val loss 0.10727428644895554\n",
      "Batch 44 Epoch 21: train loss 0.16069434583187103 val loss 0.04086589813232422\n",
      "Batch 45 Epoch 21: train loss 0.1348322629928589 val loss 0.018453305587172508\n",
      "Batch 46 Epoch 21: train loss 0.046810951083898544 val loss 0.010119779966771603\n",
      "Batch 47 Epoch 21: train loss 0.09230583161115646 val loss 0.01030447706580162\n",
      "Batch 48 Epoch 21: train loss 0.011683900840580463 val loss 0.013003705069422722\n",
      "Batch 49 Epoch 21: train loss 0.001382238115184009 val loss 0.017300713807344437\n",
      "Batch 0 Epoch 22: train loss 0.12135238200426102 val loss 0.023977084085345268\n",
      "Batch 1 Epoch 22: train loss 0.09895063191652298 val loss 0.03323809430003166\n",
      "Batch 2 Epoch 22: train loss 0.09596814960241318 val loss 0.04571586474776268\n",
      "Batch 3 Epoch 22: train loss 0.022644080221652985 val loss 2.2079339027404785\n",
      "Batch 4 Epoch 22: train loss 0.051045455038547516 val loss 2.8558664321899414\n",
      "Batch 5 Epoch 22: train loss 0.055608395487070084 val loss 3.65683650970459\n",
      "Batch 6 Epoch 22: train loss 0.030404597520828247 val loss 4.632214546203613\n",
      "Batch 7 Epoch 22: train loss 0.010008876211941242 val loss 5.746158599853516\n",
      "Batch 8 Epoch 22: train loss 0.0022768587805330753 val loss 6.849999904632568\n",
      "Batch 9 Epoch 22: train loss 0.004771332256495953 val loss 7.853229522705078\n",
      "Batch 10 Epoch 22: train loss 0.017778949812054634 val loss 8.622377395629883\n",
      "Batch 11 Epoch 22: train loss 0.031612496823072433 val loss 9.011825561523438\n",
      "Batch 12 Epoch 22: train loss 0.034021805971860886 val loss 9.012199401855469\n",
      "Batch 13 Epoch 22: train loss 0.06386025995016098 val loss 8.580087661743164\n",
      "Batch 14 Epoch 22: train loss 0.01994476281106472 val loss 7.972103595733643\n",
      "Batch 15 Epoch 22: train loss 0.012051139026880264 val loss 7.288492679595947\n",
      "Batch 16 Epoch 22: train loss 0.0060188788920640945 val loss 6.793715953826904\n",
      "Batch 17 Epoch 22: train loss 0.0007726313779130578 val loss 6.3651323318481445\n",
      "Batch 18 Epoch 22: train loss 0.0007728564669378102 val loss 6.016597270965576\n",
      "Batch 19 Epoch 22: train loss 0.0015700076473876834 val loss 5.756523132324219\n",
      "Batch 20 Epoch 22: train loss 0.0033597785513848066 val loss 5.589609146118164\n",
      "Batch 21 Epoch 22: train loss 0.0062583801336586475 val loss 5.529628276824951\n",
      "Batch 22 Epoch 22: train loss 0.009663290344178677 val loss 5.590951919555664\n",
      "Batch 23 Epoch 22: train loss 0.0048968736082315445 val loss 5.729938983917236\n",
      "Batch 24 Epoch 22: train loss 0.004957279190421104 val loss 5.945347309112549\n",
      "Batch 25 Epoch 22: train loss 0.003295955015346408 val loss 6.218122959136963\n",
      "Batch 26 Epoch 22: train loss 0.00246825790964067 val loss 6.461861610412598\n",
      "Batch 27 Epoch 22: train loss 0.0031422125175595284 val loss 6.732862949371338\n",
      "Batch 28 Epoch 22: train loss 0.0012050314107909799 val loss 6.941310882568359\n",
      "Batch 29 Epoch 22: train loss 0.0022303187288343906 val loss 7.079685688018799\n",
      "Batch 30 Epoch 22: train loss 0.003299785777926445 val loss 7.1500020027160645\n",
      "Batch 31 Epoch 22: train loss 0.0027037051040679216 val loss 7.159793376922607\n",
      "Batch 32 Epoch 22: train loss 0.011887071654200554 val loss 7.034374713897705\n",
      "Batch 33 Epoch 22: train loss 0.07761085033416748 val loss 6.671253204345703\n",
      "Batch 34 Epoch 22: train loss 0.00412744889035821 val loss 6.3646111488342285\n",
      "Batch 35 Epoch 22: train loss 0.06607484817504883 val loss 6.118429183959961\n",
      "Batch 36 Epoch 22: train loss 0.014115163125097752 val loss 5.876433372497559\n",
      "Batch 37 Epoch 22: train loss 0.06585057824850082 val loss 5.570651531219482\n",
      "Batch 38 Epoch 22: train loss 0.056141965091228485 val loss 5.1311492919921875\n",
      "Batch 39 Epoch 22: train loss 0.04609741270542145 val loss 4.531213283538818\n",
      "Batch 40 Epoch 22: train loss 0.07783208042383194 val loss 3.728996515274048\n",
      "Batch 41 Epoch 22: train loss 0.03722001612186432 val loss 0.04715584218502045\n",
      "Batch 42 Epoch 22: train loss 0.22123610973358154 val loss 0.03157463297247887\n",
      "Batch 43 Epoch 22: train loss 0.0523659773170948 val loss 0.01800239458680153\n",
      "Batch 44 Epoch 22: train loss 0.033778902143239975 val loss 0.010647698305547237\n",
      "Batch 45 Epoch 22: train loss 0.004974442068487406 val loss 0.011173450388014317\n",
      "Batch 46 Epoch 22: train loss 0.11187165975570679 val loss 0.02496298775076866\n",
      "Batch 47 Epoch 22: train loss 0.02760564722120762 val loss 0.04715913161635399\n",
      "Batch 48 Epoch 22: train loss 0.054165467619895935 val loss 0.2009170800447464\n",
      "Batch 49 Epoch 22: train loss 0.09752893447875977 val loss 0.401114821434021\n",
      "Batch 0 Epoch 23: train loss 0.10878360271453857 val loss 0.5758187174797058\n",
      "Batch 1 Epoch 23: train loss 0.09276914596557617 val loss 0.7058501243591309\n",
      "Batch 2 Epoch 23: train loss 0.10567880421876907 val loss 0.7772372961044312\n",
      "Batch 3 Epoch 23: train loss 0.029645299538969994 val loss 0.816503643989563\n",
      "Batch 4 Epoch 23: train loss 0.0429571159183979 val loss 0.8281399607658386\n",
      "Batch 5 Epoch 23: train loss 0.07611210644245148 val loss 0.8015833497047424\n",
      "Batch 6 Epoch 23: train loss 0.08742013573646545 val loss 0.7307172417640686\n",
      "Batch 7 Epoch 23: train loss 0.07820951193571091 val loss 0.6208340525627136\n",
      "Batch 8 Epoch 23: train loss 0.03687293455004692 val loss 0.4961635172367096\n",
      "Batch 9 Epoch 23: train loss 0.022538794204592705 val loss 0.3714381754398346\n",
      "Batch 10 Epoch 23: train loss 0.008515780791640282 val loss 0.2625231146812439\n",
      "Batch 11 Epoch 23: train loss 0.0007301075384020805 val loss 0.17949248850345612\n",
      "Batch 12 Epoch 23: train loss 0.005689680576324463 val loss 0.12465821951627731\n",
      "Batch 13 Epoch 23: train loss 0.050294987857341766 val loss 0.10314096510410309\n",
      "Batch 14 Epoch 23: train loss 0.037319738417863846 val loss 0.10256236046552658\n",
      "Batch 15 Epoch 23: train loss 0.042656753212213516 val loss 0.12174014002084732\n",
      "Batch 16 Epoch 23: train loss 0.041242893785238266 val loss 0.16160176694393158\n",
      "Batch 17 Epoch 23: train loss 0.025580959394574165 val loss 0.22099794447422028\n",
      "Batch 18 Epoch 23: train loss 0.01214908342808485 val loss 0.29605209827423096\n",
      "Batch 19 Epoch 23: train loss 0.002311954740434885 val loss 0.3768271207809448\n",
      "Batch 20 Epoch 23: train loss 0.0007678070687688887 val loss 0.45398199558258057\n",
      "Batch 21 Epoch 23: train loss 0.003100293455645442 val loss 0.5185164213180542\n",
      "Batch 22 Epoch 23: train loss 0.01010663527995348 val loss 0.5614824295043945\n",
      "Batch 23 Epoch 23: train loss 0.01081384439021349 val loss 0.5826540589332581\n",
      "Batch 24 Epoch 23: train loss 0.014973947778344154 val loss 0.5804826021194458\n",
      "Batch 25 Epoch 23: train loss 0.014201914891600609 val loss 0.5582855343818665\n",
      "Batch 26 Epoch 23: train loss 0.0069629130885005 val loss 0.5275625586509705\n",
      "Batch 27 Epoch 23: train loss 0.008035012520849705 val loss 0.48816731572151184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 28 Epoch 23: train loss 0.001019331975840032 val loss 0.45019984245300293\n",
      "Batch 29 Epoch 23: train loss 0.0004986075218766928 val loss 0.4186369776725769\n",
      "Batch 30 Epoch 23: train loss 0.002563644200563431 val loss 0.3951462507247925\n",
      "Batch 31 Epoch 23: train loss 0.0035122190602123737 val loss 0.3810327351093292\n",
      "Batch 32 Epoch 23: train loss 0.020417923107743263 val loss 0.38507145643234253\n",
      "Batch 33 Epoch 23: train loss 0.15198472142219543 val loss 0.4237539768218994\n",
      "Batch 34 Epoch 23: train loss 0.04682598635554314 val loss 0.46448805928230286\n",
      "Batch 35 Epoch 23: train loss 0.022134417667984962 val loss 0.5020246505737305\n",
      "Batch 36 Epoch 23: train loss 0.033069685101509094 val loss 0.546609103679657\n",
      "Batch 37 Epoch 23: train loss 0.1774788796901703 val loss 0.5991565585136414\n",
      "Batch 38 Epoch 23: train loss 0.12916716933250427 val loss 0.6235378980636597\n",
      "Batch 39 Epoch 23: train loss 0.10426221787929535 val loss 0.610136866569519\n",
      "Batch 40 Epoch 23: train loss 0.0605422668159008 val loss 0.5705128312110901\n",
      "Batch 41 Epoch 23: train loss 0.03575610741972923 val loss 0.5192270874977112\n",
      "Batch 42 Epoch 23: train loss 0.3594166338443756 val loss 0.4365052878856659\n",
      "Batch 43 Epoch 23: train loss 0.14668963849544525 val loss 0.30844834446907043\n",
      "Batch 44 Epoch 23: train loss 0.18823130428791046 val loss 0.04000832885503769\n",
      "Batch 45 Epoch 23: train loss 0.17262564599514008 val loss 0.025382131338119507\n",
      "Batch 46 Epoch 23: train loss 0.046579375863075256 val loss 0.011517114005982876\n",
      "Batch 47 Epoch 23: train loss 0.06724842637777328 val loss 0.010008787736296654\n",
      "Batch 48 Epoch 23: train loss 0.01094437949359417 val loss 0.013941656798124313\n",
      "Batch 49 Epoch 23: train loss 0.0015322702238336205 val loss 0.021658943966031075\n",
      "Batch 0 Epoch 24: train loss 0.08191867917776108 val loss 0.03370264545083046\n",
      "Batch 1 Epoch 24: train loss 0.05807296186685562 val loss 0.04999769106507301\n",
      "Batch 2 Epoch 24: train loss 0.05183298513293266 val loss 1.6396994590759277\n",
      "Batch 3 Epoch 24: train loss 0.009836188517510891 val loss 2.1879475116729736\n",
      "Batch 4 Epoch 24: train loss 0.03274588659405708 val loss 2.8310434818267822\n",
      "Batch 5 Epoch 24: train loss 0.030562447383999825 val loss 3.5892159938812256\n",
      "Batch 6 Epoch 24: train loss 0.01112159714102745 val loss 4.453855991363525\n",
      "Batch 7 Epoch 24: train loss 0.0017273698467761278 val loss 5.349398612976074\n",
      "Batch 8 Epoch 24: train loss 0.009419438429176807 val loss 6.140944957733154\n",
      "Batch 9 Epoch 24: train loss 0.012066841125488281 val loss 6.77090311050415\n",
      "Batch 10 Epoch 24: train loss 0.02008301205933094 val loss 7.185307025909424\n",
      "Batch 11 Epoch 24: train loss 0.027703192085027695 val loss 7.32037878036499\n",
      "Batch 12 Epoch 24: train loss 0.022076359018683434 val loss 7.22271203994751\n",
      "Batch 13 Epoch 24: train loss 0.04100784659385681 val loss 6.890658378601074\n",
      "Batch 14 Epoch 24: train loss 0.006153619382530451 val loss 6.500654220581055\n",
      "Batch 15 Epoch 24: train loss 0.0024550599046051502 val loss 6.111807823181152\n",
      "Batch 16 Epoch 24: train loss 0.022538460791110992 val loss 5.9192633628845215\n",
      "Batch 17 Epoch 24: train loss 0.002529393183067441 val loss 5.790282249450684\n",
      "Batch 18 Epoch 24: train loss 0.0026885266415774822 val loss 5.728739261627197\n",
      "Batch 19 Epoch 24: train loss 0.00283380807377398 val loss 5.729850769042969\n",
      "Batch 20 Epoch 24: train loss 0.0035083438269793987 val loss 5.791215896606445\n",
      "Batch 21 Epoch 24: train loss 0.004589937627315521 val loss 5.91953182220459\n",
      "Batch 22 Epoch 24: train loss 0.004738350864499807 val loss 6.119271278381348\n",
      "Batch 23 Epoch 24: train loss 0.001311501837335527 val loss 6.340818405151367\n",
      "Batch 24 Epoch 24: train loss 0.001000148942694068 val loss 6.580718994140625\n",
      "Batch 25 Epoch 24: train loss 0.0006020325236022472 val loss 6.820201873779297\n",
      "Batch 26 Epoch 24: train loss 0.004938935860991478 val loss 6.9689249992370605\n",
      "Batch 27 Epoch 24: train loss 0.002179378177970648 val loss 7.116098403930664\n",
      "Batch 28 Epoch 24: train loss 0.0024944618344306946 val loss 7.1845831871032715\n",
      "Batch 29 Epoch 24: train loss 0.0023910817690193653 val loss 7.192849636077881\n",
      "Batch 30 Epoch 24: train loss 0.002615908859297633 val loss 7.158082485198975\n",
      "Batch 31 Epoch 24: train loss 0.0015940724406391382 val loss 7.105067729949951\n",
      "Batch 32 Epoch 24: train loss 0.0061707221902906895 val loss 6.970685958862305\n",
      "Batch 33 Epoch 24: train loss 0.054639291018247604 val loss 6.662900924682617\n",
      "Batch 34 Epoch 24: train loss 0.02047576569020748 val loss 6.404198169708252\n",
      "Batch 35 Epoch 24: train loss 0.1062416061758995 val loss 6.179038047790527\n",
      "Batch 36 Epoch 24: train loss 0.014448916539549828 val loss 5.979772090911865\n",
      "Batch 37 Epoch 24: train loss 0.05298083275556564 val loss 5.719946384429932\n",
      "Batch 38 Epoch 24: train loss 0.06883280724287033 val loss 5.301787853240967\n",
      "Batch 39 Epoch 24: train loss 0.08243677020072937 val loss 4.612624645233154\n",
      "Batch 40 Epoch 24: train loss 0.1294071525335312 val loss 3.630337953567505\n",
      "Batch 41 Epoch 24: train loss 0.06393659114837646 val loss 2.6833181381225586\n",
      "Batch 42 Epoch 24: train loss 0.20693694055080414 val loss 0.04183145985007286\n",
      "Batch 43 Epoch 24: train loss 0.05510085076093674 val loss 0.020112397149205208\n",
      "Batch 44 Epoch 24: train loss 0.027548406273126602 val loss 0.010236948728561401\n",
      "Batch 45 Epoch 24: train loss 0.0032721548341214657 val loss 0.0116368243470788\n",
      "Batch 46 Epoch 24: train loss 0.08558346331119537 val loss 0.02711227349936962\n",
      "Batch 47 Epoch 24: train loss 0.020022843033075333 val loss 0.056560616940259933\n",
      "Batch 48 Epoch 24: train loss 0.05835747718811035 val loss 0.23492345213890076\n",
      "Batch 49 Epoch 24: train loss 0.10280941426753998 val loss 0.4081408977508545\n",
      "Batch 0 Epoch 25: train loss 0.07845104485750198 val loss 0.5480257868766785\n",
      "Batch 1 Epoch 25: train loss 0.06269153207540512 val loss 0.6458216309547424\n",
      "Batch 2 Epoch 25: train loss 0.0683288723230362 val loss 0.6927921772003174\n",
      "Batch 3 Epoch 25: train loss 0.014285716228187084 val loss 0.7174196243286133\n",
      "Batch 4 Epoch 25: train loss 0.02379525639116764 val loss 0.7214398980140686\n",
      "Batch 5 Epoch 25: train loss 0.04715123400092125 val loss 0.6943641901016235\n",
      "Batch 6 Epoch 25: train loss 0.05718713626265526 val loss 0.630467414855957\n",
      "Batch 7 Epoch 25: train loss 0.047003019601106644 val loss 0.5365173816680908\n",
      "Batch 8 Epoch 25: train loss 0.018805786967277527 val loss 0.43466174602508545\n",
      "Batch 9 Epoch 25: train loss 0.01060025580227375 val loss 0.335589200258255\n",
      "Batch 10 Epoch 25: train loss 0.00475899875164032 val loss 0.2510116994380951\n",
      "Batch 11 Epoch 25: train loss 0.001282908022403717 val loss 0.18819332122802734\n",
      "Batch 12 Epoch 25: train loss 0.006780375726521015 val loss 0.14809098839759827\n",
      "Batch 13 Epoch 25: train loss 0.04733949527144432 val loss 0.1378719061613083\n",
      "Batch 14 Epoch 25: train loss 0.029232310131192207 val loss 0.14706271886825562\n",
      "Batch 15 Epoch 25: train loss 0.029836716130375862 val loss 0.17515556514263153\n",
      "Batch 16 Epoch 25: train loss 0.04462048038840294 val loss 0.22746802866458893\n",
      "Batch 17 Epoch 25: train loss 0.020577412098646164 val loss 0.29897773265838623\n",
      "Batch 18 Epoch 25: train loss 0.006499676965177059 val loss 0.38209712505340576\n",
      "Batch 19 Epoch 25: train loss 0.0004956331104040146 val loss 0.46467703580856323\n",
      "Batch 20 Epoch 25: train loss 0.0018443348817527294 val loss 0.5373125672340393\n",
      "Batch 21 Epoch 25: train loss 0.005862391088157892 val loss 0.5918872356414795\n",
      "Batch 22 Epoch 25: train loss 0.013230964541435242 val loss 0.6207984089851379\n",
      "Batch 23 Epoch 25: train loss 0.013325216248631477 val loss 0.6250450015068054\n",
      "Batch 24 Epoch 25: train loss 0.01546713337302208 val loss 0.605384886264801\n",
      "Batch 25 Epoch 25: train loss 0.012429460883140564 val loss 0.5677233338356018\n",
      "Batch 26 Epoch 25: train loss 0.005375106353312731 val loss 0.5250744223594666\n",
      "Batch 27 Epoch 25: train loss 0.006396111566573381 val loss 0.47660768032073975\n",
      "Batch 28 Epoch 25: train loss 0.0004439426411408931 val loss 0.4336022436618805\n",
      "Batch 29 Epoch 25: train loss 0.001067623496055603 val loss 0.4001295268535614\n",
      "Batch 30 Epoch 25: train loss 0.003769724629819393 val loss 0.3775236904621124\n",
      "Batch 31 Epoch 25: train loss 0.00521508464589715 val loss 0.3662574887275696\n",
      "Batch 32 Epoch 25: train loss 0.02188107930123806 val loss 0.37413808703422546\n",
      "Batch 33 Epoch 25: train loss 0.15510815382003784 val loss 0.4181503355503082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 34 Epoch 25: train loss 0.047447554767131805 val loss 0.4648607075214386\n",
      "Batch 35 Epoch 25: train loss 0.021493306383490562 val loss 0.5084877014160156\n",
      "Batch 36 Epoch 25: train loss 0.03391946852207184 val loss 0.5600529313087463\n",
      "Batch 37 Epoch 25: train loss 0.17865701019763947 val loss 0.6202815771102905\n",
      "Batch 38 Epoch 25: train loss 0.1663573831319809 val loss 0.6448302865028381\n",
      "Batch 39 Epoch 25: train loss 0.10748060047626495 val loss 0.6300492286682129\n",
      "Batch 40 Epoch 25: train loss 0.05622638761997223 val loss 0.5898250341415405\n",
      "Batch 41 Epoch 25: train loss 0.033533066511154175 val loss 0.538087010383606\n",
      "Batch 42 Epoch 25: train loss 0.35229888558387756 val loss 0.4535386264324188\n",
      "Batch 43 Epoch 25: train loss 0.13733111321926117 val loss 0.3227269947528839\n",
      "Batch 44 Epoch 25: train loss 0.17510713636875153 val loss 0.04469548538327217\n",
      "Batch 45 Epoch 25: train loss 0.15582989156246185 val loss 0.01939690113067627\n",
      "Batch 46 Epoch 25: train loss 0.044185880571603775 val loss 0.009856699965894222\n",
      "Batch 47 Epoch 25: train loss 0.07823672145605087 val loss 0.011431689374148846\n",
      "Batch 48 Epoch 25: train loss 0.01164756715297699 val loss 0.01741483248770237\n",
      "Batch 49 Epoch 25: train loss 0.0035025817342102528 val loss 0.025682153180241585\n",
      "Batch 0 Epoch 26: train loss 0.08372419327497482 val loss 0.037408918142318726\n",
      "Batch 1 Epoch 26: train loss 0.06016510725021362 val loss 0.04063853248953819\n",
      "Batch 2 Epoch 26: train loss 0.052448973059654236 val loss 1.2037979364395142\n",
      "Batch 3 Epoch 26: train loss 0.010058755986392498 val loss 1.6172887086868286\n",
      "Batch 4 Epoch 26: train loss 0.03295975551009178 val loss 2.1095848083496094\n",
      "Batch 5 Epoch 26: train loss 0.02709759958088398 val loss 2.693575143814087\n",
      "Batch 6 Epoch 26: train loss 0.007841084152460098 val loss 3.3548314571380615\n",
      "Batch 7 Epoch 26: train loss 0.0023788094986230135 val loss 4.012319564819336\n",
      "Batch 8 Epoch 26: train loss 0.014588813297450542 val loss 4.544304370880127\n",
      "Batch 9 Epoch 26: train loss 0.018358958885073662 val loss 4.902083396911621\n",
      "Batch 10 Epoch 26: train loss 0.024935927242040634 val loss 5.058143615722656\n",
      "Batch 11 Epoch 26: train loss 0.03209887817502022 val loss 4.976902484893799\n",
      "Batch 12 Epoch 26: train loss 0.01999390870332718 val loss 4.737971305847168\n",
      "Batch 13 Epoch 26: train loss 0.03442836180329323 val loss 4.365293502807617\n",
      "Batch 14 Epoch 26: train loss 0.0025547342374920845 val loss 4.00037145614624\n",
      "Batch 15 Epoch 26: train loss 0.0012650268618017435 val loss 3.6797754764556885\n",
      "Batch 16 Epoch 26: train loss 0.04291190207004547 val loss 3.5467138290405273\n",
      "Batch 17 Epoch 26: train loss 0.008453074842691422 val loss 3.4922683238983154\n",
      "Batch 18 Epoch 26: train loss 0.0071287923492491245 val loss 3.51064133644104\n",
      "Batch 19 Epoch 26: train loss 0.005211345851421356 val loss 3.587520122528076\n",
      "Batch 20 Epoch 26: train loss 0.003838780103251338 val loss 3.706824541091919\n",
      "Batch 21 Epoch 26: train loss 0.004359259270131588 val loss 3.8720550537109375\n",
      "Batch 22 Epoch 26: train loss 0.0033903548028320074 val loss 4.078158378601074\n",
      "Batch 23 Epoch 26: train loss 0.0005890006432309747 val loss 4.278990745544434\n",
      "Batch 24 Epoch 26: train loss 0.00030452298233285546 val loss 4.468259334564209\n",
      "Batch 25 Epoch 26: train loss 0.0006148798856884241 val loss 4.626346111297607\n",
      "Batch 26 Epoch 26: train loss 0.008258963003754616 val loss 4.681964874267578\n",
      "Batch 27 Epoch 26: train loss 0.002500244416296482 val loss 4.713456630706787\n",
      "Batch 28 Epoch 26: train loss 0.004405016545206308 val loss 4.671051502227783\n",
      "Batch 29 Epoch 26: train loss 0.0030168460216373205 val loss 4.584046363830566\n",
      "Batch 30 Epoch 26: train loss 0.002693630987778306 val loss 4.474003314971924\n",
      "Batch 31 Epoch 26: train loss 0.0015272003365680575 val loss 4.363154888153076\n",
      "Batch 32 Epoch 26: train loss 0.00513805728405714 val loss 4.207736968994141\n",
      "Batch 33 Epoch 26: train loss 0.051624201238155365 val loss 3.941634178161621\n",
      "Batch 34 Epoch 26: train loss 0.01827675849199295 val loss 3.720538377761841\n",
      "Batch 35 Epoch 26: train loss 0.09508828818798065 val loss 3.5321807861328125\n",
      "Batch 36 Epoch 26: train loss 0.012794073671102524 val loss 3.3680925369262695\n",
      "Batch 37 Epoch 26: train loss 0.048766765743494034 val loss 3.1713554859161377\n",
      "Batch 38 Epoch 26: train loss 0.05953586474061012 val loss 2.8916056156158447\n",
      "Batch 39 Epoch 26: train loss 0.0610356330871582 val loss 2.4808189868927\n",
      "Batch 40 Epoch 26: train loss 0.09324803948402405 val loss 1.9261987209320068\n",
      "Batch 41 Epoch 26: train loss 0.0439283661544323 val loss 0.042379532009363174\n",
      "Batch 42 Epoch 26: train loss 0.2061065286397934 val loss 0.026114407926797867\n",
      "Batch 43 Epoch 26: train loss 0.04047660902142525 val loss 0.014478134922683239\n",
      "Batch 44 Epoch 26: train loss 0.020781416445970535 val loss 0.009719228371977806\n",
      "Batch 45 Epoch 26: train loss 0.0029860995709896088 val loss 0.010827997699379921\n",
      "Batch 46 Epoch 26: train loss 0.08087371289730072 val loss 0.02019616961479187\n",
      "Batch 47 Epoch 26: train loss 0.0257648304104805 val loss 0.03996763750910759\n",
      "Batch 48 Epoch 26: train loss 0.041463833302259445 val loss 0.16169384121894836\n",
      "Batch 49 Epoch 26: train loss 0.06922807544469833 val loss 0.28387951850891113\n",
      "Batch 0 Epoch 27: train loss 0.08361882716417313 val loss 0.37929436564445496\n",
      "Batch 1 Epoch 27: train loss 0.0663045197725296 val loss 0.44109851121902466\n",
      "Batch 2 Epoch 27: train loss 0.06942234188318253 val loss 0.4630017876625061\n",
      "Batch 3 Epoch 27: train loss 0.015535864047706127 val loss 0.4679069519042969\n",
      "Batch 4 Epoch 27: train loss 0.02600804716348648 val loss 0.45694273710250854\n",
      "Batch 5 Epoch 27: train loss 0.046383894979953766 val loss 0.42313912510871887\n",
      "Batch 6 Epoch 27: train loss 0.051624931395053864 val loss 0.36443498730659485\n",
      "Batch 7 Epoch 27: train loss 0.03791229426860809 val loss 0.2890380918979645\n",
      "Batch 8 Epoch 27: train loss 0.013593444600701332 val loss 0.21435928344726562\n",
      "Batch 9 Epoch 27: train loss 0.0060343495570123196 val loss 0.14842970669269562\n",
      "Batch 10 Epoch 27: train loss 0.003218159545212984 val loss 0.09798957407474518\n",
      "Batch 11 Epoch 27: train loss 0.0033173048868775368 val loss 0.06557592004537582\n",
      "Batch 12 Epoch 27: train loss 0.010483977384865284 val loss 0.047450948506593704\n",
      "Batch 13 Epoch 27: train loss 0.05150929465889931 val loss 0.04779765009880066\n",
      "Batch 14 Epoch 27: train loss 0.028964761644601822 val loss 0.059295035898685455\n",
      "Batch 15 Epoch 27: train loss 0.02814549393951893 val loss 0.08302558213472366\n",
      "Batch 16 Epoch 27: train loss 0.033142704516649246 val loss 0.12406661361455917\n",
      "Batch 17 Epoch 27: train loss 0.011231975629925728 val loss 0.17922106385231018\n",
      "Batch 18 Epoch 27: train loss 0.003911419305950403 val loss 0.2440798580646515\n",
      "Batch 19 Epoch 27: train loss 0.0004273255181033164 val loss 0.3083343505859375\n",
      "Batch 20 Epoch 27: train loss 0.0025915694423019886 val loss 0.3644293248653412\n",
      "Batch 21 Epoch 27: train loss 0.007270541042089462 val loss 0.40477022528648376\n",
      "Batch 22 Epoch 27: train loss 0.014366275630891323 val loss 0.4229947328567505\n",
      "Batch 23 Epoch 27: train loss 0.012941212393343449 val loss 0.42095115780830383\n",
      "Batch 24 Epoch 27: train loss 0.014326274394989014 val loss 0.39980852603912354\n",
      "Batch 25 Epoch 27: train loss 0.011283368803560734 val loss 0.3648449778556824\n",
      "Batch 26 Epoch 27: train loss 0.004735833499580622 val loss 0.32775604724884033\n",
      "Batch 27 Epoch 27: train loss 0.005015313625335693 val loss 0.288033127784729\n",
      "Batch 28 Epoch 27: train loss 0.00036499692942015827 val loss 0.25497278571128845\n",
      "Batch 29 Epoch 27: train loss 0.0015982325421646237 val loss 0.23086751997470856\n",
      "Batch 30 Epoch 27: train loss 0.004887061193585396 val loss 0.21653705835342407\n",
      "Batch 31 Epoch 27: train loss 0.0059321485459804535 val loss 0.21149452030658722\n",
      "Batch 32 Epoch 27: train loss 0.022151360288262367 val loss 0.2219143807888031\n",
      "Batch 33 Epoch 27: train loss 0.14259575307369232 val loss 0.2623381018638611\n",
      "Batch 34 Epoch 27: train loss 0.032196883112192154 val loss 0.3061263859272003\n",
      "Batch 35 Epoch 27: train loss 0.019014088436961174 val loss 0.34791791439056396\n",
      "Batch 36 Epoch 27: train loss 0.02980913035571575 val loss 0.39758092164993286\n",
      "Batch 37 Epoch 27: train loss 0.14853358268737793 val loss 0.45785850286483765\n",
      "Batch 38 Epoch 27: train loss 0.12213483452796936 val loss 0.4946419894695282\n",
      "Batch 39 Epoch 27: train loss 0.07740139216184616 val loss 0.5017826557159424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 40 Epoch 27: train loss 0.03925527632236481 val loss 0.48801302909851074\n",
      "Batch 41 Epoch 27: train loss 0.024784760549664497 val loss 0.4625684916973114\n",
      "Batch 42 Epoch 27: train loss 0.32713598012924194 val loss 0.4065839648246765\n",
      "Batch 43 Epoch 27: train loss 0.11827173829078674 val loss 0.30436569452285767\n",
      "Batch 44 Epoch 27: train loss 0.15192636847496033 val loss 0.04208856448531151\n",
      "Batch 45 Epoch 27: train loss 0.14355750381946564 val loss 0.020353036001324654\n",
      "Batch 46 Epoch 27: train loss 0.04249688610434532 val loss 0.010508078150451183\n",
      "Batch 47 Epoch 27: train loss 0.05648015812039375 val loss 0.009997325949370861\n",
      "Batch 48 Epoch 27: train loss 0.010372206568717957 val loss 0.013562863692641258\n",
      "Batch 49 Epoch 27: train loss 0.0020440942607820034 val loss 0.019777309149503708\n",
      "Batch 0 Epoch 28: train loss 0.0746343731880188 val loss 0.029490331187844276\n",
      "Batch 1 Epoch 28: train loss 0.050866201519966125 val loss 0.04272748902440071\n",
      "Batch 2 Epoch 28: train loss 0.04294046387076378 val loss 1.0232549905776978\n",
      "Batch 3 Epoch 28: train loss 0.009013688191771507 val loss 1.3900926113128662\n",
      "Batch 4 Epoch 28: train loss 0.029615530744194984 val loss 1.8258806467056274\n",
      "Batch 5 Epoch 28: train loss 0.026863664388656616 val loss 2.3437979221343994\n",
      "Batch 6 Epoch 28: train loss 0.00975537858903408 val loss 2.9379773139953613\n",
      "Batch 7 Epoch 28: train loss 0.001730588497593999 val loss 3.5492873191833496\n",
      "Batch 8 Epoch 28: train loss 0.008926005102694035 val loss 4.0838093757629395\n",
      "Batch 9 Epoch 28: train loss 0.011422376148402691 val loss 4.499727249145508\n",
      "Batch 10 Epoch 28: train loss 0.017550982534885406 val loss 4.765405654907227\n",
      "Batch 11 Epoch 28: train loss 0.025173548609018326 val loss 4.8310160636901855\n",
      "Batch 12 Epoch 28: train loss 0.019426807761192322 val loss 4.737093925476074\n",
      "Batch 13 Epoch 28: train loss 0.03703473135828972 val loss 4.486576557159424\n",
      "Batch 14 Epoch 28: train loss 0.004904026165604591 val loss 4.20460844039917\n",
      "Batch 15 Epoch 28: train loss 0.002188174519687891 val loss 3.929837465286255\n",
      "Batch 16 Epoch 28: train loss 0.02200431004166603 val loss 3.796898603439331\n",
      "Batch 17 Epoch 28: train loss 0.0034523578360676765 val loss 3.715529203414917\n",
      "Batch 18 Epoch 28: train loss 0.002812114078551531 val loss 3.6816210746765137\n",
      "Batch 19 Epoch 28: train loss 0.002978238044306636 val loss 3.6941781044006348\n",
      "Batch 20 Epoch 28: train loss 0.002967267530038953 val loss 3.7439749240875244\n",
      "Batch 21 Epoch 28: train loss 0.003993204794824123 val loss 3.8393781185150146\n",
      "Batch 22 Epoch 28: train loss 0.004174443427473307 val loss 3.9835100173950195\n",
      "Batch 23 Epoch 28: train loss 0.0011234610574319959 val loss 4.138298511505127\n",
      "Batch 24 Epoch 28: train loss 0.0007765847258269787 val loss 4.303399562835693\n",
      "Batch 25 Epoch 28: train loss 0.0004421997000463307 val loss 4.461986541748047\n",
      "Batch 26 Epoch 28: train loss 0.005034839268773794 val loss 4.552054405212402\n",
      "Batch 27 Epoch 28: train loss 0.0020559036638587713 val loss 4.641281604766846\n",
      "Batch 28 Epoch 28: train loss 0.002669492969289422 val loss 4.672435760498047\n",
      "Batch 29 Epoch 28: train loss 0.0020428744610399008 val loss 4.666210174560547\n",
      "Batch 30 Epoch 28: train loss 0.0025285184383392334 val loss 4.635765552520752\n",
      "Batch 31 Epoch 28: train loss 0.0013979023788124323 val loss 4.59775447845459\n",
      "Batch 32 Epoch 28: train loss 0.005446108523756266 val loss 4.504941940307617\n",
      "Batch 33 Epoch 28: train loss 0.051229868084192276 val loss 4.293324947357178\n",
      "Batch 34 Epoch 28: train loss 0.02302793599665165 val loss 4.119532585144043\n",
      "Batch 35 Epoch 28: train loss 0.10000402480363846 val loss 3.9738047122955322\n",
      "Batch 36 Epoch 28: train loss 0.012081203982234001 val loss 3.842855453491211\n",
      "Batch 37 Epoch 28: train loss 0.05014168471097946 val loss 3.6681408882141113\n",
      "Batch 38 Epoch 28: train loss 0.0775795727968216 val loss 3.3667972087860107\n",
      "Batch 39 Epoch 28: train loss 0.08074107766151428 val loss 2.897933006286621\n",
      "Batch 40 Epoch 28: train loss 0.11517764627933502 val loss 2.2626829147338867\n",
      "Batch 41 Epoch 28: train loss 0.05331853777170181 val loss 1.6635568141937256\n",
      "Batch 42 Epoch 28: train loss 0.21133022010326385 val loss 0.035256899893283844\n",
      "Batch 43 Epoch 28: train loss 0.05493483319878578 val loss 0.01818537712097168\n",
      "Batch 44 Epoch 28: train loss 0.027444345876574516 val loss 0.010130243375897408\n",
      "Batch 45 Epoch 28: train loss 0.0029360188636928797 val loss 0.010649929754436016\n",
      "Batch 46 Epoch 28: train loss 0.07956410944461823 val loss 0.02132372185587883\n",
      "Batch 47 Epoch 28: train loss 0.020394425839185715 val loss 0.04290754720568657\n",
      "Batch 48 Epoch 28: train loss 0.04592365026473999 val loss 0.19330240786075592\n",
      "Batch 49 Epoch 28: train loss 0.07358568161725998 val loss 0.33154645562171936\n",
      "Batch 0 Epoch 29: train loss 0.08505525439977646 val loss 0.44071534276008606\n",
      "Batch 1 Epoch 29: train loss 0.06633226573467255 val loss 0.5143181681632996\n",
      "Batch 2 Epoch 29: train loss 0.07092088460922241 val loss 0.5455034971237183\n",
      "Batch 3 Epoch 29: train loss 0.01713092252612114 val loss 0.5574545860290527\n",
      "Batch 4 Epoch 29: train loss 0.027957791462540627 val loss 0.5514505505561829\n",
      "Batch 5 Epoch 29: train loss 0.04925907403230667 val loss 0.5200802683830261\n",
      "Batch 6 Epoch 29: train loss 0.057554323226213455 val loss 0.45997223258018494\n",
      "Batch 7 Epoch 29: train loss 0.04463573917746544 val loss 0.3783888816833496\n",
      "Batch 8 Epoch 29: train loss 0.017788607627153397 val loss 0.29356876015663147\n",
      "Batch 9 Epoch 29: train loss 0.008777245879173279 val loss 0.21486890316009521\n",
      "Batch 10 Epoch 29: train loss 0.0039791385643184185 val loss 0.15084926784038544\n",
      "Batch 11 Epoch 29: train loss 0.002341492334380746 val loss 0.10653707385063171\n",
      "Batch 12 Epoch 29: train loss 0.008970136754214764 val loss 0.08062192052602768\n",
      "Batch 13 Epoch 29: train loss 0.0490151010453701 val loss 0.0760556161403656\n",
      "Batch 14 Epoch 29: train loss 0.028992483392357826 val loss 0.08515312522649765\n",
      "Batch 15 Epoch 29: train loss 0.02957548387348652 val loss 0.10853785276412964\n",
      "Batch 16 Epoch 29: train loss 0.040448736399412155 val loss 0.15138253569602966\n",
      "Batch 17 Epoch 29: train loss 0.013507957570254803 val loss 0.2086796909570694\n",
      "Batch 18 Epoch 29: train loss 0.00510038249194622 val loss 0.27597498893737793\n",
      "Batch 19 Epoch 29: train loss 0.0003631046274676919 val loss 0.34291017055511475\n",
      "Batch 20 Epoch 29: train loss 0.0019848058000206947 val loss 0.4020494818687439\n",
      "Batch 21 Epoch 29: train loss 0.006744429934769869 val loss 0.44556036591529846\n",
      "Batch 22 Epoch 29: train loss 0.013369361869990826 val loss 0.46740129590034485\n",
      "Batch 23 Epoch 29: train loss 0.012646723538637161 val loss 0.4688020944595337\n",
      "Batch 24 Epoch 29: train loss 0.014266500249505043 val loss 0.45067331194877625\n",
      "Batch 25 Epoch 29: train loss 0.012445155531167984 val loss 0.4172727167606354\n",
      "Batch 26 Epoch 29: train loss 0.004790887702256441 val loss 0.38074925541877747\n",
      "Batch 27 Epoch 29: train loss 0.005553014110773802 val loss 0.3402879536151886\n",
      "Batch 28 Epoch 29: train loss 0.00037968013202771544 val loss 0.3056557774543762\n",
      "Batch 29 Epoch 29: train loss 0.001246445463038981 val loss 0.2793843746185303\n",
      "Batch 30 Epoch 29: train loss 0.0041556283831596375 val loss 0.26251041889190674\n",
      "Batch 31 Epoch 29: train loss 0.005463880952447653 val loss 0.2550128102302551\n",
      "Batch 32 Epoch 29: train loss 0.02162770740687847 val loss 0.2635655701160431\n",
      "Batch 33 Epoch 29: train loss 0.1435604840517044 val loss 0.3029051125049591\n",
      "Batch 34 Epoch 29: train loss 0.032830867916345596 val loss 0.34554553031921387\n",
      "Batch 35 Epoch 29: train loss 0.018903762102127075 val loss 0.38597387075424194\n",
      "Batch 36 Epoch 29: train loss 0.030167140066623688 val loss 0.4341687858104706\n",
      "Batch 37 Epoch 29: train loss 0.15416808426380157 val loss 0.49336352944374084\n",
      "Batch 38 Epoch 29: train loss 0.1353602260351181 val loss 0.5324554443359375\n",
      "Batch 39 Epoch 29: train loss 0.08250560611486435 val loss 0.5433436036109924\n",
      "Batch 40 Epoch 29: train loss 0.04360140115022659 val loss 0.5327870845794678\n",
      "Batch 41 Epoch 29: train loss 0.027518561109900475 val loss 0.5096868276596069\n",
      "Batch 42 Epoch 29: train loss 0.337483674287796 val loss 0.4546031355857849\n",
      "Batch 43 Epoch 29: train loss 0.12672649323940277 val loss 0.35065701603889465\n",
      "Batch 44 Epoch 29: train loss 0.16967040300369263 val loss 0.04548831656575203\n",
      "Batch 45 Epoch 29: train loss 0.15756772458553314 val loss 0.021296074613928795\n",
      "Batch 46 Epoch 29: train loss 0.042143113911151886 val loss 0.010185183957219124\n",
      "Batch 47 Epoch 29: train loss 0.05634631961584091 val loss 0.01024175900965929\n",
      "Batch 48 Epoch 29: train loss 0.010603399947285652 val loss 0.015470923855900764\n",
      "Batch 49 Epoch 29: train loss 0.002982749603688717 val loss 0.023894377052783966\n",
      "Batch 0 Epoch 30: train loss 0.07058621197938919 val loss 0.036519765853881836\n",
      "Batch 1 Epoch 30: train loss 0.04680334031581879 val loss 0.04451116546988487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2 Epoch 30: train loss 0.03576602041721344 val loss 1.1251944303512573\n",
      "Batch 3 Epoch 30: train loss 0.007902804762125015 val loss 1.5340548753738403\n",
      "Batch 4 Epoch 30: train loss 0.024356182664632797 val loss 2.017853260040283\n",
      "Batch 5 Epoch 30: train loss 0.019465278834104538 val loss 2.583350658416748\n",
      "Batch 6 Epoch 30: train loss 0.005077603738754988 val loss 3.2135519981384277\n",
      "Batch 7 Epoch 30: train loss 0.003553129034116864 val loss 3.8201918601989746\n",
      "Batch 8 Epoch 30: train loss 0.01701473817229271 val loss 4.291589260101318\n",
      "Batch 9 Epoch 30: train loss 0.018568895757198334 val loss 4.5933051109313965\n",
      "Batch 10 Epoch 30: train loss 0.023878516629338264 val loss 4.708707332611084\n",
      "Batch 11 Epoch 30: train loss 0.026052679866552353 val loss 4.623133182525635\n",
      "Batch 12 Epoch 30: train loss 0.015229742042720318 val loss 4.411227226257324\n",
      "Batch 13 Epoch 30: train loss 0.028773542493581772 val loss 4.0909342765808105\n",
      "Batch 14 Epoch 30: train loss 0.0007814595010131598 val loss 3.7919459342956543\n",
      "Batch 15 Epoch 30: train loss 0.0011858323123306036 val loss 3.5443382263183594\n",
      "Batch 16 Epoch 30: train loss 0.028156401589512825 val loss 3.4488420486450195\n",
      "Batch 17 Epoch 30: train loss 0.008555198088288307 val loss 3.4300150871276855\n",
      "Batch 18 Epoch 30: train loss 0.008083694614470005 val loss 3.4860448837280273\n",
      "Batch 19 Epoch 30: train loss 0.007342363242059946 val loss 3.6100590229034424\n",
      "Batch 20 Epoch 30: train loss 0.005311894696205854 val loss 3.7860987186431885\n",
      "Batch 21 Epoch 30: train loss 0.005660644266754389 val loss 4.016097545623779\n",
      "Batch 22 Epoch 30: train loss 0.003332605818286538 val loss 4.286148548126221\n",
      "Batch 23 Epoch 30: train loss 0.0005397481145337224 val loss 4.539877891540527\n",
      "Batch 24 Epoch 30: train loss 0.0004916229518130422 val loss 4.764541149139404\n",
      "Batch 25 Epoch 30: train loss 0.0014354477170854807 val loss 4.934377193450928\n",
      "Batch 26 Epoch 30: train loss 0.009740080684423447 val loss 4.984628677368164\n",
      "Batch 27 Epoch 30: train loss 0.0026272570248693228 val loss 5.003414630889893\n",
      "Batch 28 Epoch 30: train loss 0.005633852910250425 val loss 4.934738636016846\n",
      "Batch 29 Epoch 30: train loss 0.0031392090022563934 val loss 4.8206892013549805\n",
      "Batch 30 Epoch 30: train loss 0.0025552564766258 val loss 4.687366485595703\n",
      "Batch 31 Epoch 30: train loss 0.0014299212489277124 val loss 4.561745643615723\n",
      "Batch 32 Epoch 30: train loss 0.003752738470211625 val loss 4.400910377502441\n",
      "Batch 33 Epoch 30: train loss 0.046285323798656464 val loss 4.133446216583252\n",
      "Batch 34 Epoch 30: train loss 0.026328973472118378 val loss 3.9121286869049072\n",
      "Batch 35 Epoch 30: train loss 0.10737095773220062 val loss 3.718580961227417\n",
      "Batch 36 Epoch 30: train loss 0.012152162380516529 val loss 3.5528910160064697\n",
      "Batch 37 Epoch 30: train loss 0.04424343630671501 val loss 3.3544232845306396\n",
      "Batch 38 Epoch 30: train loss 0.052689943462610245 val loss 3.084390878677368\n",
      "Batch 39 Epoch 30: train loss 0.07369031012058258 val loss 2.646437406539917\n",
      "Batch 40 Epoch 30: train loss 0.11164280772209167 val loss 2.0283851623535156\n",
      "Batch 41 Epoch 30: train loss 0.052686698734760284 val loss 1.4451743364334106\n",
      "Batch 42 Epoch 30: train loss 0.20551681518554688 val loss 0.030755214393138885\n",
      "Batch 43 Epoch 30: train loss 0.04527514800429344 val loss 0.01568393036723137\n",
      "Batch 44 Epoch 30: train loss 0.021686870604753494 val loss 0.009572046808898449\n",
      "Batch 45 Epoch 30: train loss 0.0030090673826634884 val loss 0.011067196726799011\n",
      "Batch 46 Epoch 30: train loss 0.07476187497377396 val loss 0.022066811099648476\n",
      "Batch 47 Epoch 30: train loss 0.01716024987399578 val loss 0.04311225935816765\n",
      "Batch 48 Epoch 30: train loss 0.04640757292509079 val loss 0.20558267831802368\n",
      "Batch 49 Epoch 30: train loss 0.0707608312368393 val loss 0.3317369222640991\n",
      "Batch 0 Epoch 31: train loss 0.08342096209526062 val loss 0.4231869578361511\n",
      "Batch 1 Epoch 31: train loss 0.06395513564348221 val loss 0.47740158438682556\n",
      "Batch 2 Epoch 31: train loss 0.06495453417301178 val loss 0.4904542565345764\n",
      "Batch 3 Epoch 31: train loss 0.014492119662463665 val loss 0.4871123135089874\n",
      "Batch 4 Epoch 31: train loss 0.02339327335357666 val loss 0.4689415991306305\n",
      "Batch 5 Epoch 31: train loss 0.04140668362379074 val loss 0.42952609062194824\n",
      "Batch 6 Epoch 31: train loss 0.04718291759490967 val loss 0.366850882768631\n",
      "Batch 7 Epoch 31: train loss 0.0337085984647274 val loss 0.2896963357925415\n",
      "Batch 8 Epoch 31: train loss 0.011630336754024029 val loss 0.21490295231342316\n",
      "Batch 9 Epoch 31: train loss 0.004643825348466635 val loss 0.15004757046699524\n",
      "Batch 10 Epoch 31: train loss 0.0033269119448959827 val loss 0.10093608498573303\n",
      "Batch 11 Epoch 31: train loss 0.004622717387974262 val loss 0.07017464190721512\n",
      "Batch 12 Epoch 31: train loss 0.010463868267834187 val loss 0.05419540032744408\n",
      "Batch 13 Epoch 31: train loss 0.05085347220301628 val loss 0.055183324962854385\n",
      "Batch 14 Epoch 31: train loss 0.027212917804718018 val loss 0.067367784678936\n",
      "Batch 15 Epoch 31: train loss 0.025699175894260406 val loss 0.092193104326725\n",
      "Batch 16 Epoch 31: train loss 0.03300178796052933 val loss 0.13419592380523682\n",
      "Batch 17 Epoch 31: train loss 0.010916732251644135 val loss 0.18954595923423767\n",
      "Batch 18 Epoch 31: train loss 0.00290616974234581 val loss 0.25269368290901184\n",
      "Batch 19 Epoch 31: train loss 0.0007604529382660985 val loss 0.3130267560482025\n",
      "Batch 20 Epoch 31: train loss 0.0033358512446284294 val loss 0.36372706294059753\n",
      "Batch 21 Epoch 31: train loss 0.008612498641014099 val loss 0.397774875164032\n",
      "Batch 22 Epoch 31: train loss 0.015472562052309513 val loss 0.40994906425476074\n",
      "Batch 23 Epoch 31: train loss 0.013753422535955906 val loss 0.4024091362953186\n",
      "Batch 24 Epoch 31: train loss 0.01484938245266676 val loss 0.37688755989074707\n",
      "Batch 25 Epoch 31: train loss 0.011387234553694725 val loss 0.3390428423881531\n",
      "Batch 26 Epoch 31: train loss 0.004612952470779419 val loss 0.30025815963745117\n",
      "Batch 27 Epoch 31: train loss 0.004888125695288181 val loss 0.2600887715816498\n",
      "Batch 28 Epoch 31: train loss 0.0005435717175714672 val loss 0.22745561599731445\n",
      "Batch 29 Epoch 31: train loss 0.00203923461958766 val loss 0.2041168510913849\n",
      "Batch 30 Epoch 31: train loss 0.005431266501545906 val loss 0.1904432326555252\n",
      "Batch 31 Epoch 31: train loss 0.006188948638737202 val loss 0.18558922410011292\n",
      "Batch 32 Epoch 31: train loss 0.023647651076316833 val loss 0.19559650123119354\n",
      "Batch 33 Epoch 31: train loss 0.1399802416563034 val loss 0.2333069145679474\n",
      "Batch 34 Epoch 31: train loss 0.03054730035364628 val loss 0.27436938881874084\n",
      "Batch 35 Epoch 31: train loss 0.01867668889462948 val loss 0.31369051337242126\n",
      "Batch 36 Epoch 31: train loss 0.029410723596811295 val loss 0.36048078536987305\n",
      "Batch 37 Epoch 31: train loss 0.1399163156747818 val loss 0.4173696041107178\n",
      "Batch 38 Epoch 31: train loss 0.11974872648715973 val loss 0.4527563452720642\n",
      "Batch 39 Epoch 31: train loss 0.07467024773359299 val loss 0.4612240493297577\n",
      "Batch 40 Epoch 31: train loss 0.03531976044178009 val loss 0.4511931240558624\n",
      "Batch 41 Epoch 31: train loss 0.022756189107894897 val loss 0.43014222383499146\n",
      "Batch 42 Epoch 31: train loss 0.31640058755874634 val loss 0.3808596432209015\n",
      "Batch 43 Epoch 31: train loss 0.10728186368942261 val loss 0.288972407579422\n",
      "Batch 44 Epoch 31: train loss 0.13695383071899414 val loss 0.038431260734796524\n",
      "Batch 45 Epoch 31: train loss 0.13284043967723846 val loss 0.01849164441227913\n",
      "Batch 46 Epoch 31: train loss 0.040152620524168015 val loss 0.009999061934649944\n",
      "Batch 47 Epoch 31: train loss 0.05203761160373688 val loss 0.010135454125702381\n",
      "Batch 48 Epoch 31: train loss 0.010277039371430874 val loss 0.01409428846091032\n",
      "Batch 49 Epoch 31: train loss 0.002704655984416604 val loss 0.020361201837658882\n",
      "Batch 0 Epoch 32: train loss 0.07915237545967102 val loss 0.029993345960974693\n",
      "Batch 1 Epoch 32: train loss 0.052871882915496826 val loss 0.04304027929902077\n",
      "Batch 2 Epoch 32: train loss 0.04285101965069771 val loss 0.9210361838340759\n",
      "Batch 3 Epoch 32: train loss 0.009980420581996441 val loss 1.2700151205062866\n",
      "Batch 4 Epoch 32: train loss 0.02820960059762001 val loss 1.689071536064148\n",
      "Batch 5 Epoch 32: train loss 0.0222486462444067 val loss 2.1859323978424072\n",
      "Batch 6 Epoch 32: train loss 0.00716220261529088 val loss 2.7510387897491455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7 Epoch 32: train loss 0.0022182599641382694 val loss 3.315568685531616\n",
      "Batch 8 Epoch 32: train loss 0.012546615675091743 val loss 3.7802371978759766\n",
      "Batch 9 Epoch 32: train loss 0.014752854593098164 val loss 4.107364177703857\n",
      "Batch 10 Epoch 32: train loss 0.020929230377078056 val loss 4.272568225860596\n",
      "Batch 11 Epoch 32: train loss 0.026969684287905693 val loss 4.241823673248291\n",
      "Batch 12 Epoch 32: train loss 0.016598330810666084 val loss 4.079710006713867\n",
      "Batch 13 Epoch 32: train loss 0.03342228755354881 val loss 3.792811393737793\n",
      "Batch 14 Epoch 32: train loss 0.0028855532873421907 val loss 3.500671863555908\n",
      "Batch 15 Epoch 32: train loss 0.00117427424993366 val loss 3.2397069931030273\n",
      "Batch 16 Epoch 32: train loss 0.025659170001745224 val loss 3.118288993835449\n",
      "Batch 17 Epoch 32: train loss 0.005380245391279459 val loss 3.057096242904663\n",
      "Batch 18 Epoch 32: train loss 0.005151791963726282 val loss 3.0546839237213135\n",
      "Batch 19 Epoch 32: train loss 0.0050797127187252045 val loss 3.1064958572387695\n",
      "Batch 20 Epoch 32: train loss 0.0038395654410123825 val loss 3.1987030506134033\n",
      "Batch 21 Epoch 32: train loss 0.0053193531930446625 val loss 3.3404314517974854\n",
      "Batch 22 Epoch 32: train loss 0.00380938732996583 val loss 3.5231876373291016\n",
      "Batch 23 Epoch 32: train loss 0.0007955392939038575 val loss 3.7054569721221924\n",
      "Batch 24 Epoch 32: train loss 0.0005894401692785323 val loss 3.885141372680664\n",
      "Batch 25 Epoch 32: train loss 0.0005655101267620921 val loss 4.042585849761963\n",
      "Batch 26 Epoch 32: train loss 0.0069575258530676365 val loss 4.1143012046813965\n",
      "Batch 27 Epoch 32: train loss 0.001826027873903513 val loss 4.171183109283447\n",
      "Batch 28 Epoch 32: train loss 0.0040248665027320385 val loss 4.160429000854492\n",
      "Batch 29 Epoch 32: train loss 0.002393882256001234 val loss 4.111312389373779\n",
      "Batch 30 Epoch 32: train loss 0.003130772151052952 val loss 4.033647537231445\n",
      "Batch 31 Epoch 32: train loss 0.0017229279037564993 val loss 3.9507009983062744\n",
      "Batch 32 Epoch 32: train loss 0.005146585404872894 val loss 3.8230936527252197\n",
      "Batch 33 Epoch 32: train loss 0.04953420162200928 val loss 3.5929083824157715\n",
      "Batch 34 Epoch 32: train loss 0.0219242163002491 val loss 3.4050021171569824\n",
      "Batch 35 Epoch 32: train loss 0.09519032388925552 val loss 3.2495553493499756\n",
      "Batch 36 Epoch 32: train loss 0.011535356752574444 val loss 3.1120033264160156\n",
      "Batch 37 Epoch 32: train loss 0.047571513801813126 val loss 2.939810037612915\n",
      "Batch 38 Epoch 32: train loss 0.06414259225130081 val loss 2.681043863296509\n",
      "Batch 39 Epoch 32: train loss 0.07006435096263885 val loss 2.2932450771331787\n",
      "Batch 40 Epoch 32: train loss 0.1006714254617691 val loss 1.7748738527297974\n",
      "Batch 41 Epoch 32: train loss 0.0460888035595417 val loss 0.04766111820936203\n",
      "Batch 42 Epoch 32: train loss 0.21087662875652313 val loss 0.02934398502111435\n",
      "Batch 43 Epoch 32: train loss 0.047120023518800735 val loss 0.015827592462301254\n",
      "Batch 44 Epoch 32: train loss 0.024871524423360825 val loss 0.009747046045958996\n",
      "Batch 45 Epoch 32: train loss 0.0031401950400322676 val loss 0.010487964376807213\n",
      "Batch 46 Epoch 32: train loss 0.07404777407646179 val loss 0.019781265407800674\n",
      "Batch 47 Epoch 32: train loss 0.0186845101416111 val loss 0.03837345913052559\n",
      "Batch 48 Epoch 32: train loss 0.042277343571186066 val loss 0.18446087837219238\n",
      "Batch 49 Epoch 32: train loss 0.062453072518110275 val loss 0.30665072798728943\n",
      "Batch 0 Epoch 33: train loss 0.08778756111860275 val loss 0.3974456787109375\n",
      "Batch 1 Epoch 33: train loss 0.06762603670358658 val loss 0.45276233553886414\n",
      "Batch 2 Epoch 33: train loss 0.07015512883663177 val loss 0.467650830745697\n",
      "Batch 3 Epoch 33: train loss 0.017990214750170708 val loss 0.4650014042854309\n",
      "Batch 4 Epoch 33: train loss 0.02787068858742714 val loss 0.4467467665672302\n",
      "Batch 5 Epoch 33: train loss 0.046733301132917404 val loss 0.40697816014289856\n",
      "Batch 6 Epoch 33: train loss 0.05344920977950096 val loss 0.3439859449863434\n",
      "Batch 7 Epoch 33: train loss 0.037757549434900284 val loss 0.266924649477005\n",
      "Batch 8 Epoch 33: train loss 0.013442667201161385 val loss 0.19272777438163757\n",
      "Batch 9 Epoch 33: train loss 0.005863565020263195 val loss 0.12888562679290771\n",
      "Batch 10 Epoch 33: train loss 0.0033253072760999203 val loss 0.0816442221403122\n",
      "Batch 11 Epoch 33: train loss 0.0036952572409063578 val loss 0.047061413526535034\n",
      "Batch 12 Epoch 33: train loss 0.011418807320296764 val loss 0.043836288154125214\n",
      "Batch 13 Epoch 33: train loss 0.05202852189540863 val loss 0.043933238834142685\n",
      "Batch 14 Epoch 33: train loss 0.02897314541041851 val loss 0.04614844173192978\n",
      "Batch 15 Epoch 33: train loss 0.028430165722966194 val loss 0.06612511724233627\n",
      "Batch 16 Epoch 33: train loss 0.03773646429181099 val loss 0.10290612280368805\n",
      "Batch 17 Epoch 33: train loss 0.011422104202210903 val loss 0.1532423198223114\n",
      "Batch 18 Epoch 33: train loss 0.003389383666217327 val loss 0.21247385442256927\n",
      "Batch 19 Epoch 33: train loss 0.000568675110116601 val loss 0.2708008885383606\n",
      "Batch 20 Epoch 33: train loss 0.0034280833788216114 val loss 0.3206523358821869\n",
      "Batch 21 Epoch 33: train loss 0.008456208743155003 val loss 0.35507792234420776\n",
      "Batch 22 Epoch 33: train loss 0.015539919026196003 val loss 0.3687466084957123\n",
      "Batch 23 Epoch 33: train loss 0.013731013983488083 val loss 0.3637290298938751\n",
      "Batch 24 Epoch 33: train loss 0.01503235287964344 val loss 0.34140104055404663\n",
      "Batch 25 Epoch 33: train loss 0.01175126526504755 val loss 0.3068726062774658\n",
      "Batch 26 Epoch 33: train loss 0.004691313486546278 val loss 0.27101245522499084\n",
      "Batch 27 Epoch 33: train loss 0.005314599256962538 val loss 0.23328620195388794\n",
      "Batch 28 Epoch 33: train loss 0.00039419857785105705 val loss 0.2021137773990631\n",
      "Batch 29 Epoch 33: train loss 0.0016385617200285196 val loss 0.1792832463979721\n",
      "Batch 30 Epoch 33: train loss 0.004930203314870596 val loss 0.16526290774345398\n",
      "Batch 31 Epoch 33: train loss 0.006198237184435129 val loss 0.15957529842853546\n",
      "Batch 32 Epoch 33: train loss 0.022738903760910034 val loss 0.1675219088792801\n",
      "Batch 33 Epoch 33: train loss 0.1374114751815796 val loss 0.20114286243915558\n",
      "Batch 34 Epoch 33: train loss 0.027220549061894417 val loss 0.23833045363426208\n",
      "Batch 35 Epoch 33: train loss 0.018285097554326057 val loss 0.27413466572761536\n",
      "Batch 36 Epoch 33: train loss 0.029212607070803642 val loss 0.31760483980178833\n",
      "Batch 37 Epoch 33: train loss 0.13859976828098297 val loss 0.3720535337924957\n",
      "Batch 38 Epoch 33: train loss 0.11489024758338928 val loss 0.4108445644378662\n",
      "Batch 39 Epoch 33: train loss 0.06951048970222473 val loss 0.4271283447742462\n",
      "Batch 40 Epoch 33: train loss 0.03414015471935272 val loss 0.42641544342041016\n",
      "Batch 41 Epoch 33: train loss 0.022150635719299316 val loss 0.4148902893066406\n",
      "Batch 42 Epoch 33: train loss 0.31811752915382385 val loss 0.37581944465637207\n",
      "Batch 43 Epoch 33: train loss 0.1095847487449646 val loss 0.2935156226158142\n",
      "Batch 44 Epoch 33: train loss 0.14378079771995544 val loss 0.039542779326438904\n",
      "Batch 45 Epoch 33: train loss 0.13743190467357635 val loss 0.019078506156802177\n",
      "Batch 46 Epoch 33: train loss 0.04135894775390625 val loss 0.010017366148531437\n",
      "Batch 47 Epoch 33: train loss 0.05268825963139534 val loss 0.00994813535362482\n",
      "Batch 48 Epoch 33: train loss 0.01024115551263094 val loss 0.013850009068846703\n",
      "Batch 49 Epoch 33: train loss 0.0029280357994139194 val loss 0.020194262266159058\n",
      "Batch 0 Epoch 34: train loss 0.070874884724617 val loss 0.02999008074402809\n",
      "Batch 1 Epoch 34: train loss 0.045955590903759 val loss 0.043221890926361084\n",
      "Batch 2 Epoch 34: train loss 0.035988543182611465 val loss 0.8671396374702454\n",
      "Batch 3 Epoch 34: train loss 0.008569998666644096 val loss 1.1947369575500488\n",
      "Batch 4 Epoch 34: train loss 0.02436988055706024 val loss 1.586896538734436\n",
      "Batch 5 Epoch 34: train loss 0.01987209916114807 val loss 2.050983428955078\n",
      "Batch 6 Epoch 34: train loss 0.006775330286473036 val loss 2.5789101123809814\n",
      "Batch 7 Epoch 34: train loss 0.002093092305585742 val loss 3.1031363010406494\n",
      "Batch 8 Epoch 34: train loss 0.013027405366301537 val loss 3.5269532203674316\n",
      "Batch 9 Epoch 34: train loss 0.015120474621653557 val loss 3.8161673545837402\n",
      "Batch 10 Epoch 34: train loss 0.020076947286725044 val loss 3.952383041381836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11 Epoch 34: train loss 0.0244744922965765 val loss 3.9119532108306885\n",
      "Batch 12 Epoch 34: train loss 0.01506128255277872 val loss 3.7533185482025146\n",
      "Batch 13 Epoch 34: train loss 0.03009404055774212 val loss 3.4874534606933594\n",
      "Batch 14 Epoch 34: train loss 0.002154064830392599 val loss 3.2230284214019775\n",
      "Batch 15 Epoch 34: train loss 0.001203796244226396 val loss 2.992198944091797\n",
      "Batch 16 Epoch 34: train loss 0.02027491107583046 val loss 2.8828792572021484\n",
      "Batch 17 Epoch 34: train loss 0.005108588840812445 val loss 2.829087734222412\n",
      "Batch 18 Epoch 34: train loss 0.00534840440377593 val loss 2.832043170928955\n",
      "Batch 19 Epoch 34: train loss 0.005914486479014158 val loss 2.8906397819519043\n",
      "Batch 20 Epoch 34: train loss 0.004878337495028973 val loss 2.994316339492798\n",
      "Batch 21 Epoch 34: train loss 0.005616903305053711 val loss 3.1466915607452393\n",
      "Batch 22 Epoch 34: train loss 0.004035354126244783 val loss 3.3400473594665527\n",
      "Batch 23 Epoch 34: train loss 0.0007708746707066894 val loss 3.5331978797912598\n",
      "Batch 24 Epoch 34: train loss 0.0004618281382136047 val loss 3.7211267948150635\n",
      "Batch 25 Epoch 34: train loss 0.0006884675240144134 val loss 3.8782527446746826\n",
      "Batch 26 Epoch 34: train loss 0.007234387099742889 val loss 3.947878122329712\n",
      "Batch 27 Epoch 34: train loss 0.0022998421918600798 val loss 3.9955828189849854\n",
      "Batch 28 Epoch 34: train loss 0.0045576393604278564 val loss 3.973628044128418\n",
      "Batch 29 Epoch 34: train loss 0.0027032827492803335 val loss 3.911677837371826\n",
      "Batch 30 Epoch 34: train loss 0.0030234933365136385 val loss 3.8258423805236816\n",
      "Batch 31 Epoch 34: train loss 0.001440639141947031 val loss 3.737797737121582\n",
      "Batch 32 Epoch 34: train loss 0.004860950633883476 val loss 3.6087586879730225\n",
      "Batch 33 Epoch 34: train loss 0.047334738075733185 val loss 3.384026288986206\n",
      "Batch 34 Epoch 34: train loss 0.022397112101316452 val loss 3.201115369796753\n",
      "Batch 35 Epoch 34: train loss 0.09641355276107788 val loss 3.047574281692505\n",
      "Batch 36 Epoch 34: train loss 0.010982799343764782 val loss 2.912304639816284\n",
      "Batch 37 Epoch 34: train loss 0.045079153031110764 val loss 2.7464351654052734\n",
      "Batch 38 Epoch 34: train loss 0.05500221252441406 val loss 2.513397693634033\n",
      "Batch 39 Epoch 34: train loss 0.06695232540369034 val loss 2.1545250415802\n",
      "Batch 40 Epoch 34: train loss 0.10242953896522522 val loss 1.6580548286437988\n",
      "Batch 41 Epoch 34: train loss 0.04734155163168907 val loss 0.04829484224319458\n",
      "Batch 42 Epoch 34: train loss 0.21087832748889923 val loss 0.029656514525413513\n",
      "Batch 43 Epoch 34: train loss 0.045670296996831894 val loss 0.015971653163433075\n",
      "Batch 44 Epoch 34: train loss 0.023774700239300728 val loss 0.009745635092258453\n",
      "Batch 45 Epoch 34: train loss 0.003097990993410349 val loss 0.01032400131225586\n",
      "Batch 46 Epoch 34: train loss 0.07322745770215988 val loss 0.019161012023687363\n",
      "Batch 47 Epoch 34: train loss 0.017905110493302345 val loss 0.03690619394183159\n",
      "Batch 48 Epoch 34: train loss 0.04049793258309364 val loss 0.2000022679567337\n",
      "Batch 49 Epoch 34: train loss 0.05905767157673836 val loss 0.32251065969467163\n",
      "Batch 0 Epoch 35: train loss 0.09024733304977417 val loss 0.4107000231742859\n",
      "Batch 1 Epoch 35: train loss 0.06809493154287338 val loss 0.46251100301742554\n",
      "Batch 2 Epoch 35: train loss 0.06891434639692307 val loss 0.47429412603378296\n",
      "Batch 3 Epoch 35: train loss 0.017855726182460785 val loss 0.46882325410842896\n",
      "Batch 4 Epoch 35: train loss 0.02818695269525051 val loss 0.44799530506134033\n",
      "Batch 5 Epoch 35: train loss 0.04426299035549164 val loss 0.4070000946521759\n",
      "Batch 6 Epoch 35: train loss 0.05044716224074364 val loss 0.34408116340637207\n",
      "Batch 7 Epoch 35: train loss 0.03533975034952164 val loss 0.2683289051055908\n",
      "Batch 8 Epoch 35: train loss 0.0137657904997468 val loss 0.19499310851097107\n",
      "Batch 9 Epoch 35: train loss 0.005292113870382309 val loss 0.13207735121250153\n",
      "Batch 10 Epoch 35: train loss 0.0030698233749717474 val loss 0.085157111287117\n",
      "Batch 11 Epoch 35: train loss 0.0044717565178871155 val loss 0.04540430009365082\n",
      "Batch 12 Epoch 35: train loss 0.011866627261042595 val loss 0.04260077327489853\n",
      "Batch 13 Epoch 35: train loss 0.050837982445955276 val loss 0.04273576661944389\n",
      "Batch 14 Epoch 35: train loss 0.028314458206295967 val loss 0.04474847391247749\n",
      "Batch 15 Epoch 35: train loss 0.027271604165434837 val loss 0.0493028350174427\n",
      "Batch 16 Epoch 35: train loss 0.033562254160642624 val loss 0.10647155344486237\n",
      "Batch 17 Epoch 35: train loss 0.011446237564086914 val loss 0.15444733202457428\n",
      "Batch 18 Epoch 35: train loss 0.0030970978550612926 val loss 0.2102336436510086\n",
      "Batch 19 Epoch 35: train loss 0.000632193055935204 val loss 0.26468703150749207\n",
      "Batch 20 Epoch 35: train loss 0.0031302766874432564 val loss 0.311242938041687\n",
      "Batch 21 Epoch 35: train loss 0.008173189125955105 val loss 0.34319010376930237\n",
      "Batch 22 Epoch 35: train loss 0.015468688681721687 val loss 0.35522058606147766\n",
      "Batch 23 Epoch 35: train loss 0.013493944890797138 val loss 0.34942078590393066\n",
      "Batch 24 Epoch 35: train loss 0.013939982280135155 val loss 0.32769322395324707\n",
      "Batch 25 Epoch 35: train loss 0.01124450284987688 val loss 0.2946688234806061\n",
      "Batch 26 Epoch 35: train loss 0.004472194705158472 val loss 0.2603224217891693\n",
      "Batch 27 Epoch 35: train loss 0.005159873981028795 val loss 0.2241893708705902\n",
      "Batch 28 Epoch 35: train loss 0.0004907147376798093 val loss 0.19486664235591888\n",
      "Batch 29 Epoch 35: train loss 0.0015734194312244654 val loss 0.17334142327308655\n",
      "Batch 30 Epoch 35: train loss 0.0050207641907036304 val loss 0.16026490926742554\n",
      "Batch 31 Epoch 35: train loss 0.00561472587287426 val loss 0.1547776758670807\n",
      "Batch 32 Epoch 35: train loss 0.021905679255723953 val loss 0.16216103732585907\n",
      "Batch 33 Epoch 35: train loss 0.1302473396062851 val loss 0.1935068666934967\n",
      "Batch 34 Epoch 35: train loss 0.024545595049858093 val loss 0.2278100997209549\n",
      "Batch 35 Epoch 35: train loss 0.018208546563982964 val loss 0.2606336772441864\n",
      "Batch 36 Epoch 35: train loss 0.028459457680583 val loss 0.30050230026245117\n",
      "Batch 37 Epoch 35: train loss 0.1301894187927246 val loss 0.35028937458992004\n",
      "Batch 38 Epoch 35: train loss 0.10550868511199951 val loss 0.38283878564834595\n",
      "Batch 39 Epoch 35: train loss 0.0649707093834877 val loss 0.39345014095306396\n",
      "Batch 40 Epoch 35: train loss 0.029679974541068077 val loss 0.388691782951355\n",
      "Batch 41 Epoch 35: train loss 0.019390881061553955 val loss 0.3739559054374695\n",
      "Batch 42 Epoch 35: train loss 0.3080008029937744 val loss 0.33339619636535645\n",
      "Batch 43 Epoch 35: train loss 0.09411077201366425 val loss 0.25362998247146606\n",
      "Batch 44 Epoch 35: train loss 0.1194462776184082 val loss 0.03379429876804352\n",
      "Batch 45 Epoch 35: train loss 0.11470799893140793 val loss 0.01596107892692089\n",
      "Batch 46 Epoch 35: train loss 0.040822096168994904 val loss 0.00950622372329235\n",
      "Batch 47 Epoch 35: train loss 0.057651884853839874 val loss 0.010433665476739407\n",
      "Batch 48 Epoch 35: train loss 0.010507873259484768 val loss 0.014234385453164577\n",
      "Batch 49 Epoch 35: train loss 0.0032790217082947493 val loss 0.019520830363035202\n",
      "Batch 0 Epoch 36: train loss 0.07930953800678253 val loss 0.02747958153486252\n",
      "Batch 1 Epoch 36: train loss 0.05161464214324951 val loss 0.03815076872706413\n",
      "Batch 2 Epoch 36: train loss 0.04038986191153526 val loss 0.04824153706431389\n",
      "Batch 3 Epoch 36: train loss 0.009665071964263916 val loss 1.0277951955795288\n",
      "Batch 4 Epoch 36: train loss 0.02557164616882801 val loss 1.3785374164581299\n",
      "Batch 5 Epoch 36: train loss 0.019849618896842003 val loss 1.7984375953674316\n",
      "Batch 6 Epoch 36: train loss 0.005395704414695501 val loss 2.2751481533050537\n",
      "Batch 7 Epoch 36: train loss 0.00276528880931437 val loss 2.7378294467926025\n",
      "Batch 8 Epoch 36: train loss 0.015365704894065857 val loss 3.0878615379333496\n",
      "Batch 9 Epoch 36: train loss 0.018795529380440712 val loss 3.2893152236938477\n",
      "Batch 10 Epoch 36: train loss 0.021963126957416534 val loss 3.3360486030578613\n",
      "Batch 11 Epoch 36: train loss 0.026491139084100723 val loss 3.212181806564331\n",
      "Batch 12 Epoch 36: train loss 0.014418492093682289 val loss 2.9925601482391357\n",
      "Batch 13 Epoch 36: train loss 0.027737149968743324 val loss 2.697237253189087\n",
      "Batch 14 Epoch 36: train loss 0.0012670910218730569 val loss 2.4274797439575195\n",
      "Batch 15 Epoch 36: train loss 0.0010372825199738145 val loss 2.2043187618255615\n",
      "Batch 16 Epoch 36: train loss 0.03044126182794571 val loss 2.1051928997039795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 17 Epoch 36: train loss 0.007588095963001251 val loss 2.064178466796875\n",
      "Batch 18 Epoch 36: train loss 0.007593465968966484 val loss 2.0802080631256104\n",
      "Batch 19 Epoch 36: train loss 0.0066307769156992435 val loss 2.1459364891052246\n",
      "Batch 20 Epoch 36: train loss 0.004918839782476425 val loss 2.249443292617798\n",
      "Batch 21 Epoch 36: train loss 0.005552422720938921 val loss 2.3943469524383545\n",
      "Batch 22 Epoch 36: train loss 0.003504898864775896 val loss 2.572643518447876\n",
      "Batch 23 Epoch 36: train loss 0.0007991780294105411 val loss 2.7441647052764893\n",
      "Batch 24 Epoch 36: train loss 0.00046406700857914984 val loss 2.9016807079315186\n",
      "Batch 25 Epoch 36: train loss 0.0011469250312075019 val loss 3.0240964889526367\n",
      "Batch 26 Epoch 36: train loss 0.0089750736951828 val loss 3.060513496398926\n",
      "Batch 27 Epoch 36: train loss 0.0023570202756673098 val loss 3.0746538639068604\n",
      "Batch 28 Epoch 36: train loss 0.005484560504555702 val loss 3.0241611003875732\n",
      "Batch 29 Epoch 36: train loss 0.003172581782564521 val loss 2.938171863555908\n",
      "Batch 30 Epoch 36: train loss 0.003203839063644409 val loss 2.8338279724121094\n",
      "Batch 31 Epoch 36: train loss 0.0016109098214656115 val loss 2.732011318206787\n",
      "Batch 32 Epoch 36: train loss 0.004964013583958149 val loss 2.5994789600372314\n",
      "Batch 33 Epoch 36: train loss 0.04808264225721359 val loss 2.3901214599609375\n",
      "Batch 34 Epoch 36: train loss 0.016781387850642204 val loss 2.220083713531494\n",
      "Batch 35 Epoch 36: train loss 0.08233693987131119 val loss 2.0807456970214844\n",
      "Batch 36 Epoch 36: train loss 0.009692411869764328 val loss 1.9577610492706299\n",
      "Batch 37 Epoch 36: train loss 0.04334097355604172 val loss 1.81523859500885\n",
      "Batch 38 Epoch 36: train loss 0.043178021907806396 val loss 1.6399736404418945\n",
      "Batch 39 Epoch 36: train loss 0.04652595520019531 val loss 1.3980919122695923\n",
      "Batch 40 Epoch 36: train loss 0.07383076101541519 val loss 0.04823463782668114\n",
      "Batch 41 Epoch 36: train loss 0.03250386193394661 val loss 0.031501952558755875\n",
      "Batch 42 Epoch 36: train loss 0.2130350023508072 val loss 0.020803742110729218\n",
      "Batch 43 Epoch 36: train loss 0.03947364166378975 val loss 0.012968077324330807\n",
      "Batch 44 Epoch 36: train loss 0.019781161099672318 val loss 0.009500967338681221\n",
      "Batch 45 Epoch 36: train loss 0.0038901318330317736 val loss 0.009744444862008095\n",
      "Batch 46 Epoch 36: train loss 0.07092992216348648 val loss 0.014993651770055294\n",
      "Batch 47 Epoch 36: train loss 0.021647389978170395 val loss 0.02684026025235653\n",
      "Batch 48 Epoch 36: train loss 0.030297307297587395 val loss 0.03949793428182602\n",
      "Batch 49 Epoch 36: train loss 0.04114013910293579 val loss 0.04931673780083656\n",
      "Batch 0 Epoch 37: train loss 0.09665719419717789 val loss 0.2875535190105438\n",
      "Batch 1 Epoch 37: train loss 0.07373085618019104 val loss 0.3192221522331238\n",
      "Batch 2 Epoch 37: train loss 0.07311785221099854 val loss 0.3172524571418762\n",
      "Batch 3 Epoch 37: train loss 0.020725788548588753 val loss 0.30112898349761963\n",
      "Batch 4 Epoch 37: train loss 0.032221537083387375 val loss 0.27304431796073914\n",
      "Batch 5 Epoch 37: train loss 0.04849801957607269 val loss 0.2302246391773224\n",
      "Batch 6 Epoch 37: train loss 0.052647046744823456 val loss 0.1738521009683609\n",
      "Batch 7 Epoch 37: train loss 0.03359721228480339 val loss 0.11438936740159988\n",
      "Batch 8 Epoch 37: train loss 0.011601232923567295 val loss 0.041888073086738586\n",
      "Batch 9 Epoch 37: train loss 0.0042226240038871765 val loss 0.04832275211811066\n",
      "Batch 10 Epoch 37: train loss 0.0034154143650084734 val loss 0.04504567012190819\n",
      "Batch 11 Epoch 37: train loss 0.005811762530356646 val loss 0.04298093169927597\n",
      "Batch 12 Epoch 37: train loss 0.012643692083656788 val loss 0.04223066568374634\n",
      "Batch 13 Epoch 37: train loss 0.05066318064928055 val loss 0.04339662566781044\n",
      "Batch 14 Epoch 37: train loss 0.027743572369217873 val loss 0.045767832547426224\n",
      "Batch 15 Epoch 37: train loss 0.025388142094016075 val loss 0.04921789839863777\n",
      "Batch 16 Epoch 37: train loss 0.028026198968291283 val loss 0.03644917905330658\n",
      "Batch 17 Epoch 37: train loss 0.010656735859811306 val loss 0.038058988749980927\n",
      "Batch 18 Epoch 37: train loss 0.003661851165816188 val loss 0.04290400817990303\n",
      "Batch 19 Epoch 37: train loss 0.00043362678843550384 val loss 0.04966191574931145\n",
      "Batch 20 Epoch 37: train loss 0.002305111149325967 val loss 0.13336798548698425\n",
      "Batch 21 Epoch 37: train loss 0.0078085558488965034 val loss 0.15694427490234375\n",
      "Batch 22 Epoch 37: train loss 0.012410062365233898 val loss 0.16791556775569916\n",
      "Batch 23 Epoch 37: train loss 0.011268624104559422 val loss 0.16683842241764069\n",
      "Batch 24 Epoch 37: train loss 0.013212949968874454 val loss 0.1540285348892212\n",
      "Batch 25 Epoch 37: train loss 0.010475332848727703 val loss 0.1332123577594757\n",
      "Batch 26 Epoch 37: train loss 0.004058029502630234 val loss 0.112053282558918\n",
      "Batch 27 Epoch 37: train loss 0.0056100147776305676 val loss 0.046149976551532745\n",
      "Batch 28 Epoch 37: train loss 0.0004892401630058885 val loss 0.042551182210445404\n",
      "Batch 29 Epoch 37: train loss 0.0012714171316474676 val loss 0.04019626975059509\n",
      "Batch 30 Epoch 37: train loss 0.004554341547191143 val loss 0.03888069465756416\n",
      "Batch 31 Epoch 37: train loss 0.0046782344579696655 val loss 0.038289546966552734\n",
      "Batch 32 Epoch 37: train loss 0.018093639984726906 val loss 0.038714587688446045\n",
      "Batch 33 Epoch 37: train loss 0.1155921220779419 val loss 0.041450146585702896\n",
      "Batch 34 Epoch 37: train loss 0.01510559394955635 val loss 0.04494829103350639\n",
      "Batch 35 Epoch 37: train loss 0.018520580604672432 val loss 0.04873068630695343\n",
      "Batch 36 Epoch 37: train loss 0.025005677714943886 val loss 0.13960108160972595\n",
      "Batch 37 Epoch 37: train loss 0.11209096759557724 val loss 0.17642411589622498\n",
      "Batch 38 Epoch 37: train loss 0.0790385976433754 val loss 0.20548169314861298\n",
      "Batch 39 Epoch 37: train loss 0.04794256016612053 val loss 0.2223941832780838\n",
      "Batch 40 Epoch 37: train loss 0.020236028358340263 val loss 0.230518639087677\n",
      "Batch 41 Epoch 37: train loss 0.01367525290697813 val loss 0.23194625973701477\n",
      "Batch 42 Epoch 37: train loss 0.2911326289176941 val loss 0.2155708223581314\n",
      "Batch 43 Epoch 37: train loss 0.0768040344119072 val loss 0.043099015951156616\n",
      "Batch 44 Epoch 37: train loss 0.09799941629171371 val loss 0.030009884387254715\n",
      "Batch 45 Epoch 37: train loss 0.09594389796257019 val loss 0.01629919745028019\n",
      "Batch 46 Epoch 37: train loss 0.03730746731162071 val loss 0.01023496687412262\n",
      "Batch 47 Epoch 37: train loss 0.04651791974902153 val loss 0.009511122480034828\n",
      "Batch 48 Epoch 37: train loss 0.009795294143259525 val loss 0.010820170864462852\n",
      "Batch 49 Epoch 37: train loss 0.001750635216012597 val loss 0.01362842321395874\n",
      "Batch 0 Epoch 38: train loss 0.05390164628624916 val loss 0.018505480140447617\n",
      "Batch 1 Epoch 38: train loss 0.0352260060608387 val loss 0.025532783940434456\n",
      "Batch 2 Epoch 38: train loss 0.027779310941696167 val loss 0.034957971423864365\n",
      "Batch 3 Epoch 38: train loss 0.007045329548418522 val loss 0.045754916965961456\n",
      "Batch 4 Epoch 38: train loss 0.020491749048233032 val loss 0.8519564270973206\n",
      "Batch 5 Epoch 38: train loss 0.017529495060443878 val loss 1.1013541221618652\n",
      "Batch 6 Epoch 38: train loss 0.00775260291993618 val loss 1.389702320098877\n",
      "Batch 7 Epoch 38: train loss 0.0015194290317595005 val loss 1.6837841272354126\n",
      "Batch 8 Epoch 38: train loss 0.0060912007465958595 val loss 1.9401723146438599\n",
      "Batch 9 Epoch 38: train loss 0.007460897788405418 val loss 2.1393227577209473\n",
      "Batch 10 Epoch 38: train loss 0.012898027896881104 val loss 2.266826629638672\n",
      "Batch 11 Epoch 38: train loss 0.017656603828072548 val loss 2.2959935665130615\n",
      "Batch 12 Epoch 38: train loss 0.01089693047106266 val loss 2.257097005844116\n",
      "Batch 13 Epoch 38: train loss 0.030208367854356766 val loss 2.138444662094116\n",
      "Batch 14 Epoch 38: train loss 0.0035804163198918104 val loss 2.003478527069092\n",
      "Batch 15 Epoch 38: train loss 0.0019136866321787238 val loss 1.871740698814392\n",
      "Batch 16 Epoch 38: train loss 0.014947016723453999 val loss 1.809788465499878\n",
      "Batch 17 Epoch 38: train loss 0.0022715458180755377 val loss 1.76942777633667\n",
      "Batch 18 Epoch 38: train loss 0.0019884721841663122 val loss 1.7519892454147339\n",
      "Batch 19 Epoch 38: train loss 0.002101956168189645 val loss 1.7577433586120605\n",
      "Batch 20 Epoch 38: train loss 0.0024766502901911736 val loss 1.7843317985534668\n",
      "Batch 21 Epoch 38: train loss 0.0033035341184586287 val loss 1.836546540260315\n",
      "Batch 22 Epoch 38: train loss 0.0031289979815483093 val loss 1.913516640663147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 23 Epoch 38: train loss 0.0006144045037217438 val loss 1.9921846389770508\n",
      "Batch 24 Epoch 38: train loss 0.0008466403232887387 val loss 2.0765554904937744\n",
      "Batch 25 Epoch 38: train loss 0.0005591430817730725 val loss 2.1568076610565186\n",
      "Batch 26 Epoch 38: train loss 0.004820416681468487 val loss 2.1992218494415283\n",
      "Batch 27 Epoch 38: train loss 0.0020600897260010242 val loss 2.242398977279663\n",
      "Batch 28 Epoch 38: train loss 0.002811216050758958 val loss 2.250805139541626\n",
      "Batch 29 Epoch 38: train loss 0.0017461479874327779 val loss 2.240128755569458\n",
      "Batch 30 Epoch 38: train loss 0.002721601165831089 val loss 2.2136898040771484\n",
      "Batch 31 Epoch 38: train loss 0.0014370674034580588 val loss 2.1846916675567627\n",
      "Batch 32 Epoch 38: train loss 0.005254258867353201 val loss 2.1232872009277344\n",
      "Batch 33 Epoch 38: train loss 0.0482758954167366 val loss 1.994515299797058\n",
      "Batch 34 Epoch 38: train loss 0.017100393772125244 val loss 1.8908551931381226\n",
      "Batch 35 Epoch 38: train loss 0.08071406930685043 val loss 1.8080089092254639\n",
      "Batch 36 Epoch 38: train loss 0.00988256186246872 val loss 1.7308334112167358\n",
      "Batch 37 Epoch 38: train loss 0.04686509072780609 val loss 1.62996244430542\n",
      "Batch 38 Epoch 38: train loss 0.05735991895198822 val loss 1.4778659343719482\n",
      "Batch 39 Epoch 38: train loss 0.05269739404320717 val loss 1.2623226642608643\n",
      "Batch 40 Epoch 38: train loss 0.08029624819755554 val loss 0.9758625626564026\n",
      "Batch 41 Epoch 38: train loss 0.03499029204249382 val loss 0.03639804199337959\n",
      "Batch 42 Epoch 38: train loss 0.21562038362026215 val loss 0.02427010051906109\n",
      "Batch 43 Epoch 38: train loss 0.04340441897511482 val loss 0.01482050959020853\n",
      "Batch 44 Epoch 38: train loss 0.023532969877123833 val loss 0.00993033591657877\n",
      "Batch 45 Epoch 38: train loss 0.003342405194416642 val loss 0.00954368244856596\n",
      "Batch 46 Epoch 38: train loss 0.07291089743375778 val loss 0.014552469365298748\n",
      "Batch 47 Epoch 38: train loss 0.021006235852837563 val loss 0.026395265012979507\n",
      "Batch 48 Epoch 38: train loss 0.03014870174229145 val loss 0.039489470422267914\n",
      "Batch 49 Epoch 38: train loss 0.04255300387740135 val loss 0.27215614914894104\n",
      "Batch 0 Epoch 39: train loss 0.08704278618097305 val loss 0.3473491668701172\n",
      "Batch 1 Epoch 39: train loss 0.06506195664405823 val loss 0.390908420085907\n",
      "Batch 2 Epoch 39: train loss 0.0674259290099144 val loss 0.398896723985672\n",
      "Batch 3 Epoch 39: train loss 0.018861526623368263 val loss 0.3909980356693268\n",
      "Batch 4 Epoch 39: train loss 0.03077440895140171 val loss 0.3685663044452667\n",
      "Batch 5 Epoch 39: train loss 0.045636389404535294 val loss 0.3280538022518158\n",
      "Batch 6 Epoch 39: train loss 0.05215943232178688 val loss 0.26874101161956787\n",
      "Batch 7 Epoch 39: train loss 0.034530896693468094 val loss 0.20051877200603485\n",
      "Batch 8 Epoch 39: train loss 0.013054181821644306 val loss 0.0486072339117527\n",
      "Batch 9 Epoch 39: train loss 0.00590495765209198 val loss 0.03943970054388046\n",
      "Batch 10 Epoch 39: train loss 0.003727806732058525 val loss 0.0468016192317009\n",
      "Batch 11 Epoch 39: train loss 0.003847432089969516 val loss 0.0441109836101532\n",
      "Batch 12 Epoch 39: train loss 0.010056854225695133 val loss 0.042771268635988235\n",
      "Batch 13 Epoch 39: train loss 0.04682355746626854 val loss 0.0434873104095459\n",
      "Batch 14 Epoch 39: train loss 0.0245948676019907 val loss 0.04550420492887497\n",
      "Batch 15 Epoch 39: train loss 0.02497074566781521 val loss 0.04875511676073074\n",
      "Batch 16 Epoch 39: train loss 0.027799824252724648 val loss 0.03496754914522171\n",
      "Batch 17 Epoch 39: train loss 0.011710877530276775 val loss 0.03848553076386452\n",
      "Batch 18 Epoch 39: train loss 0.004907286260277033 val loss 0.04503113776445389\n",
      "Batch 19 Epoch 39: train loss 0.0005680331378243864 val loss 0.15817078948020935\n",
      "Batch 20 Epoch 39: train loss 0.0015449791681021452 val loss 0.19801491498947144\n",
      "Batch 21 Epoch 39: train loss 0.005359271075576544 val loss 0.2291409969329834\n",
      "Batch 22 Epoch 39: train loss 0.009867859072983265 val loss 0.24661143124103546\n",
      "Batch 23 Epoch 39: train loss 0.008911800570786 val loss 0.25085926055908203\n",
      "Batch 24 Epoch 39: train loss 0.01167937833815813 val loss 0.24120742082595825\n",
      "Batch 25 Epoch 39: train loss 0.009575300849974155 val loss 0.2210661768913269\n",
      "Batch 26 Epoch 39: train loss 0.0044293091632425785 val loss 0.19857268035411835\n",
      "Batch 27 Epoch 39: train loss 0.005994398612529039 val loss 0.17240147292613983\n",
      "Batch 28 Epoch 39: train loss 0.0003567418607417494 val loss 0.15033333003520966\n",
      "Batch 29 Epoch 39: train loss 0.0009504514164291322 val loss 0.04758734628558159\n",
      "Batch 30 Epoch 39: train loss 0.0032297843135893345 val loss 0.045327525585889816\n",
      "Batch 31 Epoch 39: train loss 0.0038203224539756775 val loss 0.044160518795251846\n",
      "Batch 32 Epoch 39: train loss 0.01610131561756134 val loss 0.04483829811215401\n",
      "Batch 33 Epoch 39: train loss 0.11107977479696274 val loss 0.049368586391210556\n",
      "Batch 34 Epoch 39: train loss 0.014137334190309048 val loss 0.17360389232635498\n",
      "Batch 35 Epoch 39: train loss 0.01847360096871853 val loss 0.19993242621421814\n",
      "Batch 36 Epoch 39: train loss 0.02411940135061741 val loss 0.23325088620185852\n",
      "Batch 37 Epoch 39: train loss 0.11364838480949402 val loss 0.2787063717842102\n",
      "Batch 38 Epoch 39: train loss 0.08434589207172394 val loss 0.3149716556072235\n",
      "Batch 39 Epoch 39: train loss 0.05128588154911995 val loss 0.3367463946342468\n",
      "Batch 40 Epoch 39: train loss 0.02308845706284046 val loss 0.34729886054992676\n",
      "Batch 41 Epoch 39: train loss 0.01643037609755993 val loss 0.3492807149887085\n",
      "Batch 42 Epoch 39: train loss 0.30211448669433594 val loss 0.3293042778968811\n",
      "Batch 43 Epoch 39: train loss 0.08947806805372238 val loss 0.27010729908943176\n",
      "Batch 44 Epoch 39: train loss 0.11822681874036789 val loss 0.03586442768573761\n",
      "Batch 45 Epoch 39: train loss 0.11685625463724136 val loss 0.018910063430666924\n",
      "Batch 46 Epoch 39: train loss 0.037947557866573334 val loss 0.010580996051430702\n",
      "Batch 47 Epoch 39: train loss 0.0425012968480587 val loss 0.009404391050338745\n",
      "Batch 48 Epoch 39: train loss 0.010103074833750725 val loss 0.011419771239161491\n",
      "Batch 49 Epoch 39: train loss 0.0018058089772239327 val loss 0.01584961824119091\n",
      "Batch 0 Epoch 40: train loss 0.0414094440639019 val loss 0.023232847452163696\n",
      "Batch 1 Epoch 40: train loss 0.025407230481505394 val loss 0.03345939889550209\n",
      "Batch 2 Epoch 40: train loss 0.017408637329936028 val loss 0.0465535931289196\n",
      "Batch 3 Epoch 40: train loss 0.006734567228704691 val loss 0.8131372928619385\n",
      "Batch 4 Epoch 40: train loss 0.015445918776094913 val loss 1.0671913623809814\n",
      "Batch 5 Epoch 40: train loss 0.012139616534113884 val loss 1.3596925735473633\n",
      "Batch 6 Epoch 40: train loss 0.0038862871006131172 val loss 1.6846588850021362\n",
      "Batch 7 Epoch 40: train loss 0.0025850427336990833 val loss 1.9963765144348145\n",
      "Batch 8 Epoch 40: train loss 0.009693492203950882 val loss 2.244513511657715\n",
      "Batch 9 Epoch 40: train loss 0.009305989369750023 val loss 2.415754556655884\n",
      "Batch 10 Epoch 40: train loss 0.012266325764358044 val loss 2.507675886154175\n",
      "Batch 11 Epoch 40: train loss 0.014062282629311085 val loss 2.503194570541382\n",
      "Batch 12 Epoch 40: train loss 0.008214334957301617 val loss 2.4372706413269043\n",
      "Batch 13 Epoch 40: train loss 0.022046595811843872 val loss 2.3085169792175293\n",
      "Batch 14 Epoch 40: train loss 0.0010470367269590497 val loss 2.1800858974456787\n",
      "Batch 15 Epoch 40: train loss 0.0012584894429892302 val loss 2.0716774463653564\n",
      "Batch 16 Epoch 40: train loss 0.02228180319070816 val loss 2.0497825145721436\n",
      "Batch 17 Epoch 40: train loss 0.0039344546385109425 val loss 2.059870481491089\n",
      "Batch 18 Epoch 40: train loss 0.0035344474017620087 val loss 2.1023852825164795\n",
      "Batch 19 Epoch 40: train loss 0.002974849194288254 val loss 2.1733503341674805\n",
      "Batch 20 Epoch 40: train loss 0.0023967926390469074 val loss 2.263227701187134\n",
      "Batch 21 Epoch 40: train loss 0.0025049708783626556 val loss 2.373704671859741\n",
      "Batch 22 Epoch 40: train loss 0.001778454752638936 val loss 2.5018584728240967\n",
      "Batch 23 Epoch 40: train loss 0.0007686173776164651 val loss 2.615633249282837\n",
      "Batch 24 Epoch 40: train loss 0.0005125018651597202 val loss 2.718508005142212\n",
      "Batch 25 Epoch 40: train loss 0.0009207577095367014 val loss 2.797250986099243\n",
      "Batch 26 Epoch 40: train loss 0.0069262441247701645 val loss 2.81074857711792\n",
      "Batch 27 Epoch 40: train loss 0.0018505002371966839 val loss 2.8193907737731934\n",
      "Batch 28 Epoch 40: train loss 0.0029924630653113127 val loss 2.7871768474578857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 29 Epoch 40: train loss 0.0017370922723785043 val loss 2.736612319946289\n",
      "Batch 30 Epoch 40: train loss 0.0024818938691169024 val loss 2.6766750812530518\n",
      "Batch 31 Epoch 40: train loss 0.0014812933513894677 val loss 2.6247878074645996\n",
      "Batch 32 Epoch 40: train loss 0.0034258710220456123 val loss 2.5463411808013916\n",
      "Batch 33 Epoch 40: train loss 0.04075808450579643 val loss 2.399125576019287\n",
      "Batch 34 Epoch 40: train loss 0.025494951754808426 val loss 2.2792413234710693\n",
      "Batch 35 Epoch 40: train loss 0.0966014564037323 val loss 2.177506685256958\n",
      "Batch 36 Epoch 40: train loss 0.010383665561676025 val loss 2.0884153842926025\n",
      "Batch 37 Epoch 40: train loss 0.043498143553733826 val loss 1.9725677967071533\n",
      "Batch 38 Epoch 40: train loss 0.05656489357352257 val loss 1.7954031229019165\n",
      "Batch 39 Epoch 40: train loss 0.06326398998498917 val loss 1.5225765705108643\n",
      "Batch 40 Epoch 40: train loss 0.09625327587127686 val loss 1.1501713991165161\n",
      "Batch 41 Epoch 40: train loss 0.041796281933784485 val loss 0.04218771681189537\n",
      "Batch 42 Epoch 40: train loss 0.21152469515800476 val loss 0.026202501729130745\n",
      "Batch 43 Epoch 40: train loss 0.042945798486471176 val loss 0.014697451144456863\n",
      "Batch 44 Epoch 40: train loss 0.02119770646095276 val loss 0.00954450573772192\n",
      "Batch 45 Epoch 40: train loss 0.0033162955660372972 val loss 0.009868592023849487\n",
      "Batch 46 Epoch 40: train loss 0.06749719381332397 val loss 0.01644003391265869\n",
      "Batch 47 Epoch 40: train loss 0.017515258863568306 val loss 0.029903866350650787\n",
      "Batch 48 Epoch 40: train loss 0.03402555733919144 val loss 0.043899450451135635\n",
      "Batch 49 Epoch 40: train loss 0.0474604032933712 val loss 0.2732873558998108\n",
      "Batch 0 Epoch 41: train loss 0.0788787454366684 val loss 0.3413565754890442\n",
      "Batch 1 Epoch 41: train loss 0.057172778993844986 val loss 0.37838488817214966\n",
      "Batch 2 Epoch 41: train loss 0.05758984386920929 val loss 0.3818749189376831\n",
      "Batch 3 Epoch 41: train loss 0.01499190740287304 val loss 0.37175509333610535\n",
      "Batch 4 Epoch 41: train loss 0.02478857710957527 val loss 0.3489117920398712\n",
      "Batch 5 Epoch 41: train loss 0.03901249170303345 val loss 0.30951735377311707\n",
      "Batch 6 Epoch 41: train loss 0.04319828003644943 val loss 0.25338008999824524\n",
      "Batch 7 Epoch 41: train loss 0.02771882899105549 val loss 0.18970099091529846\n",
      "Batch 8 Epoch 41: train loss 0.009493282064795494 val loss 0.13167648017406464\n",
      "Batch 9 Epoch 41: train loss 0.0032639731653034687 val loss 0.04216465726494789\n",
      "Batch 10 Epoch 41: train loss 0.0031729964539408684 val loss 0.037093013525009155\n",
      "Batch 11 Epoch 41: train loss 0.004942640662193298 val loss 0.03532099351286888\n",
      "Batch 12 Epoch 41: train loss 0.010752465575933456 val loss 0.035109005868434906\n",
      "Batch 13 Epoch 41: train loss 0.04514896124601364 val loss 0.03507944196462631\n",
      "Batch 14 Epoch 41: train loss 0.021904969587922096 val loss 0.03515392541885376\n",
      "Batch 15 Epoch 41: train loss 0.020902462303638458 val loss 0.03617217764258385\n",
      "Batch 16 Epoch 41: train loss 0.023933010175824165 val loss 0.03936118632555008\n",
      "Batch 17 Epoch 41: train loss 0.009202910587191582 val loss 0.04534075781702995\n",
      "Batch 18 Epoch 41: train loss 0.003006404498592019 val loss 0.14218276739120483\n",
      "Batch 19 Epoch 41: train loss 0.0006146467057988048 val loss 0.1835332065820694\n",
      "Batch 20 Epoch 41: train loss 0.0021051194053143263 val loss 0.22026708722114563\n",
      "Batch 21 Epoch 41: train loss 0.006469521205872297 val loss 0.24621035158634186\n",
      "Batch 22 Epoch 41: train loss 0.011563941836357117 val loss 0.2568858563899994\n",
      "Batch 23 Epoch 41: train loss 0.009870338253676891 val loss 0.2540092468261719\n",
      "Batch 24 Epoch 41: train loss 0.010739030316472054 val loss 0.2383345514535904\n",
      "Batch 25 Epoch 41: train loss 0.008219030685722828 val loss 0.21385739743709564\n",
      "Batch 26 Epoch 41: train loss 0.0036060591228306293 val loss 0.18920598924160004\n",
      "Batch 27 Epoch 41: train loss 0.004362683743238449 val loss 0.1628287136554718\n",
      "Batch 28 Epoch 41: train loss 0.00039847707375884056 val loss 0.14131952822208405\n",
      "Batch 29 Epoch 41: train loss 0.0013776551932096481 val loss 0.048907868564128876\n",
      "Batch 30 Epoch 41: train loss 0.004481181968003511 val loss 0.046942975372076035\n",
      "Batch 31 Epoch 41: train loss 0.00469109695404768 val loss 0.04616929963231087\n",
      "Batch 32 Epoch 41: train loss 0.01791379600763321 val loss 0.04741038754582405\n",
      "Batch 33 Epoch 41: train loss 0.11304327100515366 val loss 0.1485835313796997\n",
      "Batch 34 Epoch 41: train loss 0.015626249834895134 val loss 0.1799142062664032\n",
      "Batch 35 Epoch 41: train loss 0.018272656947374344 val loss 0.20994135737419128\n",
      "Batch 36 Epoch 41: train loss 0.023451194167137146 val loss 0.24700237810611725\n",
      "Batch 37 Epoch 41: train loss 0.11167661100625992 val loss 0.2957334816455841\n",
      "Batch 38 Epoch 41: train loss 0.08678804337978363 val loss 0.333229124546051\n",
      "Batch 39 Epoch 41: train loss 0.052656110376119614 val loss 0.35477447509765625\n",
      "Batch 40 Epoch 41: train loss 0.02269277349114418 val loss 0.3644244968891144\n",
      "Batch 41 Epoch 41: train loss 0.01514048408716917 val loss 0.3652389645576477\n",
      "Batch 42 Epoch 41: train loss 0.2962925136089325 val loss 0.3424583971500397\n",
      "Batch 43 Epoch 41: train loss 0.0879625678062439 val loss 0.04749029129743576\n",
      "Batch 44 Epoch 41: train loss 0.11058130115270615 val loss 0.03138735517859459\n",
      "Batch 45 Epoch 41: train loss 0.10807634890079498 val loss 0.015073159709572792\n",
      "Batch 46 Epoch 41: train loss 0.035967662930488586 val loss 0.009322909638285637\n",
      "Batch 47 Epoch 41: train loss 0.05304580554366112 val loss 0.01038715336471796\n",
      "Batch 48 Epoch 41: train loss 0.010494254529476166 val loss 0.014084943570196629\n",
      "Batch 49 Epoch 41: train loss 0.003643268486484885 val loss 0.019037233665585518\n",
      "Batch 0 Epoch 42: train loss 0.061728570610284805 val loss 0.026474809274077415\n",
      "Batch 1 Epoch 42: train loss 0.039357420057058334 val loss 0.036370355635881424\n",
      "Batch 2 Epoch 42: train loss 0.029864907264709473 val loss 0.04901905730366707\n",
      "Batch 3 Epoch 42: train loss 0.007680737879127264 val loss 0.6902730464935303\n",
      "Batch 4 Epoch 42: train loss 0.02081933431327343 val loss 0.9390518069267273\n",
      "Batch 5 Epoch 42: train loss 0.014694473706185818 val loss 1.2346739768981934\n",
      "Batch 6 Epoch 42: train loss 0.003986694384366274 val loss 1.5689682960510254\n",
      "Batch 7 Epoch 42: train loss 0.0036556946579366922 val loss 1.8827608823776245\n",
      "Batch 8 Epoch 42: train loss 0.01432342454791069 val loss 2.1082053184509277\n",
      "Batch 9 Epoch 42: train loss 0.015758538618683815 val loss 2.2251970767974854\n",
      "Batch 10 Epoch 42: train loss 0.01855417713522911 val loss 2.2347841262817383\n",
      "Batch 11 Epoch 42: train loss 0.021471476182341576 val loss 2.1271839141845703\n",
      "Batch 12 Epoch 42: train loss 0.010127127170562744 val loss 1.9622737169265747\n",
      "Batch 13 Epoch 42: train loss 0.022923668846488 val loss 1.7518560886383057\n",
      "Batch 14 Epoch 42: train loss 0.0008931615739129484 val loss 1.5645438432693481\n",
      "Batch 15 Epoch 42: train loss 0.0014741953928023577 val loss 1.4160058498382568\n",
      "Batch 16 Epoch 42: train loss 0.02871912717819214 val loss 1.358068823814392\n",
      "Batch 17 Epoch 42: train loss 0.007266467437148094 val loss 1.3424359560012817\n",
      "Batch 18 Epoch 42: train loss 0.007053552661091089 val loss 1.3690135478973389\n",
      "Batch 19 Epoch 42: train loss 0.006443709135055542 val loss 1.4341740608215332\n",
      "Batch 20 Epoch 42: train loss 0.003981216810643673 val loss 1.5254515409469604\n",
      "Batch 21 Epoch 42: train loss 0.00399596244096756 val loss 1.6438696384429932\n",
      "Batch 22 Epoch 42: train loss 0.002398183336481452 val loss 1.7828878164291382\n",
      "Batch 23 Epoch 42: train loss 0.000734877132344991 val loss 1.9072247743606567\n",
      "Batch 24 Epoch 42: train loss 0.0006693690083920956 val loss 2.01125168800354\n",
      "Batch 25 Epoch 42: train loss 0.001458953251130879 val loss 2.0834908485412598\n",
      "Batch 26 Epoch 42: train loss 0.01007883995771408 val loss 2.0824062824249268\n",
      "Batch 27 Epoch 42: train loss 0.002349988790228963 val loss 2.0662832260131836\n",
      "Batch 28 Epoch 42: train loss 0.004576487932354212 val loss 2.0059287548065186\n",
      "Batch 29 Epoch 42: train loss 0.0023940291721373796 val loss 1.9263057708740234\n",
      "Batch 30 Epoch 42: train loss 0.0030308114364743233 val loss 1.8371726274490356\n",
      "Batch 31 Epoch 42: train loss 0.0015338972443714738 val loss 1.7560592889785767\n",
      "Batch 32 Epoch 42: train loss 0.0037946212105453014 val loss 1.6560574769973755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 33 Epoch 42: train loss 0.044497545808553696 val loss 1.5009766817092896\n",
      "Batch 34 Epoch 42: train loss 0.017288407310843468 val loss 1.3759641647338867\n",
      "Batch 35 Epoch 42: train loss 0.0817766934633255 val loss 1.2726142406463623\n",
      "Batch 36 Epoch 42: train loss 0.008737405762076378 val loss 1.1826165914535522\n",
      "Batch 37 Epoch 42: train loss 0.03996967524290085 val loss 1.079032301902771\n",
      "Batch 38 Epoch 42: train loss 0.0405036024749279 val loss 0.9511550068855286\n",
      "Batch 39 Epoch 42: train loss 0.0416647270321846 val loss 0.7801806926727295\n",
      "Batch 40 Epoch 42: train loss 0.06575188040733337 val loss 0.037920933216810226\n",
      "Batch 41 Epoch 42: train loss 0.026784546673297882 val loss 0.02413671463727951\n",
      "Batch 42 Epoch 42: train loss 0.21252070367336273 val loss 0.01598202809691429\n",
      "Batch 43 Epoch 42: train loss 0.032366055995225906 val loss 0.010790679603815079\n",
      "Batch 44 Epoch 42: train loss 0.016513798385858536 val loss 0.009183636866509914\n",
      "Batch 45 Epoch 42: train loss 0.006858489941805601 val loss 0.009812677279114723\n",
      "Batch 46 Epoch 42: train loss 0.060467518866062164 val loss 0.01386726088821888\n",
      "Batch 47 Epoch 42: train loss 0.0191455390304327 val loss 0.0227627232670784\n",
      "Batch 48 Epoch 42: train loss 0.027420418336987495 val loss 0.03182630613446236\n",
      "Batch 49 Epoch 42: train loss 0.030359024181962013 val loss 0.03840337693691254\n",
      "Batch 0 Epoch 43: train loss 0.08115127682685852 val loss 0.04256269335746765\n",
      "Batch 1 Epoch 43: train loss 0.057571981102228165 val loss 0.04474372789263725\n",
      "Batch 2 Epoch 43: train loss 0.05554739758372307 val loss 0.044900473207235336\n",
      "Batch 3 Epoch 43: train loss 0.014354736544191837 val loss 0.044365592300891876\n",
      "Batch 4 Epoch 43: train loss 0.023593775928020477 val loss 0.04326443001627922\n",
      "Batch 5 Epoch 43: train loss 0.032678987830877304 val loss 0.041368890553712845\n",
      "Batch 6 Epoch 43: train loss 0.03456153720617294 val loss 0.038456521928310394\n",
      "Batch 7 Epoch 43: train loss 0.01777661219239235 val loss 0.03494538739323616\n",
      "Batch 8 Epoch 43: train loss 0.0044775535352528095 val loss 0.031590964645147324\n",
      "Batch 9 Epoch 43: train loss 0.0008384521352127194 val loss 0.02868630364537239\n",
      "Batch 10 Epoch 43: train loss 0.004107078071683645 val loss 0.02653275988996029\n",
      "Batch 11 Epoch 43: train loss 0.009329198859632015 val loss 0.025441879406571388\n",
      "Batch 12 Epoch 43: train loss 0.015609405003488064 val loss 0.025432290509343147\n",
      "Batch 13 Epoch 43: train loss 0.04707059636712074 val loss 0.026908235624432564\n",
      "Batch 14 Epoch 43: train loss 0.021617723628878593 val loss 0.029294246807694435\n",
      "Batch 15 Epoch 43: train loss 0.016010284423828125 val loss 0.03245406970381737\n",
      "Batch 16 Epoch 43: train loss 0.018694620579481125 val loss 0.03661800175905228\n",
      "Batch 17 Epoch 43: train loss 0.004640728235244751 val loss 0.04116850718855858\n",
      "Batch 18 Epoch 43: train loss 0.0008527645841240883 val loss 0.04566657543182373\n",
      "Batch 19 Epoch 43: train loss 0.0021481753792613745 val loss 0.04947561025619507\n",
      "Batch 20 Epoch 43: train loss 0.005538425408303738 val loss 0.13991926610469818\n",
      "Batch 21 Epoch 43: train loss 0.010767409577965736 val loss 0.15153548121452332\n",
      "Batch 22 Epoch 43: train loss 0.015080709010362625 val loss 0.14949846267700195\n",
      "Batch 23 Epoch 43: train loss 0.012302021495997906 val loss 0.1364617943763733\n",
      "Batch 24 Epoch 43: train loss 0.011545867659151554 val loss 0.04854585602879524\n",
      "Batch 25 Epoch 43: train loss 0.007815534248948097 val loss 0.04751642048358917\n",
      "Batch 26 Epoch 43: train loss 0.003101240610703826 val loss 0.0447671078145504\n",
      "Batch 27 Epoch 43: train loss 0.0033702750224620104 val loss 0.04201609641313553\n",
      "Batch 28 Epoch 43: train loss 0.0013676246162503958 val loss 0.039906661957502365\n",
      "Batch 29 Epoch 43: train loss 0.002904870081692934 val loss 0.038480211049318314\n",
      "Batch 30 Epoch 43: train loss 0.00629033986479044 val loss 0.037743061780929565\n",
      "Batch 31 Epoch 43: train loss 0.005931716412305832 val loss 0.03758487105369568\n",
      "Batch 32 Epoch 43: train loss 0.019559267908334732 val loss 0.03835473582148552\n",
      "Batch 33 Epoch 43: train loss 0.10977780818939209 val loss 0.04048970714211464\n",
      "Batch 34 Epoch 43: train loss 0.009954248555004597 val loss 0.04232926294207573\n",
      "Batch 35 Epoch 43: train loss 0.01999642327427864 val loss 0.044139984995126724\n",
      "Batch 36 Epoch 43: train loss 0.020788826048374176 val loss 0.046111106872558594\n",
      "Batch 37 Epoch 43: train loss 0.0937054231762886 val loss 0.047954872250556946\n",
      "Batch 38 Epoch 43: train loss 0.06785628944635391 val loss 0.04835834354162216\n",
      "Batch 39 Epoch 43: train loss 0.03561316430568695 val loss 0.04711584001779556\n",
      "Batch 40 Epoch 43: train loss 0.012223092839121819 val loss 0.04531152546405792\n",
      "Batch 41 Epoch 43: train loss 0.007567391265183687 val loss 0.04303009808063507\n",
      "Batch 42 Epoch 43: train loss 0.27010810375213623 val loss 0.038432665169239044\n",
      "Batch 43 Epoch 43: train loss 0.06082828342914581 val loss 0.03150317445397377\n",
      "Batch 44 Epoch 43: train loss 0.07163946330547333 val loss 0.02222204953432083\n",
      "Batch 45 Epoch 43: train loss 0.07052528858184814 val loss 0.012756943702697754\n",
      "Batch 46 Epoch 43: train loss 0.0427057221531868 val loss 0.009577829390764236\n",
      "Batch 47 Epoch 43: train loss 0.04295048117637634 val loss 0.009394591674208641\n",
      "Batch 48 Epoch 43: train loss 0.010387816466391087 val loss 0.010055135004222393\n",
      "Batch 49 Epoch 43: train loss 0.0017242988105863333 val loss 0.011327552609145641\n",
      "Batch 0 Epoch 44: train loss 0.05959383025765419 val loss 0.013805942609906197\n",
      "Batch 1 Epoch 44: train loss 0.03865398094058037 val loss 0.017687415704131126\n",
      "Batch 2 Epoch 44: train loss 0.030788235366344452 val loss 0.023316893726587296\n",
      "Batch 3 Epoch 44: train loss 0.008347162976861 val loss 0.03002547286450863\n",
      "Batch 4 Epoch 44: train loss 0.021124200895428658 val loss 0.03802817314863205\n",
      "Batch 5 Epoch 44: train loss 0.018159758299589157 val loss 0.04753733053803444\n",
      "Batch 6 Epoch 44: train loss 0.010396921075880527 val loss 0.9264428615570068\n",
      "Batch 7 Epoch 44: train loss 0.0016633097548037767 val loss 1.1600104570388794\n",
      "Batch 8 Epoch 44: train loss 0.004309516400098801 val loss 1.3708882331848145\n",
      "Batch 9 Epoch 44: train loss 0.005889415740966797 val loss 1.5391724109649658\n",
      "Batch 10 Epoch 44: train loss 0.010754883289337158 val loss 1.651390790939331\n",
      "Batch 11 Epoch 44: train loss 0.0169815830886364 val loss 1.6765996217727661\n",
      "Batch 12 Epoch 44: train loss 0.012648931704461575 val loss 1.6352001428604126\n",
      "Batch 13 Epoch 44: train loss 0.031034333631396294 val loss 1.5206875801086426\n",
      "Batch 14 Epoch 44: train loss 0.004707532934844494 val loss 1.3896070718765259\n",
      "Batch 15 Epoch 44: train loss 0.0026079255621880293 val loss 1.2598665952682495\n",
      "Batch 16 Epoch 44: train loss 0.007801620755344629 val loss 1.1804025173187256\n",
      "Batch 17 Epoch 44: train loss 0.0014133662916719913 val loss 1.117516040802002\n",
      "Batch 18 Epoch 44: train loss 0.0017958417301997542 val loss 1.0759575366973877\n",
      "Batch 19 Epoch 44: train loss 0.0022850269451737404 val loss 1.0573549270629883\n",
      "Batch 20 Epoch 44: train loss 0.0025940020568668842 val loss 1.0581676959991455\n",
      "Batch 21 Epoch 44: train loss 0.004595340229570866 val loss 1.0847864151000977\n",
      "Batch 22 Epoch 44: train loss 0.004673290997743607 val loss 1.1374274492263794\n",
      "Batch 23 Epoch 44: train loss 0.0015899640275165439 val loss 1.2007006406784058\n",
      "Batch 24 Epoch 44: train loss 0.0014416093472391367 val loss 1.2750494480133057\n",
      "Batch 25 Epoch 44: train loss 0.000597409438341856 val loss 1.3517382144927979\n",
      "Batch 26 Epoch 44: train loss 0.004256854765117168 val loss 1.400960922241211\n",
      "Batch 27 Epoch 44: train loss 0.0018406984163448215 val loss 1.4521251916885376\n",
      "Batch 28 Epoch 44: train loss 0.00255550816655159 val loss 1.474704384803772\n",
      "Batch 29 Epoch 44: train loss 0.0021759250666946173 val loss 1.4763438701629639\n",
      "Batch 30 Epoch 44: train loss 0.003539976431056857 val loss 1.4571954011917114\n",
      "Batch 31 Epoch 44: train loss 0.001906505087390542 val loss 1.4288409948349\n",
      "Batch 32 Epoch 44: train loss 0.006742880214005709 val loss 1.3694353103637695\n",
      "Batch 33 Epoch 44: train loss 0.05212169885635376 val loss 1.251502513885498\n",
      "Batch 34 Epoch 44: train loss 0.009955991059541702 val loss 1.158186674118042\n",
      "Batch 35 Epoch 44: train loss 0.06456771492958069 val loss 1.088797926902771\n",
      "Batch 36 Epoch 44: train loss 0.009412435814738274 val loss 1.0212007761001587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 37 Epoch 44: train loss 0.045204129070043564 val loss 0.9375560283660889\n",
      "Batch 38 Epoch 44: train loss 0.038990430533885956 val loss 0.8370535969734192\n",
      "Batch 39 Epoch 44: train loss 0.03324954956769943 val loss 0.044207580387592316\n",
      "Batch 40 Epoch 44: train loss 0.05665716528892517 val loss 0.03318105265498161\n",
      "Batch 41 Epoch 44: train loss 0.022078150883316994 val loss 0.02343616634607315\n",
      "Batch 42 Epoch 44: train loss 0.2153671383857727 val loss 0.017045916989445686\n",
      "Batch 43 Epoch 44: train loss 0.03424656018614769 val loss 0.012088879942893982\n",
      "Batch 44 Epoch 44: train loss 0.02005700021982193 val loss 0.009561996906995773\n",
      "Batch 45 Epoch 44: train loss 0.004161563701927662 val loss 0.009402752853929996\n",
      "Batch 46 Epoch 44: train loss 0.0679689422249794 val loss 0.01273312047123909\n",
      "Batch 47 Epoch 44: train loss 0.018752260133624077 val loss 0.02121230959892273\n",
      "Batch 48 Epoch 44: train loss 0.025534966960549355 val loss 0.03066936507821083\n",
      "Batch 49 Epoch 44: train loss 0.029770413413643837 val loss 0.03827575221657753\n",
      "Batch 0 Epoch 45: train loss 0.09348083287477493 val loss 0.04333554580807686\n",
      "Batch 1 Epoch 45: train loss 0.06993923336267471 val loss 0.04615967348217964\n",
      "Batch 2 Epoch 45: train loss 0.06700170040130615 val loss 0.04666610434651375\n",
      "Batch 3 Epoch 45: train loss 0.020522091537714005 val loss 0.04622633010149002\n",
      "Batch 4 Epoch 45: train loss 0.03327573090791702 val loss 0.04501624405384064\n",
      "Batch 5 Epoch 45: train loss 0.0440787598490715 val loss 0.04280548170208931\n",
      "Batch 6 Epoch 45: train loss 0.04697173088788986 val loss 0.03937714174389839\n",
      "Batch 7 Epoch 45: train loss 0.026973605155944824 val loss 0.035180870443582535\n",
      "Batch 8 Epoch 45: train loss 0.0092591792345047 val loss 0.031048588454723358\n",
      "Batch 9 Epoch 45: train loss 0.002933615120127797 val loss 0.027364715933799744\n",
      "Batch 10 Epoch 45: train loss 0.003648134646937251 val loss 0.024462249130010605\n",
      "Batch 11 Epoch 45: train loss 0.00767197459936142 val loss 0.022719893604516983\n",
      "Batch 12 Epoch 45: train loss 0.013985412195324898 val loss 0.022091392427682877\n",
      "Batch 13 Epoch 45: train loss 0.048978451639413834 val loss 0.02295057289302349\n",
      "Batch 14 Epoch 45: train loss 0.026033785194158554 val loss 0.02481039986014366\n",
      "Batch 15 Epoch 45: train loss 0.022943448275327682 val loss 0.027629593387246132\n",
      "Batch 16 Epoch 45: train loss 0.016910186037421227 val loss 0.0314268060028553\n",
      "Batch 17 Epoch 45: train loss 0.008389214985072613 val loss 0.03586952015757561\n",
      "Batch 18 Epoch 45: train loss 0.002458326518535614 val loss 0.040587738156318665\n",
      "Batch 19 Epoch 45: train loss 0.0006183995283208787 val loss 0.04497210308909416\n",
      "Batch 20 Epoch 45: train loss 0.0024881302379071712 val loss 0.048670075833797455\n",
      "Batch 21 Epoch 45: train loss 0.007445870898663998 val loss 0.16929765045642853\n",
      "Batch 22 Epoch 45: train loss 0.0123731829226017 val loss 0.1771848052740097\n",
      "Batch 23 Epoch 45: train loss 0.009745761752128601 val loss 0.17356474697589874\n",
      "Batch 24 Epoch 45: train loss 0.011686684563755989 val loss 0.0481937937438488\n",
      "Batch 25 Epoch 45: train loss 0.009167038835585117 val loss 0.04876033589243889\n",
      "Batch 26 Epoch 45: train loss 0.003399884095415473 val loss 0.04650857299566269\n",
      "Batch 27 Epoch 45: train loss 0.004575869534164667 val loss 0.044022683054208755\n",
      "Batch 28 Epoch 45: train loss 0.0005465808208100498 val loss 0.04197303578257561\n",
      "Batch 29 Epoch 45: train loss 0.0014765324303880334 val loss 0.0404261015355587\n",
      "Batch 30 Epoch 45: train loss 0.004495119675993919 val loss 0.03948577865958214\n",
      "Batch 31 Epoch 45: train loss 0.004214772954583168 val loss 0.039079271256923676\n",
      "Batch 32 Epoch 45: train loss 0.016292376443743706 val loss 0.03959923982620239\n",
      "Batch 33 Epoch 45: train loss 0.0996532291173935 val loss 0.04156561940908432\n",
      "Batch 34 Epoch 45: train loss 0.008162076584994793 val loss 0.043289147317409515\n",
      "Batch 35 Epoch 45: train loss 0.020421776920557022 val loss 0.044968876987695694\n",
      "Batch 36 Epoch 45: train loss 0.020675551146268845 val loss 0.046848710626363754\n",
      "Batch 37 Epoch 45: train loss 0.09137824922800064 val loss 0.048701293766498566\n",
      "Batch 38 Epoch 45: train loss 0.058475468307733536 val loss 0.04931684210896492\n",
      "Batch 39 Epoch 45: train loss 0.03574274107813835 val loss 0.048279523849487305\n",
      "Batch 40 Epoch 45: train loss 0.013276280835270882 val loss 0.046563684940338135\n",
      "Batch 41 Epoch 45: train loss 0.008110529743134975 val loss 0.044279903173446655\n",
      "Batch 42 Epoch 45: train loss 0.2709949314594269 val loss 0.0395478680729866\n",
      "Batch 43 Epoch 45: train loss 0.06192336976528168 val loss 0.032333601266145706\n",
      "Batch 44 Epoch 45: train loss 0.07517161220312119 val loss 0.022610969841480255\n",
      "Batch 45 Epoch 45: train loss 0.07869274169206619 val loss 0.01264238078147173\n",
      "Batch 46 Epoch 45: train loss 0.03756988048553467 val loss 0.009316262789070606\n",
      "Batch 47 Epoch 45: train loss 0.04651985689997673 val loss 0.009502015076577663\n",
      "Batch 48 Epoch 45: train loss 0.010169859044253826 val loss 0.010823076590895653\n",
      "Batch 49 Epoch 45: train loss 0.0020990173798054457 val loss 0.012856792658567429\n",
      "Batch 0 Epoch 46: train loss 0.044043827801942825 val loss 0.016274873167276382\n",
      "Batch 1 Epoch 46: train loss 0.027137570083141327 val loss 0.021112637594342232\n",
      "Batch 2 Epoch 46: train loss 0.018967730924487114 val loss 0.027537163347005844\n",
      "Batch 3 Epoch 46: train loss 0.00675610639154911 val loss 0.034708090126514435\n",
      "Batch 4 Epoch 46: train loss 0.016349589452147484 val loss 0.04294458404183388\n",
      "Batch 5 Epoch 46: train loss 0.013656162656843662 val loss 0.4746972322463989\n",
      "Batch 6 Epoch 46: train loss 0.006491100415587425 val loss 0.6205319166183472\n",
      "Batch 7 Epoch 46: train loss 0.0015273072058334947 val loss 0.7697882056236267\n",
      "Batch 8 Epoch 46: train loss 0.005620627198368311 val loss 0.8973584771156311\n",
      "Batch 9 Epoch 46: train loss 0.005920632742345333 val loss 0.9921114444732666\n",
      "Batch 10 Epoch 46: train loss 0.009526971727609634 val loss 1.0497205257415771\n",
      "Batch 11 Epoch 46: train loss 0.013239068910479546 val loss 1.0526070594787598\n",
      "Batch 12 Epoch 46: train loss 0.008632328361272812 val loss 1.0185433626174927\n",
      "Batch 13 Epoch 46: train loss 0.025463327765464783 val loss 0.9415935277938843\n",
      "Batch 14 Epoch 46: train loss 0.0025660227984189987 val loss 0.8592971563339233\n",
      "Batch 15 Epoch 46: train loss 0.0015037598786875606 val loss 0.7825992703437805\n",
      "Batch 16 Epoch 46: train loss 0.012100307270884514 val loss 0.7466065287590027\n",
      "Batch 17 Epoch 46: train loss 0.0014253964181989431 val loss 0.7219061255455017\n",
      "Batch 18 Epoch 46: train loss 0.0019374724943190813 val loss 0.711370587348938\n",
      "Batch 19 Epoch 46: train loss 0.002216772176325321 val loss 0.7157766222953796\n",
      "Batch 20 Epoch 46: train loss 0.0025453853886574507 val loss 0.732634961605072\n",
      "Batch 21 Epoch 46: train loss 0.0032485052943229675 val loss 0.7653269171714783\n",
      "Batch 22 Epoch 46: train loss 0.002914059441536665 val loss 0.8135025501251221\n",
      "Batch 23 Epoch 46: train loss 0.0006334750214591622 val loss 0.8623746633529663\n",
      "Batch 24 Epoch 46: train loss 0.0007062042132019997 val loss 0.9144233465194702\n",
      "Batch 25 Epoch 46: train loss 0.0005609196959994733 val loss 0.9613745808601379\n",
      "Batch 26 Epoch 46: train loss 0.004731725435703993 val loss 0.9825448393821716\n",
      "Batch 27 Epoch 46: train loss 0.0020583022851496935 val loss 1.004432201385498\n",
      "Batch 28 Epoch 46: train loss 0.002983688609674573 val loss 1.0032958984375\n",
      "Batch 29 Epoch 46: train loss 0.001851538079790771 val loss 0.9894827604293823\n",
      "Batch 30 Epoch 46: train loss 0.002775985049083829 val loss 0.9672529101371765\n",
      "Batch 31 Epoch 46: train loss 0.001533739734441042 val loss 0.9439332485198975\n",
      "Batch 32 Epoch 46: train loss 0.004811334889382124 val loss 0.9020419716835022\n",
      "Batch 33 Epoch 46: train loss 0.04711783304810524 val loss 0.8181539177894592\n",
      "Batch 34 Epoch 46: train loss 0.013627711683511734 val loss 0.7528683543205261\n",
      "Batch 35 Epoch 46: train loss 0.07237125188112259 val loss 0.703700065612793\n",
      "Batch 36 Epoch 46: train loss 0.008984426967799664 val loss 0.657667875289917\n",
      "Batch 37 Epoch 46: train loss 0.04282163456082344 val loss 0.5994795560836792\n",
      "Batch 38 Epoch 46: train loss 0.041278909891843796 val loss 0.5259962677955627\n",
      "Batch 39 Epoch 46: train loss 0.03606783226132393 val loss 0.04603622481226921\n",
      "Batch 40 Epoch 46: train loss 0.06121770665049553 val loss 0.03368089720606804\n",
      "Batch 41 Epoch 46: train loss 0.024200696498155594 val loss 0.022956112399697304\n",
      "Batch 42 Epoch 46: train loss 0.21418076753616333 val loss 0.01617918536067009\n",
      "Batch 43 Epoch 46: train loss 0.032337140291929245 val loss 0.011371320113539696\n",
      "Batch 44 Epoch 46: train loss 0.015895351767539978 val loss 0.00927532184869051\n",
      "Batch 45 Epoch 46: train loss 0.006900523789227009 val loss 0.009262599982321262\n",
      "Batch 46 Epoch 46: train loss 0.055298276245594025 val loss 0.01177262607961893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 47 Epoch 46: train loss 0.020504023879766464 val loss 0.01853378489613533\n",
      "Batch 48 Epoch 46: train loss 0.022642549127340317 val loss 0.02609986811876297\n",
      "Batch 49 Epoch 46: train loss 0.023934032768011093 val loss 0.03215751424431801\n",
      "Batch 0 Epoch 47: train loss 0.07629459351301193 val loss 0.036063216626644135\n",
      "Batch 1 Epoch 47: train loss 0.05410264432430267 val loss 0.03812102600932121\n",
      "Batch 2 Epoch 47: train loss 0.05018043890595436 val loss 0.03828156739473343\n",
      "Batch 3 Epoch 47: train loss 0.01409568265080452 val loss 0.03776315227150917\n",
      "Batch 4 Epoch 47: train loss 0.024691415950655937 val loss 0.03661979362368584\n",
      "Batch 5 Epoch 47: train loss 0.03236651420593262 val loss 0.034668561071157455\n",
      "Batch 6 Epoch 47: train loss 0.034087393432855606 val loss 0.03172554820775986\n",
      "Batch 7 Epoch 47: train loss 0.01747436635196209 val loss 0.028258828446269035\n",
      "Batch 8 Epoch 47: train loss 0.004652142524719238 val loss 0.025004884228110313\n",
      "Batch 9 Epoch 47: train loss 0.0010619242675602436 val loss 0.022244960069656372\n",
      "Batch 10 Epoch 47: train loss 0.0038019774947315454 val loss 0.020168544724583626\n",
      "Batch 11 Epoch 47: train loss 0.009037302806973457 val loss 0.019093206152319908\n",
      "Batch 12 Epoch 47: train loss 0.013061031699180603 val loss 0.018921062350273132\n",
      "Batch 13 Epoch 47: train loss 0.043363623321056366 val loss 0.019999222829937935\n",
      "Batch 14 Epoch 47: train loss 0.018744736909866333 val loss 0.021886060014367104\n",
      "Batch 15 Epoch 47: train loss 0.015464517287909985 val loss 0.02454841136932373\n",
      "Batch 16 Epoch 47: train loss 0.012704555876553059 val loss 0.0280312467366457\n",
      "Batch 17 Epoch 47: train loss 0.00579087482765317 val loss 0.03204558417201042\n",
      "Batch 18 Epoch 47: train loss 0.0012188733089715242 val loss 0.03620468080043793\n",
      "Batch 19 Epoch 47: train loss 0.0011917210649698973 val loss 0.03993260860443115\n",
      "Batch 20 Epoch 47: train loss 0.003233964554965496 val loss 0.042937058955430984\n",
      "Batch 21 Epoch 47: train loss 0.00755457254126668 val loss 0.04482576623558998\n",
      "Batch 22 Epoch 47: train loss 0.011970523744821548 val loss 0.04535600543022156\n",
      "Batch 23 Epoch 47: train loss 0.008825067430734634 val loss 0.04478224739432335\n",
      "Batch 24 Epoch 47: train loss 0.009732288308441639 val loss 0.04317020624876022\n",
      "Batch 25 Epoch 47: train loss 0.006641761399805546 val loss 0.04088454321026802\n",
      "Batch 26 Epoch 47: train loss 0.0027939314022660255 val loss 0.03871913254261017\n",
      "Batch 27 Epoch 47: train loss 0.003676295280456543 val loss 0.03641461208462715\n",
      "Batch 28 Epoch 47: train loss 0.0010301375295966864 val loss 0.034638162702322006\n",
      "Batch 29 Epoch 47: train loss 0.0019381330348551273 val loss 0.03340404853224754\n",
      "Batch 30 Epoch 47: train loss 0.005016352981328964 val loss 0.03277897834777832\n",
      "Batch 31 Epoch 47: train loss 0.004642147570848465 val loss 0.03266926109790802\n",
      "Batch 32 Epoch 47: train loss 0.016313379630446434 val loss 0.03344527259469032\n",
      "Batch 33 Epoch 47: train loss 0.09643197804689407 val loss 0.03566514700651169\n",
      "Batch 34 Epoch 47: train loss 0.006406502798199654 val loss 0.03769133612513542\n",
      "Batch 35 Epoch 47: train loss 0.021172380074858665 val loss 0.039656005799770355\n",
      "Batch 36 Epoch 47: train loss 0.019039111211895943 val loss 0.041825369000434875\n",
      "Batch 37 Epoch 47: train loss 0.08488268405199051 val loss 0.04406319558620453\n",
      "Batch 38 Epoch 47: train loss 0.057834699749946594 val loss 0.045093998312950134\n",
      "Batch 39 Epoch 47: train loss 0.0324765145778656 val loss 0.04459204524755478\n",
      "Batch 40 Epoch 47: train loss 0.011349146254360676 val loss 0.0434795543551445\n",
      "Batch 41 Epoch 47: train loss 0.006912499666213989 val loss 0.041797541081905365\n",
      "Batch 42 Epoch 47: train loss 0.2671665847301483 val loss 0.03774634376168251\n",
      "Batch 43 Epoch 47: train loss 0.05959795042872429 val loss 0.031136300414800644\n",
      "Batch 44 Epoch 47: train loss 0.06961371004581451 val loss 0.021973852068185806\n",
      "Batch 45 Epoch 47: train loss 0.07345044612884521 val loss 0.012448168359696865\n",
      "Batch 46 Epoch 47: train loss 0.037682078778743744 val loss 0.009350376203656197\n",
      "Batch 47 Epoch 47: train loss 0.04089809209108353 val loss 0.00951642356812954\n",
      "Batch 48 Epoch 47: train loss 0.010150116868317127 val loss 0.010771620087325573\n",
      "Batch 49 Epoch 47: train loss 0.0019278497202321887 val loss 0.01270182617008686\n",
      "Batch 0 Epoch 48: train loss 0.04617249220609665 val loss 0.01602923683822155\n",
      "Batch 1 Epoch 48: train loss 0.029027193784713745 val loss 0.02082972414791584\n",
      "Batch 2 Epoch 48: train loss 0.020416144281625748 val loss 0.027313001453876495\n",
      "Batch 3 Epoch 48: train loss 0.006986993830651045 val loss 0.03464914485812187\n",
      "Batch 4 Epoch 48: train loss 0.01757630705833435 val loss 0.0431530699133873\n",
      "Batch 5 Epoch 48: train loss 0.013237022794783115 val loss 0.5622422695159912\n",
      "Batch 6 Epoch 48: train loss 0.005766079295426607 val loss 0.7324578166007996\n",
      "Batch 7 Epoch 48: train loss 0.0012888347264379263 val loss 0.9046444892883301\n",
      "Batch 8 Epoch 48: train loss 0.0061591980047523975 val loss 1.0488052368164062\n",
      "Batch 9 Epoch 48: train loss 0.006003131158649921 val loss 1.1547685861587524\n",
      "Batch 10 Epoch 48: train loss 0.010212389752268791 val loss 1.2151305675506592\n",
      "Batch 11 Epoch 48: train loss 0.013609916903078556 val loss 1.2133315801620483\n",
      "Batch 12 Epoch 48: train loss 0.009064036421477795 val loss 1.1687977313995361\n",
      "Batch 13 Epoch 48: train loss 0.024378718808293343 val loss 1.0799319744110107\n",
      "Batch 14 Epoch 48: train loss 0.0024201113265007734 val loss 0.9864915013313293\n",
      "Batch 15 Epoch 48: train loss 0.0015035585965961218 val loss 0.8999065160751343\n",
      "Batch 16 Epoch 48: train loss 0.01231570914387703 val loss 0.8573437929153442\n",
      "Batch 17 Epoch 48: train loss 0.001750544412061572 val loss 0.828690230846405\n",
      "Batch 18 Epoch 48: train loss 0.0027888112235814333 val loss 0.8192775249481201\n",
      "Batch 19 Epoch 48: train loss 0.002557427855208516 val loss 0.8269038796424866\n",
      "Batch 20 Epoch 48: train loss 0.0027198183815926313 val loss 0.849203884601593\n",
      "Batch 21 Epoch 48: train loss 0.003679798450320959 val loss 0.8902840614318848\n",
      "Batch 22 Epoch 48: train loss 0.0031450747046619654 val loss 0.9478362798690796\n",
      "Batch 23 Epoch 48: train loss 0.00088187784422189 val loss 1.00578773021698\n",
      "Batch 24 Epoch 48: train loss 0.0006413225783035159 val loss 1.0656428337097168\n",
      "Batch 25 Epoch 48: train loss 0.0005231176037341356 val loss 1.1189059019088745\n",
      "Batch 26 Epoch 48: train loss 0.005642308387905359 val loss 1.1403084993362427\n",
      "Batch 27 Epoch 48: train loss 0.0017156150424852967 val loss 1.1597726345062256\n",
      "Batch 28 Epoch 48: train loss 0.0032438647467643023 val loss 1.153128743171692\n",
      "Batch 29 Epoch 48: train loss 0.001882252749055624 val loss 1.1330773830413818\n",
      "Batch 30 Epoch 48: train loss 0.0030279653146862984 val loss 1.1030689477920532\n",
      "Batch 31 Epoch 48: train loss 0.0014564754674211144 val loss 1.073265552520752\n",
      "Batch 32 Epoch 48: train loss 0.004302761983126402 val loss 1.0245450735092163\n",
      "Batch 33 Epoch 48: train loss 0.04590831324458122 val loss 0.931627094745636\n",
      "Batch 34 Epoch 48: train loss 0.013502665795385838 val loss 0.8592791557312012\n",
      "Batch 35 Epoch 48: train loss 0.0696425810456276 val loss 0.8060309886932373\n",
      "Batch 36 Epoch 48: train loss 0.008443836122751236 val loss 0.7559183835983276\n",
      "Batch 37 Epoch 48: train loss 0.04188009351491928 val loss 0.6920529007911682\n",
      "Batch 38 Epoch 48: train loss 0.03925485908985138 val loss 0.611221194267273\n",
      "Batch 39 Epoch 48: train loss 0.036262400448322296 val loss 0.043793875724077225\n",
      "Batch 40 Epoch 48: train loss 0.058387383818626404 val loss 0.0320495180785656\n",
      "Batch 41 Epoch 48: train loss 0.022557543590664864 val loss 0.021966757252812386\n",
      "Batch 42 Epoch 48: train loss 0.21549542248249054 val loss 0.015594863332808018\n",
      "Batch 43 Epoch 48: train loss 0.033044327050447464 val loss 0.01108481828123331\n",
      "Batch 44 Epoch 48: train loss 0.01829627715051174 val loss 0.009222942404448986\n",
      "Batch 45 Epoch 48: train loss 0.005347359459847212 val loss 0.00941759254783392\n",
      "Batch 46 Epoch 48: train loss 0.057961899787187576 val loss 0.012455865740776062\n",
      "Batch 47 Epoch 48: train loss 0.016025878489017487 val loss 0.019421828910708427\n",
      "Batch 48 Epoch 48: train loss 0.022999094799160957 val loss 0.02684497833251953\n",
      "Batch 49 Epoch 48: train loss 0.02472068928182125 val loss 0.032578032463788986\n",
      "Batch 0 Epoch 49: train loss 0.08351445943117142 val loss 0.03595134988427162\n",
      "Batch 1 Epoch 49: train loss 0.05952218919992447 val loss 0.037357572466135025\n",
      "Batch 2 Epoch 49: train loss 0.05496888980269432 val loss 0.036814309656620026\n",
      "Batch 3 Epoch 49: train loss 0.015714474022388458 val loss 0.03562120720744133\n",
      "Batch 4 Epoch 49: train loss 0.02716095931828022 val loss 0.033849578350782394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5 Epoch 49: train loss 0.03617021068930626 val loss 0.031316593289375305\n",
      "Batch 6 Epoch 49: train loss 0.035152457654476166 val loss 0.027953730896115303\n",
      "Batch 7 Epoch 49: train loss 0.01887500286102295 val loss 0.024218345060944557\n",
      "Batch 8 Epoch 49: train loss 0.004880711901932955 val loss 0.02084413543343544\n",
      "Batch 9 Epoch 49: train loss 0.0011203044559806585 val loss 0.018078822642564774\n",
      "Batch 10 Epoch 49: train loss 0.004096623044461012 val loss 0.016085850074887276\n",
      "Batch 11 Epoch 49: train loss 0.009051613509654999 val loss 0.015031840652227402\n",
      "Batch 12 Epoch 49: train loss 0.014126721769571304 val loss 0.01478530652821064\n",
      "Batch 13 Epoch 49: train loss 0.04576249420642853 val loss 0.015620922669768333\n",
      "Batch 14 Epoch 49: train loss 0.019752169027924538 val loss 0.01719011552631855\n",
      "Batch 15 Epoch 49: train loss 0.0167846716940403 val loss 0.01953434757888317\n",
      "Batch 16 Epoch 49: train loss 0.014398634433746338 val loss 0.022788280621170998\n",
      "Batch 17 Epoch 49: train loss 0.0054186568595469 val loss 0.026633553206920624\n",
      "Batch 18 Epoch 49: train loss 0.0012921154266223311 val loss 0.030712803825736046\n",
      "Batch 19 Epoch 49: train loss 0.0010068012634292245 val loss 0.03445613011717796\n",
      "Batch 20 Epoch 49: train loss 0.0035059740766882896 val loss 0.03753229230642319\n",
      "Batch 21 Epoch 49: train loss 0.0075344811193645 val loss 0.03953897953033447\n",
      "Batch 22 Epoch 49: train loss 0.0118476040661335 val loss 0.04021622613072395\n",
      "Batch 23 Epoch 49: train loss 0.008463433012366295 val loss 0.039813559502363205\n",
      "Batch 24 Epoch 49: train loss 0.009320566430687904 val loss 0.03839259594678879\n",
      "Batch 25 Epoch 49: train loss 0.00736002903431654 val loss 0.03624693676829338\n",
      "Batch 26 Epoch 49: train loss 0.003060444025322795 val loss 0.03417050838470459\n",
      "Batch 27 Epoch 49: train loss 0.0034644322004169226 val loss 0.031965721398591995\n",
      "Batch 28 Epoch 49: train loss 0.000924224965274334 val loss 0.030273864045739174\n",
      "Batch 29 Epoch 49: train loss 0.0018472581868991256 val loss 0.02908857725560665\n",
      "Batch 30 Epoch 49: train loss 0.0047318884171545506 val loss 0.028473762795329094\n",
      "Batch 31 Epoch 49: train loss 0.004505099728703499 val loss 0.028345083817839622\n",
      "Batch 32 Epoch 49: train loss 0.015730751678347588 val loss 0.029070638120174408\n",
      "Batch 33 Epoch 49: train loss 0.09555619955062866 val loss 0.031228382140398026\n",
      "Batch 34 Epoch 49: train loss 0.005902427714318037 val loss 0.0332409106194973\n",
      "Batch 35 Epoch 49: train loss 0.021517885848879814 val loss 0.035190243273973465\n",
      "Batch 36 Epoch 49: train loss 0.018988417461514473 val loss 0.037367306649684906\n",
      "Batch 37 Epoch 49: train loss 0.08473657071590424 val loss 0.039682984352111816\n",
      "Batch 38 Epoch 49: train loss 0.05556771159172058 val loss 0.041043296456336975\n",
      "Batch 39 Epoch 49: train loss 0.030332474038004875 val loss 0.041030023247003555\n",
      "Batch 40 Epoch 49: train loss 0.011064714752137661 val loss 0.0404551699757576\n",
      "Batch 41 Epoch 49: train loss 0.006204382050782442 val loss 0.039314158260822296\n",
      "Batch 42 Epoch 49: train loss 0.26466915011405945 val loss 0.035789854824543\n",
      "Batch 43 Epoch 49: train loss 0.05924772098660469 val loss 0.029713794589042664\n",
      "Batch 44 Epoch 49: train loss 0.07138197124004364 val loss 0.021033763885498047\n",
      "Batch 45 Epoch 49: train loss 0.07371179759502411 val loss 0.011984970420598984\n",
      "Batch 46 Epoch 49: train loss 0.03776714578270912 val loss 0.00912616215646267\n",
      "Batch 47 Epoch 49: train loss 0.04163708537817001 val loss 0.009568657726049423\n",
      "Batch 48 Epoch 49: train loss 0.00997609831392765 val loss 0.011156613938510418\n",
      "Batch 49 Epoch 49: train loss 0.002054462442174554 val loss 0.013434037566184998\n",
      "best validation loss = 0.008132786490023136\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "model = LSTM(input_size=num_features, seq_length=train_window)\n",
    "model.to(device)\n",
    "\n",
    "model, history, best_loss = train_model(model, device, train_batches, val_batches, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008132786490023136\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(best_loss)\n",
    "visualization(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth, predictions, losses = [], [], []\n",
    "device = torch.device(\"cpu\")\n",
    "criterion = nn.L1Loss(reduction='sum').to(device)\n",
    "model.to(device)\n",
    "\n",
    "X_test, y_test = target_seq\n",
    "test_set_size = X_test.size(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i in range(test_set_size):\n",
    "        x_i = X_test[i:i+1]\n",
    "        y_i = y_test[i:i+1]\n",
    "        x_i.to(device)\n",
    "        y_i.to(device)\n",
    "        model.init_hidden(x_i.size(0), device)\n",
    "        y_pred = model(x_i)\n",
    "        predictions.append(y_pred.cpu().numpy().flatten())\n",
    "        loss = criterion(y_pred, y_i)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "truth = y_test.cpu().numpy().flatten()\n",
    "predictions = np.array(predictions).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fce1c6dc048>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(datetime_target,predictions, label=\"Predictions\")\n",
    "plt.plot(datetime_target, truth, label=\"Truth\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25226812963176065"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(datetime_target ,losses, label=\"Prediction Loss\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving prediciton result as csv file for the entire dataset\n",
    "truth, predictions, losses = [], [], []\n",
    "device = torch.device(\"cpu\")\n",
    "criterion = nn.MSELoss(reduction='sum').to(device)\n",
    "model.to(device)\n",
    "\n",
    "X_test, y_test = gamestop_seq\n",
    "test_set_size = X_test.size(0)\n",
    "\n",
    "res = df_gamestop.copy()\n",
    "res['predictions'] = np.nan\n",
    "res['loss'] = np.nan\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i in range(test_set_size):\n",
    "        x_i = X_test[i:i+1]\n",
    "        y_i = y_test[i:i+1]\n",
    "        x_i.to(device)\n",
    "        y_i.to(device)\n",
    "        model.init_hidden(x_i.size(0), device)\n",
    "        y_pred = model(x_i)\n",
    "        loss = criterion(y_pred, y_i)\n",
    "        res.at[datetime_gamestop[i], 'predictions'] = y_pred.cpu().numpy().flatten()\n",
    "        res.at[datetime_gamestop[i], 'predictions'] = loss.item()\n",
    "        predictions.append(y_pred.cpu().numpy().flatten())\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        \n",
    "truth = y_test.cpu().numpy().flatten()\n",
    "predictions = np.array(predictions).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('../foobar/data/processed/gme_predictions_m1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fce1c9685f8>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(datetime_gamestop,predictions, label=\"Predictions\")\n",
    "plt.plot(datetime_gamestop, truth, label=\"Truth\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_index = 0\n",
    "for train_window in train_window_list:\n",
    "    for prediction_horizon in prediction_horizon_list:\n",
    "        for feature_set in feature_sets:\n",
    "\n",
    "            train_set, target_set = scale(df_train, df_target, feature_set)\n",
    "            train_set , val_set = split(train_set, 0.8)\n",
    "\n",
    "            train_seq, num_features = generate_window(train_set, train_window, prediction_horizon)\n",
    "            val_seq, _ = generate_window(val_set, train_window, prediction_horizon)\n",
    "            target_seq, _ = generate_window(target_set, train_window, prediction_horizon)\n",
    "\n",
    "            datetime_target = test_datetime_list[train_window+prediction_horizon:]\n",
    "\n",
    "            model = LSTM(input_size=num_features, seq_length=train_window)\n",
    "            model = model.to(device)\n",
    "\n",
    "            model, history = train_model(model, device, train_seq, val_seq)\n",
    "            visualization(history, )\n",
    "            model_index += 1\n",
    "\n",
    "            print('Saving...')\n",
    "            state = {\n",
    "                'model': model,\n",
    "                'feature_set': feature_set,\n",
    "                'history': history,\n",
    "                'pred_horizon': prediction_horizon,\n",
    "                'train_window': train_window\n",
    "            }\n",
    "\n",
    "            if not os.path.isdir('checkpoint'):\n",
    "                os.mkdir('checkpoint')\n",
    "            torch.save(state, f'./checkpoint/train_{model_index}.pth')\n",
    "            \n",
    "            \n",
    "            plt.plot(history['train'], label=\"Training loss\")\n",
    "            plt.plot(history['val'], label=\"Test loss\")\n",
    "            plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def \n",
    "\n",
    "plt.plot(history['train'], label=\"Training loss\")\n",
    "plt.plot(history['val'], label=\"Test loss\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd0e4b32c4c655fa694298ff80f540860bd845c191a9c38f277b9165c7be10f4e8e",
   "display_name": "Python 3.8.8 64-bit ('cmpt733': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}